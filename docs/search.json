[
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "Now",
    "section": "",
    "text": "A few things keep me busy these days:"
  },
  {
    "objectID": "now.html#kids",
    "href": "now.html#kids",
    "title": "Now",
    "section": "Kids",
    "text": "Kids\n\nRaising 2 young kids and spending 12 hours a day with them when their school is closed"
  },
  {
    "objectID": "now.html#research",
    "href": "now.html#research",
    "title": "Now",
    "section": "Research",
    "text": "Research\n\n\n\n\n\n\nTipBig picture question\n\n\n\nHow can we achieve carbon neutrality by mid-century to address climate change?\nHow to make the clean energy transition more:\n\nSustainable (energy and environment)\nResilient (energy and climate)\nEquitable (energy and justice)\n\n\n\n\nWorking with Kaifang Luo, Michael Davidson, Ahmad Lashkaripour, Ilaria Mazzocco, Minghao Qiu and collaborators and partners on the project Made in America: Drivers and Impacts of Domestic Clean Energy Manufacturing in the United States supported by the Alfred P. Sloan Foundation.\nWorking with California China Climate Institute to study clean energy supply chains and power system resources adequency\n\nWorking with Baruch colleagues on Exploring Energy Burden Among the Older Adults and People with Disabilities in New York City Public Housing Communities\n\nWorking on the climate, air pollution, and human health effect of imported solar PV supported by the ClimateWorks Foundation\n\nCollaborating with LBL on power system decarbonization\n\n\nWelcome collaborations on\n\nClean energy global supply chain\nPower system decarbonization modeling\nClimate impact on energy systems\nJust energy transition\nEnergy-water-carbon nexus\n\nOther interdisciplinary aspects of carbon neutrality"
  },
  {
    "objectID": "now.html#teaching",
    "href": "now.html#teaching",
    "title": "Now",
    "section": "Teaching",
    "text": "Teaching\nFall Semester:\n\nPAF 9187: Energy and Climate Policy\n\nSpring Semester:\n\nPAF 3195: Energy, Climate, and Society\nPAF 9174: Program Evaluation\n\nI’m seeking guest speakers who are passionate about sharing their experiences and inspiring next generation of climate leaders:\n\nClimate Guest Speakers"
  },
  {
    "objectID": "notes/new-york-state-climate-policy.html",
    "href": "notes/new-york-state-climate-policy.html",
    "title": "New York State Climate Policy Goals Clarified",
    "section": "",
    "text": "New York State is recognized as a climate leader, yet navigating its policies can be challenging without a deep understanding of the policymaking landscape. Here, I’ve aimed to link key policy goals with relevant legislation, agencies, and provide direct access to the original legislative text. I hope you find these resources helpful.\nNotes: ECL: Environmental Conservation Law; Emission reductin baseline: 1990 level; Renewable energy: solar thermal, PV, land and offshore wind, hydroelctric, geothermal electric, geothermal ground source heat, tidal energy, wave energy, ocean thermal, and fuel cells which do not utilize a fossil fuel resource in the process1; ZEV(zero emission vehicles): Battery Electric Vehicles, Fuel Cell Vehicles, Plug-in Hybrid Electric Vehicles (PHEV)2.\nUpdates: Distributed PV: On October 17, 2024, Governor Kathy Hochul announced that 6 gigawatts (GW) of distributed solar have been installed across New York, a year ahead of schedule3. Storage: On June 20, 2024, the New York Public Service Commission approved the Order Establishing Updated Energy Storage Goal and Deployment Policy which expands the State’s goal to 6GW of energy storage to be installed by 20304."
  },
  {
    "objectID": "notes/new-york-state-climate-policy.html#new-york-state-climate-leadership-and-community-protection-act",
    "href": "notes/new-york-state-climate-policy.html#new-york-state-climate-leadership-and-community-protection-act",
    "title": "New York State Climate Policy Goals Clarified",
    "section": "New York State Climate Leadership and Community Protection Act",
    "text": "New York State Climate Leadership and Community Protection Act\n\nGHG emissions goals\n\n2030: Reduce GHG by 40% below 1990 levels\n\n2050: Reduce GHG by 85% below 1990 levels\n\n\n§ 75-0107. Statewide greenhouse gas emissions limits.\n1. No later than one year after the effective date of this article, the department shall, pursuant to rules and regulations promulgated after at least one public hearing, establish a statewide greenhouse gas emissions limit as a percentage of 1990 emissions, as estimated pursuant to section 75-0105 of this article, as follows:\na. 2030: 60% of 1990 emissions.\nb. 2050: 15% of 1990 emissions.\n2. Greenhouse gas emission limits shall be measured in units of carbon dioxide equivalents and identified for each individual type of green-house gas.\n\n\n\nPower sector and renewable goals\n\n2030: Generate 70% of electricity from renewable energy sources\n\n2040: Achieve 100% zero-emission electricity\n\n\n§ 66-p. Establishment of a renewable energy program.\n1. As used in this section:\n(a) “jurisdictional load serving entity” means any entity subject to the jurisdiction of the commission that secures energy to serve the electrical energy requirements of end-use customers in New York state;\n(b) “renewable energy systems” means systems that generate electricity or thermal energy through use of the following technologies: solar thermal, photovoltaics, on land and offshore wind, hydroelectric, geothermal electric, geothermal ground source heat, tidal energy, wave energy, ocean thermal, and fuel cells which do not utilize a fossil fuel resource in the process of generating electricity.\n2. No later than June thirtieth, two thousand twenty-one, the commission shall establish a program to require that: (a) a minimum of seventy percent of the state wide electric generation secured by jurisdictional load serving entities to meet the electrical energy requirements of all end-use customers in New York state in two thousand thirty shall be generated by renewable energy systems; and (b) that by the year two thousand forty (collectively, the “targets”) the statewide electrical demand system will be zero emissions.\n\n\nTechnological specific goals:\n\n2025: Install 6 GW solar PV\n\n2030: Install 3 GW energy storage\n\n2035: Install 9 GW of offshore wind\n\n\n\n\nNo later than July first, two thousand twenty-four, the commission shall establish programs to require the procurement by the state’s load serving entities of at least nine gigawatts of offshore wind electricity generation by two thousand thirty-five and six gigawatts of photovoltaic solar generation by two thousand twenty-five, and to support three giga-watts of statewide energy storage capacity by two thousand thirty.\n\n\n\n\nWhat also in the bill?\n\nEstablish the New York state climate action council (“council”)\nAt least 35% of the benefits directed to disadvantaged communities with a goal to achieve 40%\n\n\n75-0117. Investment of funds.\nState agencies, authorities and entities, in consultation with the environmental justice working group and the climate action council, shall, to the extent practicable, invest or direct available and relevant programmatic resources in a manner designed to achieve a goal for disadvantaged communities to receive forty percent of overall benefits of spending on clean energy and energy efficiency programs, projects or investments in the areas of housing, workforce development, pollution reduction, low income energy assistance, energy, transportation and economic development, provided however, that disadvantaged communities shall receive no less than thirty-five percent of the overall benefits of spending on clean energy and energy efficiency programs, projects or investments and provided further that this section shall not alter funds already contracted or committed as of the effective date of this section.\n\nSource Document: Senate Bill S6599"
  },
  {
    "objectID": "notes/new-york-state-climate-policy.html#amendaments-to-envirnmental-coservation-law",
    "href": "notes/new-york-state-climate-policy.html#amendaments-to-envirnmental-coservation-law",
    "title": "New York State Climate Policy Goals Clarified",
    "section": "Amendaments to Envirnmental Coservation Law",
    "text": "Amendaments to Envirnmental Coservation Law\n\nEV sales goals\n\n2035: 100% new sale of passager cars and trucks shall be zero emission\n2045: 100% new sale of medium-duty and heavy-duty vehicles shall be zero emission\n\n\nSection 1. The environmental conservation law is amended by adding a new section 19-0306-b to read as follows:\n§ 19-0306-b. Zero-emissions cars and trucks.\n1. It shall be a goal of the state that one hundred percent of new passenger cars and trucks offered for sale or lease, or sold, or leased, for registration in the state shall be zero-emissions by two thousand thirty-five. It shall be a further goal of the state that one hundred percent of medium-duty and heavy-duty vehicles offered for sale or lease, or sold, or leased, for registration in the state be zero-emissions by two thousand forty-five for all operations where feasible. It shall be further a goal of the state to transition to one hundred percent zero-emissions off-road vehicles and equipment by two thousand thirty-five where feasible.\n\nSource Document: Senate Bill S2758\nNote: This is an ongoing policy issue, more updates are expected."
  },
  {
    "objectID": "notes/new-york-state-climate-policy.html#footnotes",
    "href": "notes/new-york-state-climate-policy.html#footnotes",
    "title": "New York State Climate Policy Goals Clarified",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.nysenate.gov/legislation/bills/2019/S6599↩︎\nhttps://dec.ny.gov/environmental-protection/air-quality/controlling-motor-vehicle-pollution/low-and-zero-emission-vehicles↩︎\nhttps://www.nyserda.ny.gov/About/Newsroom/2024-Announcements/2024-10-17-Governor-Hochul-Announces-New-York-State-Has-Achieved-Major-Solar-Milestone↩︎\nhttps://www.nyserda.ny.gov/All-Programs/Energy-Storage-Program↩︎"
  },
  {
    "objectID": "letters.html",
    "href": "letters.html",
    "title": "Letters",
    "section": "",
    "text": "I’m happpy to provide recommendation letters if you were in my class or collaborated with me directly. If not, I may not know you well enough to write a stong letter.\nWhen requesting a letter, please send bullet points addressing the following:\n\nWhat (class) project did you work on?\nWhat contributions did you make to the project?\nWhat challenges did you face during the project?\nHow did you handle those challenges?\nWhy are you interested in the opportunity or position?\n\nPlease also include links to the position or opportunity and note the deadlines. Please reach out at least one month before the deadline. If you need letters for multiple applications, please send all submission links together if possible."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "I’m a graduate student at Mila, broadly interested in building reliable machine learning systems that work at scale. At Mila, my research has focused on topics in RL, mainly offline (explainability and adaptive regularization), real-time RL, and benchmarking SSL methods for Atari games.\nMy larger goal is to develop safe, interpretable, real-time, and sample-efficient agents, motivating my interests in areas like mechanistic interpretability, world models, and real-time systems.\nBefore Mila, I co-founded Offside (scaled to 100k users), spent ~2 years at DFKI in Germany developing real-time vision algorithms for precision farming (blog post), and earned a BEng (Thesis) in Computer Science from BITS Pilani in 2020."
  },
  {
    "objectID": "about.html#beyond-research",
    "href": "about.html#beyond-research",
    "title": "about",
    "section": "Beyond Research",
    "text": "Beyond Research\nOutside of research, I enjoy reading about ancient civilizations, listening to classic rock, trekking, and strength training. I also write blogs reflecting on projects and life learnings. Check them out here."
  },
  {
    "objectID": "quarto-academic-site-examples.html",
    "href": "quarto-academic-site-examples.html",
    "title": "Quarto Academic Website Examples and Tips",
    "section": "",
    "text": "Web\nGitHub\n\n\n\n\nhttps://quarto.org/\nhttps://github.com/quarto-dev/quarto-web\n\n\nhttps://www.andrewheiss.com/\nhttps://github.com/andrewheiss/ath-quarto\n\n\nhttps://www.jhelvy.com/\nhttps://github.com/jhelvy/jhelvy_quarto\n\n\nhttps://faculty.washington.edu/masiello/\nhttps://github.com/MasielloGroup/MasielloGroupWebsite\n\n\nhttps://affcom.ku.edu/\nhttps://github.com/jmgirard/affcomlab\n\n\nhttps://silviacanelon.com/\nhttps://github.com/spcanelon/silvia"
  },
  {
    "objectID": "quarto-academic-site-examples.html#example-quarto-academic-websites-i-reference-occasionally",
    "href": "quarto-academic-site-examples.html#example-quarto-academic-websites-i-reference-occasionally",
    "title": "Quarto Academic Website Examples and Tips",
    "section": "",
    "text": "Web\nGitHub\n\n\n\n\nhttps://quarto.org/\nhttps://github.com/quarto-dev/quarto-web\n\n\nhttps://www.andrewheiss.com/\nhttps://github.com/andrewheiss/ath-quarto\n\n\nhttps://www.jhelvy.com/\nhttps://github.com/jhelvy/jhelvy_quarto\n\n\nhttps://faculty.washington.edu/masiello/\nhttps://github.com/MasielloGroup/MasielloGroupWebsite\n\n\nhttps://affcom.ku.edu/\nhttps://github.com/jmgirard/affcomlab\n\n\nhttps://silviacanelon.com/\nhttps://github.com/spcanelon/silvia"
  },
  {
    "objectID": "quarto-academic-site-examples.html#template-i-adapt-you-may-find-useful",
    "href": "quarto-academic-site-examples.html#template-i-adapt-you-may-find-useful",
    "title": "Quarto Academic Website Examples and Tips",
    "section": "Template I adapt you may find useful",
    "text": "Template I adapt you may find useful\n\n\n\nTemplate\nWeb\nGitHub\n\n\n\n\nAcademic\nhttps://drganghe.github.io/quarto-academic-website-template/\nhttps://github.com/drganghe/quarto-academic-website-template\n\n\n\n\nWebsite examples of the template\n\n\n\nType\nWeb\nGitHub\n\n\n\n\nPersonal\nhttps://drganghe.github.io/\nhttps://github.com/drganghe/drganghe.github.io\n\n\nLab/Group\nhttps://deeppolicylab.github.io/\nhttps://github.com/deeppolicylab/deeppolicylab.github.io\n\n\nClass\nhttps://drganghe.github.io/energy-climate-policy/\nhttps://github.com/drganghe/energy-climate-policy\n\n\nEvents\nhttps://drganghe.github.io/climate-guest-speakers/\nhttps://github.com/drganghe/climate-guest-speakers\n\n\n\nNote: If you’d like to use the template, please avoid forking my actual websites directly, as you will receive notifications whenever I update it. If you choose to do so, please be sure to remove my personal Google Analytics ID and MS Clarity ID from your site. Instead, I recommend forking the Quarto Academic website template to build your own website.\n\n\nOther examples\n\n\n\nType\nFile\nGitHub\n\n\n\n\nManuscript\nhttps://drganghe.github.io/files/papers/2025-nature-nuclear-costs.pdf\nhttps://github.com/drganghe/quarto-manuscript-example\n\n\nSlides\nhttps://drganghe.github.io/files/slides/solar-supply-chains-slides.html\nhttps://quarto.org/docs/presentations/revealjs/\n\n\nPoster\nhttps://drganghe.github.io/files/posters/nature-solar-supply-chains-poster.png\nhttps://github.com/brentthorne/posterdown"
  },
  {
    "objectID": "quarto-academic-site-examples.html#useful-tips-for-building-an-academic-quarto-site",
    "href": "quarto-academic-site-examples.html#useful-tips-for-building-an-academic-quarto-site",
    "title": "Quarto Academic Website Examples and Tips",
    "section": "Useful tips for building an academic Quarto site",
    "text": "Useful tips for building an academic Quarto site\n\nUsing “Includes” for reusable content\nTo streamline updates across your site, use Quarto’s includes feature for reusable elements like syllabus statements, social sharing options, academic badges, and other reuseable materials. By doing this, you only need to update a single source file, and then rerender the site to reflect changes across all pages.\n\"{{&lt; include _statement.qmd &gt;}}\"\nPrefixing include file names with an underscore (“_“) ensures they won’t be rendered directly by Quarto.\nHere is an example course description, and reuse it in mainpage and syllabus.\n\n\nAcademic badges\nDisplay academic badges like the Dimension Citation badge and Altmetric badge.\n\nHere’s an example for embedding them:\n&lt;span class=\"__dimensions_badge_embed__\" data-doi=\"10.1038/s41586-022-05316-6\" data-style=\"large_rectangle\" data-hide-zero-citations=\"true\"&gt;&lt;/span&gt; &lt;div data-badge-popover=\"right\" data-badge-type=\"1\" data-doi=\"10.1038/s41586-022-05316-6\" data-hide-no-mentions=\"true\" class=\"altmetric-embed\"&gt;&lt;/div&gt;\nReplace the doi with your own.\nMake sure to include below scripts in the same page to enable the badges:\n&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;\n\n&lt;script async src=\"https://badge.dimensions.ai/badge.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\nYou can also showcase your highly cited and hot papers using custom badges.\n \n![](https://drganghe.github.io/files/images/highly-cited-paper-badge.png \"ESI Highly Cited Paper\"){width=20 fig-alt=\"Highly cited paper\"} ![](https://drganghe.github.io/files/images/hot-paper-badge.png \"ESI Hot Paper\"){width=20 fig-alt=\"Hot paper\"}\nHere is an example, and the source code.\n\n\nSocial sharing\nFor easy setup of social sharing buttons, AddToAny is a simple solution.\nUsing includes files here will make it easier to manage updates, as any changes to social buttons need only be made once.\nHere is an example.\n\n\nThe art of listing\nListings are a powerful tool in Quarto, offering flexibility for creating visually appealing and organized pages. By adjusting listing options, you can control how content is presented.\nType: default, grid, table\nOptions: grid-columns, image-height, grid-item-align\nDisplay: fields and field-display-names\nBy mastering these options, you can easily create pages that fit your content needs while maintaining a clean, user-friendly design.\nHere is an example, and source code; another example, and source code.\n\n\nListing specific categories\nTo display a subset of items, use the include and exclude options. Below is an example of listing publications by category:\n  content:\n    - publications.yml\n  include:\n    categories: \"supply-chain\"\nHere, publications.yml is a YAML file containing a list of publications with predfined categories, and categories is the key used for filtering. The result will display only publications that match the specified category.\nHere is an example, and source code.\n\n\nListing external items\nTo list external resources with key metadata, you can use this format to create clear, linked listings.\n    - path: https://www.nature.com/articles/s41586-022-05316-6\n      image: /files/images/journal/nature.avif\n      title: \"Quantifying the cost savings of global solar photovoltaic supply chains\"\n      subtitle: \"*Nature*\"\n      description: \"Globalized supply chain has saved solar installers in the U.S., Germany, and China $67B 2008-2020, and solar prices will be 20-30% higher in 2030 if countries move to produce domestically.\"\n      date: \"2022-10-26\"\nFor efficiency, store multiple items in a YAML(.yml) file and reference it in Quarto.\nHere is an example, and source code.\n\n\nExporting Quarto Revealjs slides to PDF\nRevealjs’s default PDF Export omits footnotes, which is not acceptable if citations and sources are used. A workaround is to use decktape:\ndecktape automatic lecture1.html lecture1.pdf\nIf you encounter font error when opening the PDF:\n\n“Cannot extract the embedded font ‘BAAAAA+SourceSansPro-Regular’. Some characters may not display or print correctly.”\n\nUse Adobe Acrobat tools to fix it:\n\nOpen the PDF in Adobe Acrobat.\nGo to Use print production &gt; Preflight &gt; Analyze and fix it (default options).\nApply the necessary font correction.\n\nNote: you will see two Google AdSense in this page, it is a test for me to learn how AdSense works, please ignore them."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rishav’s Website",
    "section": "",
    "text": "GitHub\n  \n  \n    \n     Email\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Twitter\n  \n  \n    \n     Google Scholar\n  \n\n  \n  \nI’m a graduate researcher at Mila, broadly interested in building reliable machine learning systems that work at scale. At Mila, my research has focused on topics in RL, mainly offline (explainability and adaptive regularization), real-time RL, and benchmarking SSL methods for Atari games.\nMy larger goal is to develop safe, interpretable, real-time, and sample-efficient agents that scale, motivating my interests in areas like mechanistic interpretability, world models, and real-time distributed systems.\nBefore Mila, I co-founded Offside (scaled to 100k users), spent ~2 years at DFKI in Germany developing real-time vision algorithms for precision farming, and earned a BEng (Thesis) in Computer Science from BITS Pilani in 2020."
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "Rishav’s Website",
    "section": "Latest Posts",
    "text": "Latest Posts\n\n\n\n\n\n\n\n\n\n\nThe Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections\n\n\nmHC\n\n\n\nJan 3, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nReal-Time Reinforcement Learning\n\n\nArticle on real-time reinforcement learning\n\n\n\nJun 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nGSPO vs GRPO - Theory, Practice, and the Limits of Approximation\n\n\nGSPO vs GRPO\n\n\n\nNov 23, 2025\n\n\n\n\n\n\nNo matching items\n\n\nAll Posts »"
  },
  {
    "objectID": "index.html#beyond-research",
    "href": "index.html#beyond-research",
    "title": "Rishav’s Website",
    "section": "Beyond Research",
    "text": "Beyond Research\nOutside of research, I enjoy reading about ancient civilizations, listening to classic rock, trekking, and strength training. I also write blogs reflecting on projects and life learnings."
  },
  {
    "objectID": "MIGRATION_COMPLETE.html",
    "href": "MIGRATION_COMPLETE.html",
    "title": "Jekyll to Quarto Migration - Complete ✓",
    "section": "",
    "text": "Your Jekyll site has been successfully converted to Quarto format!\nLocation: /Users/rishav/website/drganghe.github.io\n\n\n\n\n\n\n✓ 5 Blog Posts (2024-2026)\n\nspotspraying.md\njax_distributed.md\nGSPO_GRPO.md\ncudp.md\nmHC.md\n\n✓ 6 Static Pages\n\nabout.qmd\ncv.qmd\nprojects.qmd\npublications.qmd\nrepositories.qmd\nteaching.qmd\n\n\n\n\n\n\n✓ 61 Images (profiles, thumbnails, research figures)\n✓ Bibliography (references.bib with full BibTeX entries)\n✓ Data Files (CV, coauthors, repositories, venues YAML)\n\n\n\n\n\n✓ **_quarto.yml** - Full site configuration with:\n\nSite metadata and description\nNavigation structure (About, Blog, Projects, Publications)\nSocial media links (GitHub, Twitter, LinkedIn)\nGoogle Analytics (G-1ZTF5BQFYQ)\nRSS feed support\nOpen Graph and Twitter card metadata\n\n\n\n\n\n\n\n\nPosts: - Jekyll: _posts/YYYY-MM-DD-title.md - Quarto: posts/YYYY-MM-DD-title/index.qmd\nPages: - Jekyll: _pages/about.md (with Jekyll-specific YAML) - Quarto: about.qmd (with Quarto-specific YAML)\nImages: - Jekyll: assets/img/ - Quarto: files/images/\nBibliography: - Jekyll: _bibliography/papers.bib - Quarto: references.bib\n\n\n\nJekyll example:\n---\nlayout: post\ntitle: \"Title\"\ndate: 2026-01-03\ntags: [ai, research]\nthumbnail: assets/img/image.png\n---\nQuarto equivalent:\n---\ntitle: \"Title\"\ndate: \"2026-01-03\"\ncategories: [ai, research]\nimage: \"/files/images/image.png\"\n---\n\n\n\n\ndrganghe.github.io/\n├── _quarto.yml              # Main configuration\n├── index.qmd                # Homepage with about section\n├── posts.qmd                # Blog listing page\n├── posts/                   # Blog posts (67 total)\n│   ├── 2024-12-24-spotspraying/\n│   ├── 2025-10-18-jax_distributed/\n│   ├── 2025-11-23-GSPO_GRPO/\n│   ├── 2025-3-20-cudp/\n│   └── 2026-01-03-mHC/\n├── about.qmd\n├── cv.qmd\n├── projects.qmd\n├── publications.qmd\n├── repositories.qmd\n├── teaching.qmd\n├── files/\n│   ├── images/              # All migrated images (61 files)\n│   ├── cv.yml\n│   ├── coauthors.yml\n│   ├── repositories.yml\n│   └── venues.yml\n├── references.bib           # Bibliography\n├── BUILD_INSTRUCTIONS.md    # Build guide\n└── docs/                    # Output (after running quarto render)\n\n\n\nTo build and deploy your site:\n\nInstall Quarto (if not already done):\nbrew install quarto\n# OR download from https://quarto.org/docs/get-started/\nBuild the site:\ncd /Users/rishav/website/drganghe.github.io\nquarto render\nPreview locally:\nquarto preview\n# Opens at http://localhost:3456\nDeploy to GitHub Pages:\n\nPush the repository to GitHub\nIn GitHub Pages settings, set source to “docs” directory\nSite will be published at https://rish-av.github.io\n\n\n\n\n\n✓ All Jekyll front matter converted to Quarto format ✓ All markdown content preserved ✓ All images copied and paths updated ✓ Bibliography data migrated completely ✓ Site metadata and navigation configured ✓ Google Analytics ID preserved ✓ Social links configured ✓ Responsive design maintained\n\n\n\n\nSee BUILD_INSTRUCTIONS.md for detailed build commands\nReview _quarto.yml to customize if needed\nRun quarto preview to test locally\nPush to GitHub and enable Pages deployment\n\n\n\n\nYou can customize the site by editing:\n\n**_quarto.yml** - Site title, description, navigation, colors\nindex.qmd - Homepage content and layout\nCustom styling - Modify CSS in format section\n\nExample _quarto.yml customizations:\nwebsite:\n  title: \"Your New Title\"\n  description: \"Your site description\"\n  \nformat:\n  html:\n    theme: cosmo  # Change theme (cosmo, darkly, etc.)\n\nMigration Date: January 4, 2026 Status: ✓ Complete and Ready to Build Total Files: 5 posts + 6 pages + 61 images + config Build System: Quarto 1.8.26+"
  },
  {
    "objectID": "MIGRATION_COMPLETE.html#summary",
    "href": "MIGRATION_COMPLETE.html#summary",
    "title": "Jekyll to Quarto Migration - Complete ✓",
    "section": "",
    "text": "Your Jekyll site has been successfully converted to Quarto format!\nLocation: /Users/rishav/website/drganghe.github.io"
  },
  {
    "objectID": "MIGRATION_COMPLETE.html#what-was-migrated",
    "href": "MIGRATION_COMPLETE.html#what-was-migrated",
    "title": "Jekyll to Quarto Migration - Complete ✓",
    "section": "",
    "text": "✓ 5 Blog Posts (2024-2026)\n\nspotspraying.md\njax_distributed.md\nGSPO_GRPO.md\ncudp.md\nmHC.md\n\n✓ 6 Static Pages\n\nabout.qmd\ncv.qmd\nprojects.qmd\npublications.qmd\nrepositories.qmd\nteaching.qmd\n\n\n\n\n\n\n✓ 61 Images (profiles, thumbnails, research figures)\n✓ Bibliography (references.bib with full BibTeX entries)\n✓ Data Files (CV, coauthors, repositories, venues YAML)\n\n\n\n\n\n✓ **_quarto.yml** - Full site configuration with:\n\nSite metadata and description\nNavigation structure (About, Blog, Projects, Publications)\nSocial media links (GitHub, Twitter, LinkedIn)\nGoogle Analytics (G-1ZTF5BQFYQ)\nRSS feed support\nOpen Graph and Twitter card metadata"
  },
  {
    "objectID": "MIGRATION_COMPLETE.html#file-conversion",
    "href": "MIGRATION_COMPLETE.html#file-conversion",
    "title": "Jekyll to Quarto Migration - Complete ✓",
    "section": "",
    "text": "Posts: - Jekyll: _posts/YYYY-MM-DD-title.md - Quarto: posts/YYYY-MM-DD-title/index.qmd\nPages: - Jekyll: _pages/about.md (with Jekyll-specific YAML) - Quarto: about.qmd (with Quarto-specific YAML)\nImages: - Jekyll: assets/img/ - Quarto: files/images/\nBibliography: - Jekyll: _bibliography/papers.bib - Quarto: references.bib\n\n\n\nJekyll example:\n---\nlayout: post\ntitle: \"Title\"\ndate: 2026-01-03\ntags: [ai, research]\nthumbnail: assets/img/image.png\n---\nQuarto equivalent:\n---\ntitle: \"Title\"\ndate: \"2026-01-03\"\ncategories: [ai, research]\nimage: \"/files/images/image.png\"\n---"
  },
  {
    "objectID": "MIGRATION_COMPLETE.html#directory-structure",
    "href": "MIGRATION_COMPLETE.html#directory-structure",
    "title": "Jekyll to Quarto Migration - Complete ✓",
    "section": "",
    "text": "drganghe.github.io/\n├── _quarto.yml              # Main configuration\n├── index.qmd                # Homepage with about section\n├── posts.qmd                # Blog listing page\n├── posts/                   # Blog posts (67 total)\n│   ├── 2024-12-24-spotspraying/\n│   ├── 2025-10-18-jax_distributed/\n│   ├── 2025-11-23-GSPO_GRPO/\n│   ├── 2025-3-20-cudp/\n│   └── 2026-01-03-mHC/\n├── about.qmd\n├── cv.qmd\n├── projects.qmd\n├── publications.qmd\n├── repositories.qmd\n├── teaching.qmd\n├── files/\n│   ├── images/              # All migrated images (61 files)\n│   ├── cv.yml\n│   ├── coauthors.yml\n│   ├── repositories.yml\n│   └── venues.yml\n├── references.bib           # Bibliography\n├── BUILD_INSTRUCTIONS.md    # Build guide\n└── docs/                    # Output (after running quarto render)"
  },
  {
    "objectID": "MIGRATION_COMPLETE.html#building-the-site",
    "href": "MIGRATION_COMPLETE.html#building-the-site",
    "title": "Jekyll to Quarto Migration - Complete ✓",
    "section": "",
    "text": "To build and deploy your site:\n\nInstall Quarto (if not already done):\nbrew install quarto\n# OR download from https://quarto.org/docs/get-started/\nBuild the site:\ncd /Users/rishav/website/drganghe.github.io\nquarto render\nPreview locally:\nquarto preview\n# Opens at http://localhost:3456\nDeploy to GitHub Pages:\n\nPush the repository to GitHub\nIn GitHub Pages settings, set source to “docs” directory\nSite will be published at https://rish-av.github.io"
  },
  {
    "objectID": "MIGRATION_COMPLETE.html#migration-quality-checks",
    "href": "MIGRATION_COMPLETE.html#migration-quality-checks",
    "title": "Jekyll to Quarto Migration - Complete ✓",
    "section": "",
    "text": "✓ All Jekyll front matter converted to Quarto format ✓ All markdown content preserved ✓ All images copied and paths updated ✓ Bibliography data migrated completely ✓ Site metadata and navigation configured ✓ Google Analytics ID preserved ✓ Social links configured ✓ Responsive design maintained"
  },
  {
    "objectID": "MIGRATION_COMPLETE.html#next-steps",
    "href": "MIGRATION_COMPLETE.html#next-steps",
    "title": "Jekyll to Quarto Migration - Complete ✓",
    "section": "",
    "text": "See BUILD_INSTRUCTIONS.md for detailed build commands\nReview _quarto.yml to customize if needed\nRun quarto preview to test locally\nPush to GitHub and enable Pages deployment"
  },
  {
    "objectID": "MIGRATION_COMPLETE.html#customization-options",
    "href": "MIGRATION_COMPLETE.html#customization-options",
    "title": "Jekyll to Quarto Migration - Complete ✓",
    "section": "",
    "text": "You can customize the site by editing:\n\n**_quarto.yml** - Site title, description, navigation, colors\nindex.qmd - Homepage content and layout\nCustom styling - Modify CSS in format section\n\nExample _quarto.yml customizations:\nwebsite:\n  title: \"Your New Title\"\n  description: \"Your site description\"\n  \nformat:\n  html:\n    theme: cosmo  # Change theme (cosmo, darkly, etc.)\n\nMigration Date: January 4, 2026 Status: ✓ Complete and Ready to Build Total Files: 5 posts + 6 pages + 61 images + config Build System: Quarto 1.8.26+"
  },
  {
    "objectID": "projects/compiler/index.html",
    "href": "projects/compiler/index.html",
    "title": "Compiler Construction",
    "section": "",
    "text": "Resources\n\nCode Repository: GitHub Link"
  },
  {
    "objectID": "projects/projec1/index.html",
    "href": "projects/projec1/index.html",
    "title": "Unsupervised Cross Spectral Stereo Matching",
    "section": "",
    "text": "Resources\n\nCode Repository: GitHub Link"
  },
  {
    "objectID": "projects/hsid/index.html",
    "href": "projects/hsid/index.html",
    "title": "HSID-CNN",
    "section": "",
    "text": "Resources\n\nCode Repository: GitHub Link"
  },
  {
    "objectID": "rish-av.github.io/CONTRIBUTING.html",
    "href": "rish-av.github.io/CONTRIBUTING.html",
    "title": "Contributing to al-folio",
    "section": "",
    "text": "Thank you for considering to contribute to al-folio!\n\n\nWe welcome your pull requests (PRs). For minor fixes (e.g., documentation improvements), feel free to submit a PR directly. If you would like to implement a new feature or a bug, please make sure you (or someone else) has opened an appropriate issue first; in your PR, please mention the issue it addresses.\n\n\n\nWe use GitHub issues to track bugs and feature requests. Before submitting an issue, please make sure:\n\nYou have read the FAQ section of the README and your question is NOT addressed there.\nYou have done your best to ensure that your issue is NOT a duplicate of one of the previous issues.\nYour issue is either a bug (unexpected/undesirable behavior) or a feature request. If it is just a question, please ask it in the Discussions forum.\n\nWhen submitting an issue, please make sure to use the appropriate template.\n\n\n\nBy contributing to al-folio, you agree that your contributions will be licensed under the LICENSE file in the root directory of the source tree."
  },
  {
    "objectID": "rish-av.github.io/CONTRIBUTING.html#pull-requests",
    "href": "rish-av.github.io/CONTRIBUTING.html#pull-requests",
    "title": "Contributing to al-folio",
    "section": "",
    "text": "We welcome your pull requests (PRs). For minor fixes (e.g., documentation improvements), feel free to submit a PR directly. If you would like to implement a new feature or a bug, please make sure you (or someone else) has opened an appropriate issue first; in your PR, please mention the issue it addresses."
  },
  {
    "objectID": "rish-av.github.io/CONTRIBUTING.html#issues",
    "href": "rish-av.github.io/CONTRIBUTING.html#issues",
    "title": "Contributing to al-folio",
    "section": "",
    "text": "We use GitHub issues to track bugs and feature requests. Before submitting an issue, please make sure:\n\nYou have read the FAQ section of the README and your question is NOT addressed there.\nYou have done your best to ensure that your issue is NOT a duplicate of one of the previous issues.\nYour issue is either a bug (unexpected/undesirable behavior) or a feature request. If it is just a question, please ask it in the Discussions forum.\n\nWhen submitting an issue, please make sure to use the appropriate template."
  },
  {
    "objectID": "rish-av.github.io/CONTRIBUTING.html#license",
    "href": "rish-av.github.io/CONTRIBUTING.html#license",
    "title": "Contributing to al-folio",
    "section": "",
    "text": "By contributing to al-folio, you agree that your contributions will be licensed under the LICENSE file in the root directory of the source tree."
  },
  {
    "objectID": "BUILD_INSTRUCTIONS.html",
    "href": "BUILD_INSTRUCTIONS.html",
    "title": "Quarto Build Instructions for Rishav’s Website",
    "section": "",
    "text": "Quarto installation can be complex. Here are the recommended approaches:\n\n\n\nVisit: https://quarto.org/docs/get-started/\nDownload the macOS (arm64) installer for your system\nFollow the installation wizard\n\n\n\n\nbrew install quarto\n\n\n\n# For macOS ARM64 (Apple Silicon)\ncd /tmp\ncurl -L -o quarto.tar.gz https://github.com/quarto-dev/quarto-cli/releases/download/v1.8.26/quarto-1.8.26-macos-aarch64.tar.gz\ntar -xzf quarto.tar.gz\nsudo mv quarto-*/bin/quarto /usr/local/bin/\n\n\n\n\nOnce Quarto is installed:\ncd /Users/rishav/website/drganghe.github.io\n\n# Render the entire site\nquarto render\n\n# Or for preview mode with live reload\nquarto preview\nThe built site will be generated in the docs/ directory.\n\n\n\n✓ Blog Posts: 5 migrated posts ✓ Pages: 6 migrated pages (about, CV, projects, publications, etc.) ✓ Assets: 61 images ✓ Bibliography: Complete with references.bib ✓ Configuration: Full _quarto.yml with navigation and settings\n\n\n\ndrganghe.github.io/\n├── _quarto.yml          # Main configuration\n├── index.qmd            # Homepage\n├── about.qmd\n├── cv.qmd\n├── projects.qmd\n├── publications.qmd\n├── posts.qmd            # Blog listing page\n├── posts/               # 67 post directories\n│   ├── 2024-12-24-spotspraying/\n│   ├── 2025-10-18-jax_distributed/\n│   ├── 2025-11-23-GSPO_GRPO/\n│   ├── 2025-3-20-cudp/\n│   └── 2026-01-03-mHC/\n├── files/\n│   ├── images/          # 61 migrated images\n│   ├── cv.yml\n│   ├── coauthors.yml\n│   ├── repositories.yml\n│   └── venues.yml\n├── references.bib       # Bibliography\n└── docs/                # Output directory (after build)\n\n\n\nThe site is configured with: - Title: “Rishav’s Website” - Navigation: About, Blog, Projects, Publications - Social Links: GitHub, Twitter, LinkedIn - Analytics: Google Analytics (G-1ZTF5BQFYQ) - Theme: Modern Quarto responsive theme\n\n\n\nIf you encounter issues:\n\nQuarto not found: Ensure installation is complete: quarto --version\nImage paths broken: Check that image URLs are using /files/images/ format\nPosts not showing: Ensure posts.qmd exists and lists the posts directory\nBuild errors: Check console output for specific file/path issues\n\n\n\n\n\nInstall Quarto\nRun quarto render\nDeploy docs/ directory to GitHub Pages or your hosting\nUpdate GitHub Pages settings to serve from /docs branch"
  },
  {
    "objectID": "BUILD_INSTRUCTIONS.html#installation",
    "href": "BUILD_INSTRUCTIONS.html#installation",
    "title": "Quarto Build Instructions for Rishav’s Website",
    "section": "",
    "text": "Quarto installation can be complex. Here are the recommended approaches:\n\n\n\nVisit: https://quarto.org/docs/get-started/\nDownload the macOS (arm64) installer for your system\nFollow the installation wizard\n\n\n\n\nbrew install quarto\n\n\n\n# For macOS ARM64 (Apple Silicon)\ncd /tmp\ncurl -L -o quarto.tar.gz https://github.com/quarto-dev/quarto-cli/releases/download/v1.8.26/quarto-1.8.26-macos-aarch64.tar.gz\ntar -xzf quarto.tar.gz\nsudo mv quarto-*/bin/quarto /usr/local/bin/"
  },
  {
    "objectID": "BUILD_INSTRUCTIONS.html#building-the-site",
    "href": "BUILD_INSTRUCTIONS.html#building-the-site",
    "title": "Quarto Build Instructions for Rishav’s Website",
    "section": "",
    "text": "Once Quarto is installed:\ncd /Users/rishav/website/drganghe.github.io\n\n# Render the entire site\nquarto render\n\n# Or for preview mode with live reload\nquarto preview\nThe built site will be generated in the docs/ directory."
  },
  {
    "objectID": "BUILD_INSTRUCTIONS.html#verification",
    "href": "BUILD_INSTRUCTIONS.html#verification",
    "title": "Quarto Build Instructions for Rishav’s Website",
    "section": "",
    "text": "✓ Blog Posts: 5 migrated posts ✓ Pages: 6 migrated pages (about, CV, projects, publications, etc.) ✓ Assets: 61 images ✓ Bibliography: Complete with references.bib ✓ Configuration: Full _quarto.yml with navigation and settings"
  },
  {
    "objectID": "BUILD_INSTRUCTIONS.html#site-structure",
    "href": "BUILD_INSTRUCTIONS.html#site-structure",
    "title": "Quarto Build Instructions for Rishav’s Website",
    "section": "",
    "text": "drganghe.github.io/\n├── _quarto.yml          # Main configuration\n├── index.qmd            # Homepage\n├── about.qmd\n├── cv.qmd\n├── projects.qmd\n├── publications.qmd\n├── posts.qmd            # Blog listing page\n├── posts/               # 67 post directories\n│   ├── 2024-12-24-spotspraying/\n│   ├── 2025-10-18-jax_distributed/\n│   ├── 2025-11-23-GSPO_GRPO/\n│   ├── 2025-3-20-cudp/\n│   └── 2026-01-03-mHC/\n├── files/\n│   ├── images/          # 61 migrated images\n│   ├── cv.yml\n│   ├── coauthors.yml\n│   ├── repositories.yml\n│   └── venues.yml\n├── references.bib       # Bibliography\n└── docs/                # Output directory (after build)"
  },
  {
    "objectID": "BUILD_INSTRUCTIONS.html#configuration",
    "href": "BUILD_INSTRUCTIONS.html#configuration",
    "title": "Quarto Build Instructions for Rishav’s Website",
    "section": "",
    "text": "The site is configured with: - Title: “Rishav’s Website” - Navigation: About, Blog, Projects, Publications - Social Links: GitHub, Twitter, LinkedIn - Analytics: Google Analytics (G-1ZTF5BQFYQ) - Theme: Modern Quarto responsive theme"
  },
  {
    "objectID": "BUILD_INSTRUCTIONS.html#troubleshooting",
    "href": "BUILD_INSTRUCTIONS.html#troubleshooting",
    "title": "Quarto Build Instructions for Rishav’s Website",
    "section": "",
    "text": "If you encounter issues:\n\nQuarto not found: Ensure installation is complete: quarto --version\nImage paths broken: Check that image URLs are using /files/images/ format\nPosts not showing: Ensure posts.qmd exists and lists the posts directory\nBuild errors: Check console output for specific file/path issues"
  },
  {
    "objectID": "BUILD_INSTRUCTIONS.html#next-steps",
    "href": "BUILD_INSTRUCTIONS.html#next-steps",
    "title": "Quarto Build Instructions for Rishav’s Website",
    "section": "",
    "text": "Install Quarto\nRun quarto render\nDeploy docs/ directory to GitHub Pages or your hosting\nUpdate GitHub Pages settings to serve from /docs branch"
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html",
    "href": "posts/2026-01-03-mHC/index.html",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "",
    "text": "In the world of Deep Learning, we have a “Golden Rule” that has allowed models to evolve from the image classifiers of 2015 to the reasoning giants of today: the Identity Mapping. For a very long time, this was seen a standard which did not need any further engineering but DeepSeek’s team begs to differ here.\nThink of a standard Residual Network (ResNet) as a single-lane highway. The “Identity Mapping” is the rule that allows traffic (information) to flow straight through from start to finish without stopping. Mathematically, we express this as:\n\\[x_{l+1} = x_l + F(x_l, W_l)\\]\n Figure 1(a-c) from the paper - shows residual connection, HC architecture, and mHC architecture\n\n\nThis simple addition was a key enabler in scaling deep networks. The identity mapping allows gradients to flow cleanly through hundreds of layers, which became essential as architectures grew from models like ResNet-50 (~25 million parameters) to modern LLMs with hundreds of billions of parameters.\nThe Physics: During training, when the model looks backward to learn (backpropagation), the gradient flows through the identity path (the x_l term) without any modification, ensuring the signal doesn’t vanish or explode, even after traveling through hundreds of layers.\nConcrete Example: Imagine you’re training a 100-layer network. During backpropagation, gradients need to flow from layer 100 back to layer 1. Without the identity mapping, each layer might multiply the gradient by some value like 0.9. After 100 layers: 0.9^100 ≈ 0.0000266. Your gradient has essentially vanished - the early layers can’t learn anything. With the identity mapping, there’s always a direct gradient path back to early layers, preserving the signal."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#part-1-the-golden-rule-of-deep-learning",
    "href": "posts/2026-01-03-mHC/index.html#part-1-the-golden-rule-of-deep-learning",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "",
    "text": "In the world of Deep Learning, we have a “Golden Rule” that has allowed models to evolve from the image classifiers of 2015 to the reasoning giants of today: the Identity Mapping. For a very long time, this was seen a standard which did not need any further engineering but DeepSeek’s team begs to differ here.\nThink of a standard Residual Network (ResNet) as a single-lane highway. The “Identity Mapping” is the rule that allows traffic (information) to flow straight through from start to finish without stopping. Mathematically, we express this as:\n\\[x_{l+1} = x_l + F(x_l, W_l)\\]\n Figure 1(a-c) from the paper - shows residual connection, HC architecture, and mHC architecture\n\n\nThis simple addition was a key enabler in scaling deep networks. The identity mapping allows gradients to flow cleanly through hundreds of layers, which became essential as architectures grew from models like ResNet-50 (~25 million parameters) to modern LLMs with hundreds of billions of parameters.\nThe Physics: During training, when the model looks backward to learn (backpropagation), the gradient flows through the identity path (the x_l term) without any modification, ensuring the signal doesn’t vanish or explode, even after traveling through hundreds of layers.\nConcrete Example: Imagine you’re training a 100-layer network. During backpropagation, gradients need to flow from layer 100 back to layer 1. Without the identity mapping, each layer might multiply the gradient by some value like 0.9. After 100 layers: 0.9^100 ≈ 0.0000266. Your gradient has essentially vanished - the early layers can’t learn anything. With the identity mapping, there’s always a direct gradient path back to early layers, preserving the signal."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#part-2-the-innovation---hyper-connections-hc",
    "href": "posts/2026-01-03-mHC/index.html#part-2-the-innovation---hyper-connections-hc",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "Part 2: The Innovation - Hyper-Connections (HC)",
    "text": "Part 2: The Innovation - Hyper-Connections (HC)\nRecent research introduced “Hyper-Connections” (HC) to upgrade this highway. HC widens the road by an expansion rate of n (typically n=4).\n\nUnderstanding the Dimensions\nIt’s important to visualize this correctly. We aren’t just making the single vector 4 times longer. Instead, we are building 4 parallel streams (lanes) that run side-by-side.\nStandard: 1 Stream of size d (e.g., 4096)\nHC (n=4): 4 Streams, each of size d\n\\[M^{(t)} = T_r(T_c(M^{(t-1)}))\\]\nIntuition: Think of it like having 4 different “perspectives” on the same information. One stream might specialize in syntax, another in semantics, a third in world knowledge, and the fourth in reasoning patterns. By having these parallel streams, the model can maintain multiple specialized representations simultaneously. (See Figure 1(b) above)\nHC builds complex interchanges to mix the traffic between these lanes:\n\\[\\prod_{i=1}^{100} H^{res}_{100-i} \\text{ is ALSO doubly stochastic}\\]\nWhere:\n\nreads: \\(H^{pre}_l \\in \\mathbb{R}^{1 \\times n}\\) reads from the streams into the layer (aggregates 4 streams → 1 input)\nwrites: \\(H^{post}_l \\in \\mathbb{R}^{1 \\times n}\\) writes the layer output back to the streams (distributes 1 output → 4 streams)\nmixes: \\(H^{res}_l \\in \\mathbb{R}^{n \\times n}\\) mixes information between the parallel streams (4 streams → 4 streams)\n\nConcrete Example of Mixing: Suppose stream 1 contains grammatical information and stream 2 contains semantic information. The mixing matrix \\(H^{res}_l\\) might have learned that for certain tasks, you need 70% grammar + 30% semantics in the first output stream, and 20% grammar + 80% semantics in the second output stream. This is what “mixing” means - creating weighted combinations of the specialized streams.\nThis diversification drastically increases the model’s capacity to learn and reason by allowing different “lanes” to specialize in different features."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#part-3-the-problem---the-crash",
    "href": "posts/2026-01-03-mHC/index.html#part-3-the-problem---the-crash",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "Part 3: The Problem - The Crash",
    "text": "Part 3: The Problem - The Crash\nBut there was a catch. HC removed the traffic rules. Without the safety of the Identity Mapping, the mixing matrices could arbitrarily multiply the signal strength.\n\nVisualizing the Failure\nImagine a graph where the X-axis is the Layer Number (Depth) and the Y-axis is the Signal Variance (Energy).\nStable ResNet: The line is flat. The energy stays constant at 1.0 from Layer 1 to Layer 100, effectively following the “speed limit.”\nUnstable HC: The line looks like an exponential “rocket launch.” It starts small, but the compound effect causes it to explode.\nThe Math Behind the Explosion: In standard ResNet, after 100 layers you have:\n\\[\\|H^{res}_l x\\|_2 \\leq \\|x\\|_2\\]\nThe x_0 term is unchanged - it’s literally the same vector that entered layer 0.\nIn HC, after 100 layers you have:\n\\[\\mathcal{M}_{res} = \\{H^{res}_l \\in \\mathbb{R}^{n \\times n} \\mid H^{res}_l \\mathbf{1}_n = \\mathbf{1}_n, \\mathbf{1}_n^T H^{res}_l = \\mathbf{1}_n^T, H^{res}_l \\geq 0\\}\\]\nThat product of 100 matrices is the problem. Even if each H^{res}_l has a maximum eigenvalue of just 1.02 (only 2% above unity), after 100 layers: 1.02^100 ≈ 7.24. Your signal has amplified by 7×! And with uncontrolled matrices, you might see eigenvalues of 1.1 or higher, leading to: 1.1^100 ≈ 13,780 - complete explosion.\n Figure 3 from the paper - shows the dramatic explosion in gradient magnitude for HC reaching nearly 10^4\n Figure 2 from the paper - shows the training instability and loss spikes in HC\nWhy This Breaks Training: When gradients explode to magnitudes of 10^4, the optimizer (Adam, SGD, etc.) receives nonsensical update signals. It’s like trying to park a car when the speedometer randomly jumps between 5 mph and 5,000 mph - you have no reliable information to make good decisions."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#part-4-the-rules-of-the-road-the-math",
    "href": "posts/2026-01-03-mHC/index.html#part-4-the-rules-of-the-road-the-math",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "Part 4: The Rules of the Road (The Math)",
    "text": "Part 4: The Rules of the Road (The Math)\nTo fix the instability caused by Hyper-Connections, the authors had to impose strict “traffic rules” on the mixing matrices. They restrict \\(H^{res}_l\\) to the Birkhoff Polytope (\\(\\mathcal{M}_{res}\\)), the geometric set of all Doubly Stochastic matrices:\n\\[x_100 = \\left(\\prod_{i=1}^{100} H^{res}_{100-i}\\right) x_0 + ...\\]\nWhere \\(\\mathbf{1}_n\\) is a column vector of ones. While this looks abstract, it translates to two simple physical rules:\n\nRule 1: Row Stochasticity\n\\[H^{res}_l \\mathbf{1}_n = \\mathbf{1}_n\\]\nThe sum of weights for each outgoing stream is exactly 1.\nWhat this means: Imagine you have 100 “units of energy” in stream 1. Row stochasticity says: “You can redistribute this energy to streams 1, 2, 3, 4 in any proportion you want (e.g., 25% to each, or 70% to stream 1 and 10% to each of the others), BUT the total output must still be 100 units.” You cannot create energy out of nowhere.\nExample:\nStream 1 (100 units) → [0.7×100 → Stream 1, 0.1×100 → Stream 2, \n                         0.1×100 → Stream 3, 0.1×100 → Stream 4]\nTotal output = 70 + 10 + 10 + 10 = 100 units\nThis prevents the “Rocket Launch” effect by ensuring the total signal energy cannot be amplified.\n\n\nRule 2: Column Stochasticity\n\\[\\mathbf{1}_n^T H^{res}_l = \\mathbf{1}_n^T\\]\nThe sum of weights for each incoming stream is exactly 1.\nWhat this means: For any output stream, the contributions from all input streams must sum to exactly 1. This ensures every input feature is fully utilized and not “lost.”\nExample:\nOutput Stream 1 receives:\n  0.4 from Input Stream 1\n  0.3 from Input Stream 2\n  0.2 from Input Stream 3\n  0.1 from Input Stream 4\nTotal = 0.4 + 0.3 + 0.2 + 0.1 = 1.0\nThis prevents vanishing gradients by ensuring no stream is “forgotten” or “zeroed out.”\n\n\nThe Mathematical Guarantee: Conservation Properties\nWhen a matrix is doubly stochastic, it provides powerful conservation guarantees. The result is a convex combination - a weighted average where the weights sum to 1.\nWhy this matters:\n\\[\\|H^{res}_l x\\|_1 = \\|x\\|_1\\]\nDoubly stochastic matrices exactly preserve the 1-norm (sum of absolute values) of any vector. This is stronger than just bounding the norm - it’s perfect conservation. Additionally, they bound the 2-norm: \\(\\|H^{res}_l x\\|_2 \\leq \\|x\\|_2\\). This dual property ensures that the signal energy is conserved during propagation, preventing both explosions and vanishing.\nFurthermore - the crucial closure property: If you multiply two doubly stochastic matrices together, you get another doubly stochastic matrix! This means:\n\\[\\prod_{i=1}^{100} H^{res}_{100-i} \\text{ is ALSO doubly stochastic}\\]\nSo even after 100 layers, the composite mapping still respects the “speed limit” of 1.0.\nWhy doubly stochastic instead of spectral normalization? While spectral normalization (constraining maximum singular value to 1) also prevents explosions, doubly stochastic matrices offer a key advantage: they allow flexible mixing between streams while preserving total energy. Spectral normalization only preserves norm without enabling the rich cross-stream information exchange that makes multi-stream architectures powerful. The doubly stochastic constraint provides stability AND expressivity.\n\n\nThe Enforcer: Sinkhorn-Knopp\nWe cannot train these constrained parameters directly using standard gradient descent. Instead, we train a “messy,” unconstrained parameter matrix \\(\\tilde{H}^{res}_l\\) and force it to follow the rules during the forward pass using the Sinkhorn-Knopp algorithm.\nThis algorithm acts like a strict accountant. It alternates between normalizing rows and columns:\n\\[M^{(t)} = T_r(T_c(M^{(t-1)}))\\]\nWhere \\(T_r\\) normalizes rows (divide each row by its sum) and \\(T_c\\) normalizes columns (divide each column by its sum).\nStep-by-step example:\nStarting matrix (after exp to make it positive):\n[1.0  3.0]\n[2.0 10.0]\nRow sums: [4, 12], Column sums: [3, 13]\nIteration 1 - Normalize rows:\n[0.25  0.75]  (row 1 / 4)\n[0.17  0.83]  (row 2 / 12)\nColumn sums: [0.42, 1.58]\nIteration 1 - Normalize columns:\n[0.60  0.47]  (col 1 / 0.42, col 2 / 1.58)\n[0.40  0.53]\nRow sums: [1.07, 0.93]\nAfter ~20 iterations, both constraints are satisfied to high precision! The Birkhoff polytope is a convex set, and Sinkhorn-Knopp is performing an alternating projection between two linear constraints, guaranteed to converge.\nWhat Do These Matrices Actually Look Like?  Figure 8 from the paper - Visualizations of learned mappings. Top row shows unstable HC matrices with extreme values. Bottom row shows mHC’s doubly stochastic matrices with controlled, balanced weights.\nThis figure reveals the difference in practice:\nHC (top row): The unconstrained matrices show extreme values (ranging from -259 to +509 in composite mappings). When you see a row sum of 18.73 or -15.29, that’s the “rocket launch” happening - signals being amplified or attenuated wildly. Notice how the forward signal gain and backward gradient gain (labeled on axes) deviate massively from 1.0.\nmHC (bottom row): Every matrix is beautifully balanced. Individual entries vary (showing the network learned something!), but crucially: row sums ≈ 1.0, column sums ≈ 1.0. Even in the composite mapping ∏ P_Mres(H^res) after 60 layers, the gains stay near 1.0. The Sinkhorn constraint is working exactly as designed."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#this-is-the-before-and-after-of-manifold-constraints---transforming-chaos-into-controlled-stable-mixing.",
    "href": "posts/2026-01-03-mHC/index.html#this-is-the-before-and-after-of-manifold-constraints---transforming-chaos-into-controlled-stable-mixing.",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "This is the “before and after” of manifold constraints - transforming chaos into controlled, stable mixing.",
    "text": "This is the “before and after” of manifold constraints - transforming chaos into controlled, stable mixing."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#part-5-cheating-the-memory-wall",
    "href": "posts/2026-01-03-mHC/index.html#part-5-cheating-the-memory-wall",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "Part 5: Cheating the Memory Wall",
    "text": "Part 5: Cheating the Memory Wall\nThe mathematical elegance of the Birkhoff Polytope comes with a heavy price tag. By setting the expansion rate to n=4, the authors effectively widened the highway by four times, creating a massive data pile-up.\n\nQuantifying the Cost\nConsider training a standard 100-layer Large Language Model with hidden dimension d = 4096, batch size = 1 million tokens, and FP16 precision (2 bytes per number).\nStandard Model (n=1):\nMemory = 100 layers × 1M tokens × 4096 dim × 2 bytes\n       = 819.2 GB ≈ 800 GB\nHyper-Connected Model (n=4):\nMemory = 100 layers × 1M tokens × (4 × 4096) dim × 2 bytes\n       = 3,276.8 GB ≈ 3.2 TB\nThis 3.2 Terabytes is the Memory Wall. For reference, an NVIDIA H100 GPU has 80 GB of HBM memory. You’d need 41 H100 GPUs just to hold the activations!\n\n\nWhy is mHC Memory-Heavy but Compute-Light?\nLet’s break down the operations to understand this crucial trade-off.\nComputational Complexity (FLOPs): For a mixing operation \\(H^{res}_l x_l\\) where \\(H^{res}_l \\in \\mathbb{R}^{4 \\times 4}\\) and \\(x_l \\in \\mathbb{R}^{4 \\times 4096}\\):\nFLOPs = 2 × 4 × 4 × 4096 ≈ 131K operations per token\nCompare this to the FFN layer:\nFLOPs = 2 × 4096 × (4 × 4096) ≈ 134M operations per token\nThe mHC mixing is 1000× cheaper in terms of compute! It’s literally just multiplying a tiny 4×4 matrix by the streams. This is “lightweight math.”\nMemory Complexity (Bytes): But we need to store those 4 expanded streams:\nMemory = 4 streams × 4096 dim × 2 bytes = 32,768 bytes per token\nvs.\nStandard = 1 stream × 4096 dim × 2 bytes = 8,192 bytes per token\n4× more memory, but the computation is negligible. Modern GPUs are memory-bandwidth limited, not compute-limited. Reading 3.2 TB of data from memory takes over 1 second, even if the actual math only takes 0.1 seconds! This is why the “recomputation trick” works - we trade a cheap 0.1s of extra compute to avoid paying the expensive 1s+ of memory I/O.\n\n\nSolution 1: Kernel Fusion (The “Countertop” Strategy)\nThe first bottleneck is speed. The Sinkhorn algorithm requires reading and writing the matrix from memory 40 times (20 iterations × 2 operations per iteration).\nThe Delivery Truck Problem: Without fusion, each iteration loads from slow GPU HBM memory, performs fast computation, then writes back. The frequent memory transfers dominate the execution time.\nWith Kernel Fusion: 1. Load matrix from HBM to GPU registers (on-chip, high bandwidth) 2. Do ALL 20 iterations entirely in registers 3. Write final result back to HBM\nThe solution is Kernel Fusion. By writing a custom kernel (using TileLang), the engineers load the small \\(n \\times n\\) mixing matrix into the GPU’s ultra-fast registers. They perform all 20 iterations of the math right there, without ever sending intermediate results back to main memory. This turns a bandwidth-bound operation into a compute-bound one, significantly reducing the overhead of the Sinkhorn iterations.\n\n\nSolution 2: Selective Recomputing (The “Salt” Trick)\nFusion fixes the speed, but we still have a 3.2 TB storage problem. This is where Selective Recomputing saves the day.\nThe engineers realized that the mHC mixing operation is computationally cheap (lightweight math) but memory-heavy (massive tensors). Therefore, they made the strategic decision to delete the massive output immediately after using it. Instead of paying the “rent” of storing these expanded streams, they pay a tiny “tax” of extra compute to re-calculate them from scratch during the backward pass.\nThe optimal block size:\n\\[x_{l+1} = x_l + F(x_l, W_l)\\]\nThis formula minimizes total memory by balancing two factors: If blocks are too small, you need to store many checkpoints; if blocks are too large, you need huge transient memory for the active block.\nFor a 100-layer model with n=4:\nL_r* = √(4 × 100 / 6) ≈ 8-10 layers per block\nMemory breakdown with \\(L_r = 10\\):\nResident memory (first layer of each block):\n  10 blocks × 1M tokens × 4 × 4096 × 2 bytes = 328 GB\n\nTransient memory (active block during backprop):\n  From Table 3: (n+2)C per layer = (4+2) × 4096 = 24,576 elements\n  10 layers × 1M tokens × 24,576 × 2 bytes = 492 GB\n\nTotal peak: 820 GB (down from 3.2 TB, a 4× reduction!)\n Figure 4 from the paper - shows the communication-computation overlapping strategy\nThis trade-off allows the impossible model to fit onto standard hardware."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#part-6-from-theory-to-code",
    "href": "posts/2026-01-03-mHC/index.html#part-6-from-theory-to-code",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "Part 6: From Theory to Code",
    "text": "Part 6: From Theory to Code\nThis is just the basic variant, I will be writing more about the code in details in coming posts.\nTo realize the savings we calculated - avoiding the 3.2 TB memory explosion - we must translate our “Traffic Rules” and “Salt Trick” into actual code. We can replicate the exact logic using JAX, where JIT (Just-In-Time) handles Kernel Fusion and Checkpointing handles Recomputing.\nNote: The code below shows only the Sinkhorn-Knopp projection and core mixing operation. A complete mHC implementation requires integrating the pre-aggregation (\\(H^{pre}_l\\)) and post-distribution (\\(H^{post}_l\\)) matrices along with the residual function \\(F(x, W)\\). See the paper for full architectural details.\n# 1. THE ENFORCER: Sinkhorn-Knopp Projection\n@jax.jit\ndef sinkhorn_knopp(log_matrix, iterations=20, eps=1e-8):\n    # Line below implements: M^(0) = exp(H̃^res_l)\n    # From Equation 8 (page 9): H^res_l = Sinkhorn-Knopp(H̃^res_l)\n    # Initial step before Equation 9\n    M = jnp.exp(log_matrix)  # ← Initial M^(0)\n    \n    def body_fun(i, mat):\n        # Both lines below implement Equation 9 (page 9):\n        # M^(t) = T_r(T_c(M^(t-1)))\n        \n        mat = mat / (jnp.sum(mat, axis=1, keepdims=True) + eps)  # ← T_r (row normalization)\n        mat = mat / (jnp.sum(mat, axis=0, keepdims=True) + eps)  # ← T_c (column normalization)\n        return mat\n\n    M = jax.lax.fori_loop(0, iterations, body_fun, M)  # ← Iterates Equation 9 for t_max iterations\n    return M\n\n\n# 2. THE LAYER: Putting it together\n@jax.checkpoint \ndef mhc_layer(x, w_res_log):\n    # Line below: Third line of Equation 8 (page 9)\n    # H^res_l = Sinkhorn-Knopp(H̃^res_l)\n    H_res = sinkhorn_knopp(w_res_log)  # ← Equation 8 (third line)\n    \n    # Line below: First term of Equation 3 (page 3)\n    # x_{l+1} = H^res_l x_l + (H^post_l)^T F(H^pre_l x_l, W_l)\n    #           ^^^^^^^^^^^^ (this part only)\n    x_new = jnp.matmul(H_res, x)  # ← First term of Equation 3\n    return x_new\nKey Implementation Details:\n@jax.jit - Kernel Fusion: Tells JAX to compile the function into optimized GPU kernels. The fori_loop iterations are unrolled and fused into a single kernel where all 20 Sinkhorn iterations stay in GPU registers.\n@jax.checkpoint - Selective Recomputing: Discards function outputs after forward pass and automatically recomputes them during backward pass. Saves \\(4 \\times\\) memory at ~0.4% compute cost."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#part-7-so-where-are-we",
    "href": "posts/2026-01-03-mHC/index.html#part-7-so-where-are-we",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "Part 7: So where are we?",
    "text": "Part 7: So where are we?\nWe’ve walked through the theoretical crash of Hyper-Connections and the engineering gymnastics required to fix it. But does the Manifold Constraint actually work in practice? The results from the DeepSeek-V3 technical report offer a definitive “yes.”\n\nThe “Rocket Launch” Confirmed\nRecall our fear that unconstrained Hyper-Connections would lead to exploding gradients. We can now look at the empirical evidence in the paper.\n Figure 7 from the paper - shows mHC maintains stable gradient magnitude around 1.0 while HC explodes to nearly 10^4\nThe blue line (standard Hyper-Connected model, unconstrained) shoots up exponentially, with the backward gradient gain reaching a magnitude of nearly 10^4. This is the “Rocket Launch” in real life - a signal explosion that destroys training stability.\nIn stark contrast, the grey line (mHC model) stays perfectly flat near 1.0. The “Traffic Rules” work. The signal is conserved, allowing the model to train as stably as a standard ResNet, even with the expanded highway.\n\n\nStability at Scale\nThis stability isn’t just a neat chart; it translates directly to training performance.\n Figure 5 from the paper - shows smooth training curves for mHC vs unstable HC\nmHC achieves a loss gap improvement of roughly 0.021 compared to the baseline. In the world of Large Language Models, where improvements are measured in fractions of a percent, this is a massive leap in efficiency.\n\n\nThe Cost of Safety\nThe most impressive part of this story, however, is the price tag. Because of the Kernel Fusion and Selective Recomputing strategies, the paper reports that mHC introduces only a 6.7% increase in training time compared to a standard model.\nThis is the “Free Lunch” of Deep Learning: we get the massive capacity increase of a 4-lane highway for nearly the price of a single-lane road.\n Figure 6 from the paper - shows mHC maintains advantages across different scales\n\n\nPerformance on Real Benchmarks\n\n\n\nBenchmark\nBaseline\nHC\nmHC\n\n\n\n\nBBH (Reasoning)\n43.8\n48.9\n51.0\n\n\nDROP (Reading)\n47.0\n51.6\n53.9\n\n\nGSM8K (Math)\n46.7\n53.2\n53.8\n\n\nMMLU (Knowledge)\n59.0\n63.0\n63.4\n\n\n\nmHC yields comprehensive improvements, consistently outperforming the baseline and surpassing HC on the majority of tasks. Notably, compared to HC, mHC further enhances the model’s reasoning capabilities, delivering performance gains of 2.1% on BBH and 2.3% on DROP."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#part-8-open-questions",
    "href": "posts/2026-01-03-mHC/index.html#part-8-open-questions",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "Part 8: Open Questions",
    "text": "Part 8: Open Questions\nWhile mHC has solved the immediate problem of stabilizing Hyper-Connections, it opens a fascinating door for future research. We used the Birkhoff Polytope because it intuitively maps to “conservation of energy.” But is this the only - or even the best - manifold for deep learning?\n\nAlternative Manifolds: A Brief History\nThe idea of constraining weights to specific manifolds isn’t new. It’s part of a rich research tradition in Geometric Deep Learning. Two notable successes stand out:\nSpectral Normalization (2018) - One of the most successful manifold constraints for GANs (Miyato et al., 2018). They constrain weight matrices to have a maximum singular value of 1, which is geometrically equivalent to projecting onto a specific manifold. Just like mHC, spectral normalization prevents gradient explosions by bounding the Lipschitz constant of the network. It became standard in GAN training because it stabilized discriminator training.\nStiefel Manifold for Orthogonal Weights - The set of all orthonormal matrices (where columns are perpendicular unit vectors). Several papers have explored this:\nOrthogonal RNNs (Henaff et al., 2016) showed that orthogonal recurrent weight matrices help RNNs learn long-term dependencies. The Riemannian Approach to Batch Normalization (Cho & Lee, 2017) used manifold optimization for normalization layers.\nThe connection to mHC: Orthogonality constraints preserve norm (like doubly stochastic matrices) but they force diversity between features rather than mixing. mHC chose doubly stochastic because it allows flexible mixing while preserving total energy. Could the Stiefel manifold work for mHC? It might encourage more specialized stream representations. The challenge is computational cost - orthogonal projections require SVD, which is more expensive than Sinkhorn iterations.\nMathematically, the Birkhoff Polytope is just one of many choices. We could imagine projecting weights onto other manifolds that capture different properties of the loss landscape.\n\n\nEfficiency Questions\nThere is also the question of efficiency. We currently use 20 iterations of Sinkhorn-Knopp to enforce the rules. Could we get away with 5? Or is there a learned approximation - a small neural network that predicts the projection in a single step - that could replace the iterative loop entirely? As models continue to grow, these questions of “Geometric Deep Learning” will likely become the new frontier of optimization."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#conclusion-the-physics-of-the-signal",
    "href": "posts/2026-01-03-mHC/index.html#conclusion-the-physics-of-the-signal",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "Conclusion: The Physics of the Signal",
    "text": "Conclusion: The Physics of the Signal\nThe story of DeepSeek-V3’s Manifold-Constrained Hyper-Connections is a masterclass in modern AI research. It identifies a fundamental structural flaw (the lack of Identity Mapping in expanded streams), solves it with rigorous mathematics (the Birkhoff Polytope), and makes it feasible with hardcore systems engineering (TileLang Fusion and Recomputing).\nFor us developers and researchers, the lesson is clear: Scaling isn’t just about making things bigger. It’s about understanding the “Physics” of the signal. If you can control the flow of information - keeping it on the “Safe Manifold” - you can break the memory wall and build models that are both larger and smarter than we thought possible. (See Figure 1(c) at the beginning of this post)"
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#references-further-reading",
    "href": "posts/2026-01-03-mHC/index.html#references-further-reading",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "References & Further Reading",
    "text": "References & Further Reading\nPaper: mHC: Manifold-Constrained Hyper-Connections (DeepSeek-AI, 2025)\nOriginal HC Paper: Hyper-Connections (Zhu et al., 2024)\nClassic Reference: Identity Mappings in Deep Residual Networks (He et al., 2016)\nManifold Constraints: - Spectral Normalization for GANs (Miyato et al., 2018) - Orthogonal RNNs (Henaff et al., 2016) - Riemannian Batch Normalization (Cho & Lee, 2017)"
  },
  {
    "objectID": "posts/2024-12-24-spotspraying/index.html",
    "href": "posts/2024-12-24-spotspraying/index.html",
    "title": "Precision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System",
    "section": "",
    "text": "Conventional spraying methods apply herbicides uniformly across fields, resulting in excessive chemical use, environmental risks, and increased operational costs. This project implements a precision weeding system for sugarbeet fields by combining semantic segmentation, depth sensing, and advanced model optimization techniques. The system targets only weed-infested areas, reducing herbicide usage and minimizing environmental impact."
  },
  {
    "objectID": "posts/2024-12-24-spotspraying/index.html#introduction",
    "href": "posts/2024-12-24-spotspraying/index.html#introduction",
    "title": "Precision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System",
    "section": "",
    "text": "Conventional spraying methods apply herbicides uniformly across fields, resulting in excessive chemical use, environmental risks, and increased operational costs. This project implements a precision weeding system for sugarbeet fields by combining semantic segmentation, depth sensing, and advanced model optimization techniques. The system targets only weed-infested areas, reducing herbicide usage and minimizing environmental impact."
  },
  {
    "objectID": "posts/2024-12-24-spotspraying/index.html#system-overview",
    "href": "posts/2024-12-24-spotspraying/index.html#system-overview",
    "title": "Precision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System",
    "section": "System Overview",
    "text": "System Overview\nThe system pipeline is summarized in the following flowchart:\nRGB + Depth Images\n       │\n       ▼\nSemantic Segmentation \n(DeepLabV3+ with Channel Attention)\n       │\n       ▼\nDepth Projection & 3D Processing\n       │\n       ▼\nField Actuation (Coordinate Transformation & Control)\nThe process begins with the acquisition of synchronized RGB and depth images. It then uses semantic segmentation to distinguish sugarbeets from weeds, followed by processing to obtain 3D information. Finally, the system translates these 3D coordinates to guide precise actuation in the field."
  },
  {
    "objectID": "posts/2024-12-24-spotspraying/index.html#semantic-segmentation-with-channel-attention",
    "href": "posts/2024-12-24-spotspraying/index.html#semantic-segmentation-with-channel-attention",
    "title": "Precision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System",
    "section": "Semantic Segmentation with Channel Attention",
    "text": "Semantic Segmentation with Channel Attention\n\nOverview\nThe segmentation network is built on a modified DeepLabV3+ architecture that classifies each pixel as sugarbeet, weed, or background. A channel attention module enhances the network’s ability to differentiate between crops and weeds, even under challenging conditions such as variable illumination, occlusions, or subtle texture differences.\n\n\nBenefits of Channel Attention\n\nAdaptive Feature Recalibration:\nGlobal average pooling generates a channel descriptor that is then processed to produce channel-specific weights. This allows the network to emphasize important features while suppressing less informative ones.\nEnhanced Discrimination:\nRecalibrated feature maps improve the network’s ability to distinguish between visually similar classes, leading to a\n\\[\\textbf{19\\% increase in Intersection over Union (IoU)}\\].\nRobustness to Variability:\nDynamic adjustment of feature importance helps maintain consistent performance despite changes in weather, soil, or crop growth stages.\n\n\n\nMathematical Formulation\nThe channel attention module computes a channel descriptor via global average pooling:\n\\[\nz_c = \\frac{1}{H \\times W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} x_c(i,j),\n\\]\nwhich is transformed into channel-specific weights using:\n\\[\ns = \\sigma(W_2 \\cdot \\text{ReLU}(W_1 \\cdot z)),\n\\]\nand applied to rescale the feature maps:\n\\[\n\\tilde{x}_c = s_c \\cdot x_c.\n\\]\n\n\nPyTorch Implementation\nimport torch\nimport torch.nn as nn\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(in_channels, in_channels // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(in_channels // reduction, in_channels, bias=False),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y"
  },
  {
    "objectID": "posts/2024-12-24-spotspraying/index.html#real-time-deployment-and-model-optimization",
    "href": "posts/2024-12-24-spotspraying/index.html#real-time-deployment-and-model-optimization",
    "title": "Precision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System",
    "section": "Real-Time Deployment and Model Optimization",
    "text": "Real-Time Deployment and Model Optimization\n\nEmbedded System Architecture\nDue to remote field conditions and limited internet connectivity, the entire inference pipeline is implemented in C++ and deployed on an NXP i.MX8 board running Yocto Linux with a Hailo-8 accelerator. libtorch is used to integrate the PyTorch models into the C++ environment, ensuring seamless execution in the field.\n\n\nModel Optimization Techniques\n\nINT8 Quantization:\nThe model is optimized for real-time performance using INT8 precision. This involves:\n\nDynamic Quantization:\nimport torch.quantization as quant\n\nmodel_fp32 = CustomSegNet(num_classes=3)\nmodel_int8 = quant.quantize_dynamic(model_fp32, {nn.Conv2d, nn.Linear}, dtype=torch.qint8)\nQuantization Aware Training (QAT):\nDuring QAT, fake quantization layers simulate INT8 precision. The activation quantization function is defined as:\n\\[\n\\hat{x} = Q(x) = s \\cdot \\text{clip}\\left(\\left\\lfloor \\frac{x}{s} \\right\\rceil, -q_{\\min}, q_{\\max}\\right),\n\\]\nwhere \\[s\\] is the scale factor and \\[\\lfloor \\cdot \\rceil\\] denotes rounding. The gradient is approximated via:\n\\[\n\\frac{\\partial L}{\\partial x} \\approx \\frac{\\partial L}{\\partial \\hat{x}}.\n\\]\nThe optimal scale \\[s^*\\] minimizes the Kullback-Leibler divergence between the full-precision gradient distribution \\[P(g)\\] and the quantized distribution \\[Q(g; s)\\]:\n\\[\ns^* = \\arg\\min_{s} \\, \\mathrm{KL}(P(g) \\parallel Q(g; s)) = \\arg\\min_{s} \\sum_{i} P(g_i) \\log \\frac{P(g_i)}{Q(g_i; s)}.\n\\]\n\nModel Freezing and Tracing:\nThe optimized model is frozen and traced using libtorch, resulting in a static computation graph that is exported to the ONNX format:\nimport torch\n\nmodel.eval()  # Freeze layers like batch normalization and dropout\nexample_input = torch.randn(1, 3, 512, 512)\ntraced_model = torch.jit.trace(model, example_input)\ntraced_model.save(\"model_traced.pt\")\n\ntorch.onnx.export(\n    model, \n    example_input, \n    \"model_int8.onnx\", \n    export_params=True,\n    opset_version=11,\n    do_constant_folding=True,\n    input_names=['input'],\n    output_names=['output']\n)\nIntegration with Hailo AI:\nThe traced ONNX model is integrated into the C++ inference pipeline and executed using the Hailo AI inference server, providing optimized INT8 performance for real-time processing."
  },
  {
    "objectID": "posts/2024-12-24-spotspraying/index.html#field-actuation-and-3d-processing",
    "href": "posts/2024-12-24-spotspraying/index.html#field-actuation-and-3d-processing",
    "title": "Precision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System",
    "section": "Field Actuation and 3D Processing",
    "text": "Field Actuation and 3D Processing\nAfter real-time inference, the system must translate the segmentation output into actionable data for field actuation. This section integrates depth projection, 3D localization, and coordinate transformation.\n\nDepth Projection and 3D Localization\nThe 2D segmentation mask is projected into 3D space using the intrinsic camera matrix \\[K\\]:\n\\[\n\\begin{bmatrix}\nX \\\\\nY \\\\\nZ\n\\end{bmatrix} = d(u,v) \\cdot K^{-1}\n\\begin{bmatrix}\nu \\\\\nv \\\\\n1\n\\end{bmatrix},\n\\]\nwhere \\[d(u,v)\\] represents the depth at pixel \\[(u,v)\\]. Clustering the resulting 3D points enables the estimation of weed density and the computation of average spatial coordinates for each weed cluster.\n\n\nCoordinate Transformation for Field Actuation\nTo accurately guide the mechanical weeding nib, the computed 3D coordinates (initially defined in the camera frame) must be transformed into the machine’s coordinate system. This alignment is performed using a pre-computed transformation matrix.\n\nC++ Implementation\n#include &lt;Eigen/Dense&gt;\n\nEigen::Matrix4f getTransformMatrix();  // Retrieves the transformation matrix\n\nEigen::Vector4f getPlantCoordinates(float x, float y, float z) {\n    Eigen::Matrix4f camToActuator = getTransformMatrix();\n    Eigen::Vector4f plantCoordCam(x, y, z, 1);\n    return camToActuator * plantCoordCam;\n}\nThis transformation ensures that the machine’s location in the field is accurately adjusted for effective actuation."
  },
  {
    "objectID": "posts/2024-12-24-spotspraying/index.html#weedicide-usage-calculation-and-motivation",
    "href": "posts/2024-12-24-spotspraying/index.html#weedicide-usage-calculation-and-motivation",
    "title": "Precision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System",
    "section": "Weedicide Usage Calculation and Motivation",
    "text": "Weedicide Usage Calculation and Motivation\n\nWhy Precision Spot Spraying?\nConventional systems apply herbicides uniformly, often wasting chemicals and affecting non-target crops. By precisely identifying weed-infested areas using real-time semantic segmentation and depth estimation, the system applies herbicides only where necessary, reducing chemical waste and environmental impact.\n\n\nCalculating Weedicide Requirements\n\nSegmentation Map \\[S(u,v)\\]:\n\\[\nS(u,v) =\n\\begin{cases}\n1, & \\text{if pixel } (u,v) \\text{ is classified as weed} \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]\nReal-World Area Calculation:\nThe area corresponding to a pixel is approximated by:\n\\[\nA_{pixel}(u,v) = \\left(\\frac{d(u,v)}{f}\\right)^2,\n\\]\nwhere \\[d(u,v)\\] is the depth at pixel \\[(u,v)\\] and \\[f\\] is the focal length.\nTotal Weed-Covered Area \\[A_w\\]:\n\\[\nA_w = \\sum_{u,v} S(u,v) \\cdot \\left(\\frac{d(u,v)}{f}\\right)^2.\n\\]\nHerbicide Amount:\nGiven \\[\\beta\\] as the application rate (liters per square meter), the total herbicide required is:\n\\[\n\\text{Weedicide Amount} = \\beta \\cdot A_w.\n\\]\n\nThis calculation guarantees that herbicide is applied only where needed."
  },
  {
    "objectID": "posts/2024-12-24-spotspraying/index.html#key-results",
    "href": "posts/2024-12-24-spotspraying/index.html#key-results",
    "title": "Precision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System",
    "section": "Key Results",
    "text": "Key Results\n\nImproved Segmentation Accuracy:\nIntegration of the channel attention module resulted in a\n\\[\\textbf{19\\% increase in IoU}\\].\nEnhanced Processing Speed:\nOptimizations increased the frame rate from 1.3 fps to 32 fps on the NVIDIA Xavier AGX platform.\nReal-Time Deployment:\nThe embedded system on an NXP board achieved 23 fps with INT8 quantization."
  },
  {
    "objectID": "posts/2024-12-24-spotspraying/index.html#conclusion",
    "href": "posts/2024-12-24-spotspraying/index.html#conclusion",
    "title": "Precision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System",
    "section": "Conclusion",
    "text": "Conclusion\nThis project demonstrates a comprehensive approach to precision weeding in sugarbeet fields. By combining advanced semantic segmentation with robust model optimizations and efficient embedded deployment, the system offers a scalable solution for targeted weed treatment. The integrated field actuation module—comprising depth projection, 3D localization, and coordinate transformation—ensures that the machine’s location is accurately adjusted for precise herbicide application. This method not only reduces herbicide waste and environmental impact but also offers significant cost savings and improved operational efficiency—key benefits for modern precision agriculture."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Gang He\nMarxe School of Public and International Affairs\nBaruch College\nCity University of New York\n135 East 22nd Street, Room 615\nNew York, NY 10010\nPhone: +1 (646) 660-6716\nE-mail: gang.he at baruch dot cuny dot edu"
  },
  {
    "objectID": "misc/refer-a-friend.html",
    "href": "misc/refer-a-friend.html",
    "title": "Refer a Friend",
    "section": "",
    "text": "Below are some of the services I use that offer refer a friend programs:\n\n\n\nService\nLinks\nWhat you get\nWhat I receive\n\n\n\n\nAiralo\nCode: GANG5991\n$3\n$3\n\n\nAirBnB\nLink\n$5\n$5\n\n\nAmerican Express\nLink\n$75\n$75\n\n\nChase\nLink\n$50\n$50\n\n\nDropbox\nLink\n500MB\n500MB\n\n\nPayPal\nLink\n$10\n$10\n\n\nRobinhood\nLink\n$5\n$5\n\n\nVenmo\nLink\n$5\n$5"
  },
  {
    "objectID": "misc/refer-a-friend.html#refer-a-friend-programs",
    "href": "misc/refer-a-friend.html#refer-a-friend-programs",
    "title": "Refer a Friend",
    "section": "",
    "text": "Below are some of the services I use that offer refer a friend programs:\n\n\n\nService\nLinks\nWhat you get\nWhat I receive\n\n\n\n\nAiralo\nCode: GANG5991\n$3\n$3\n\n\nAirBnB\nLink\n$5\n$5\n\n\nAmerican Express\nLink\n$75\n$75\n\n\nChase\nLink\n$50\n$50\n\n\nDropbox\nLink\n500MB\n500MB\n\n\nPayPal\nLink\n$10\n$10\n\n\nRobinhood\nLink\n$5\n$5\n\n\nVenmo\nLink\n$5\n$5"
  },
  {
    "objectID": "misc/refer-a-friend.html#i-will-donate-all-i-receive-to-charity",
    "href": "misc/refer-a-friend.html#i-will-donate-all-i-receive-to-charity",
    "title": "Refer a Friend",
    "section": "I will donate all I receive to charity",
    "text": "I will donate all I receive to charity\nI donate regularly to the institutions and the charities I support.\n\nMarxe School of Public and International Affairs\nEnergy and Resources Group\nClimate and Society, The Climate School"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nCUNY Baruch College Visiting Scholar Paperwork Guide\n\n\nJan 3, 2025\n\n\n\n\n\n\nNew York State Climate Policy Goals Clarified\n\n\nNov 1, 2024\n\n\n\n\n\n\nQuarto Academic Website Examples and Tips\n\n\nOct 15, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "",
    "text": "When Qwen released their GSPO paper questioning GRPO’s theoretical foundations, they ignited a debate about theoretical correctness versus empirical success. GSPO offers stronger theoretical guarantees. GRPO makes approximations in its importance sampling approach that can be problematic. Yet GRPO powers state-of-the-art models like DeepSeek-R1, which achieved breakthrough performance on mathematical reasoning tasks. This post examines why both perspectives have merit and what this reveals about the theory-practice gap in deep learning."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#the-theoretical-problem",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#the-theoretical-problem",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "The Theoretical Problem",
    "text": "The Theoretical Problem\nGRPO’s objective computes token-level importance weights using single samples from each token distribution:\n\\[J_{\\text{GRPO}}(\\theta) = \\mathbb{E}\\left[\\frac{1}{G} \\sum_{i=1}^{G} \\frac{1}{\\lvert y_i \\rvert} \\sum_{t=1}^{\\lvert y_i \\rvert} \\min\\left(w_{i,t}(\\theta)\\hat{A}_i, \\text{clip}(w_{i,t}(\\theta), 1-\\varepsilon, 1+\\varepsilon)\\hat{A}_i\\right)\\right]\\]\nwhere the importance ratio at each token is:\n\\[w_{i,t}(\\theta) = \\frac{\\pi_\\theta(y_{i,t} \\mid x, y_{i,&lt;t})}{\\pi_{\\theta_{\\text{old}}}(y_{i,t} \\mid x, y_{i,&lt;t})}\\]\nThe importance sampling principle requires multiple samples to make reweighting valid. For a random variable \\(z\\), proper importance sampling states:\n\\[\\mathbb{E}_{\\pi_{\\text{target}}}[f(z)] = \\mathbb{E}_{\\pi_{\\text{behavior}}}\\left[\\frac{\\pi_{\\text{target}}(z)}{\\pi_{\\text{behavior}}(z)} \\cdot f(z)\\right]\\]\nThis equality holds asymptotically as the number of samples from \\(\\pi_{\\text{behavior}}\\) approaches infinity. GRPO computes \\(w_{i,t}\\) using a single sample \\(y_{i,t}\\) from \\(\\pi_{\\theta_{\\text{old}}}(\\cdot \\mid x, y_{i,&lt;t})\\). With only one sample per token position, the importance weight cannot perform valid distribution correction. Instead, it introduces high-variance noise that accumulates multiplicatively across the sequence length.\nIt’s important to note that using single samples with importance weights is not inherently invalid. Many policy gradient methods do this. The question is whether GRPO’s specific token-level aggregation of these weights introduces problematic variance, particularly as sequence length increases. The concern is less about “violating” importance sampling and more about whether this approximation remains reasonable under various conditions.\nConsider a 1000-token response. GRPO computes 1000 independent importance weights, each based on a single sample. If we denote the estimation error at token \\(t\\) as \\(\\epsilon_t\\), the accumulated effect scales as \\(\\exp(\\sum_t \\epsilon_t)\\) or \\(\\prod_t (1 + \\epsilon_t)\\). Small per-token errors compound into large sequence-level errors. Qwen’s experiments show this accumulation leads to “catastrophic and irreversible model collapse” particularly in Mixture-of-Experts models and long-context scenarios.\nGSPO corrects this by computing importance ratios at the sequence level:\n\\[J_{\\text{GSPO}}(\\theta) = \\mathbb{E}\\left[\\frac{1}{G} \\sum_{i=1}^{G} \\min\\left(s_i(\\theta)\\hat{A}_i, \\text{clip}(s_i(\\theta), 1-\\varepsilon, 1+\\varepsilon)\\hat{A}_i\\right)\\right]\\]\nwhere:\n\\[s_i(\\theta) = \\left(\\frac{\\pi_\\theta(y_i \\mid x)}{\\pi_{\\theta_{\\text{old}}}(y_i \\mid x)}\\right)^{\\frac{1}{\\lvert y_i \\rvert}}\\]\nThe exponent \\(\\frac{1}{\\lvert y_i \\rvert}\\) applies length normalization via geometric mean, preventing longer sequences from dominating the importance ratio. All tokens in sequence \\(y_i\\) share the same weight \\(s_i(\\theta)\\), eliminating token-level fluctuations. This aligns the optimization unit with the reward unit, since rewards are assigned to complete sequences rather than individual tokens."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#gradient-analysis",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#gradient-analysis",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "Gradient Analysis",
    "text": "Gradient Analysis\nThe gradient expressions reveal the fundamental difference. For GRPO:\n\\[\\nabla_\\theta J_{\\text{GRPO}} = \\mathbb{E}\\left[\\frac{1}{G} \\sum_{i=1}^{G} \\hat{A}_i \\cdot \\frac{1}{\\lvert y_i \\rvert} \\sum_{t=1}^{\\lvert y_i \\rvert} \\frac{\\pi_\\theta(y_{i,t} \\mid x, y_{i,&lt;t})}{\\pi_{\\theta_{\\text{old}}}(y_{i,t} \\mid x, y_{i,&lt;t})} \\cdot \\nabla_\\theta \\log \\pi_\\theta(y_{i,t} \\mid x, y_{i,&lt;t})\\right]\\]\nFor GSPO:\n\\[\\nabla_\\theta J_{\\text{GSPO}} = \\mathbb{E}\\left[\\frac{1}{G} \\sum_{i=1}^{G} \\left(\\frac{\\pi_\\theta(y_i \\mid x)}{\\pi_{\\theta_{\\text{old}}}(y_i \\mid x)}\\right)^{\\frac{1}{\\lvert y_i \\rvert}} \\cdot \\hat{A}_i \\cdot \\frac{1}{\\lvert y_i \\rvert} \\sum_{t=1}^{\\lvert y_i \\rvert} \\nabla_\\theta \\log \\pi_\\theta(y_{i,t} \\mid x, y_{i,&lt;t})\\right]\\]\nIn GRPO, each token \\(t\\) receives its own importance weight. These weights can vary arbitrarily: token 1 might get weight 0.5, token 2 weight 2.8, token 500 weight 0.09. The unequal weighting creates gradient variance that accumulates across the sequence. In GSPO, all tokens in sequence \\(i\\) share the same scalar weight, producing uniform treatment and stable gradients.\nThe following diagram illustrates the difference:\n\n\n\nToken-Level vs Sequence-Level Weighting\n\n\n\nGRPO Token-Level Weighting: Token: [ 1 ] [ 2 ] [ 3 ] … [ 999 ] [1000] Weight: [0.8 ] [2.1 ] [0.3 ] … [1.7 ] [0.1 ] → High variance, noisy gradients\nGSPO Sequence-Level Weighting: Token: [ 1 ] [ 2 ] [ 3 ] … [ 999 ] [1000] Weight: [1.05] [1.05] [1.05] … [1.05] [1.05] → Uniform, stable gradients"
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#empirical-evidence",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#empirical-evidence",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "Empirical Evidence",
    "text": "Empirical Evidence\nNote on experimental setup: The following results are reported from the RSPO paper. A fair comparison requires equivalent hyperparameter tuning effort for all methods. While these results suggest significant issues with GRPO on MoE architectures, the approximations (“~”) in GRPO’s scores and the specific experimental conditions warrant careful interpretation.\nThe evidence from training on Mixture-of-Experts architectures is striking. The RSPO paper evaluated Qwen3-30B-A3B across five mathematical reasoning benchmarks:\n\n\n\nBenchmark\nBase Model\nGRPO\nGSPO\nGMPO\nRSPO\n\n\n\n\nAIME24\n43.3\n~20\n74.1\n73.3\n80.0\n\n\nAMC23\n69.9\n~45\n77.1\n75.9\n79.5\n\n\nMATH500\n82.8\n~70\n88.2\n88.6\n88.4\n\n\nMinerva\n48.5\n~35\n58.1\n57.0\n61.8\n\n\nOlympiadBench\n44.7\n~40\n54.2\n54.8\n52.6\n\n\nAverage\n57.8\n35.0\n70.3\n69.9\n77.1\n\n\n\nGRPO not only underperforms GSPO but actually degrades below the base model. Training curves show pronounced collapse around 200 to 500 steps. The cause is expert activation volatility: after each gradient update in a 48-layer MoE model, approximately 10% of activated experts change for the same input. Token-level importance ratios \\(w_{i,t}\\) fluctuate drastically as different experts are selected, preventing convergence.\nThis expert volatility explanation is plausible and consistent with the observed failures, though definitively proving causation would require ablation studies isolating this factor from other potential causes like learning rates, batch sizes, or other architectural interactions.\nGSPO avoids this failure mode because sequence-level likelihoods remain stable even when individual token expert assignments shift. The sequence likelihood \\(\\pi_\\theta(y_i \\mid x)\\) aggregates over all token-level expert decisions, smoothing out routing variability. This eliminates the need for “Routing Replay,” a complex workaround that caches expert routes from the old policy and replays them during importance ratio computation.\nOn AIME 2024 using Qwen2.5-32B base, the performance gap is equally stark:\n\n\n\nMethod\nScore\nTraining Steps\n\n\n\n\nVanilla GRPO\n30\nBaseline\n\n\nGSPO\n70-80\nSame\n\n\nGRPO + engineering (DAPO)\n50\n50% of DeepSeek\n\n\nGRPO + engineering (SRPO)\n50\n10% of DeepSeek\n\n\n\nVanilla GRPO achieves only 30 points, while GSPO reaches 70 to 80 points with equivalent compute. The DAPO and SRPO variants improve GRPO by adding extensive engineering: asymmetric clipping, dynamic sampling, token-level loss modifications, and two-stage training. These modifications compensate for GRPO’s theoretical deficiencies but require significant implementation complexity.\nA counter-intuitive finding emerges from analyzing clipping statistics. GRPO clips approximately 0.13% of tokens during training, while GSPO clips 15% of tokens (two orders of magnitude more). Yet GSPO achieves superior performance. This demonstrates that GRPO’s token-level gradients are inherently noisy. Even unclipped gradients hurt rather than help. GSPO’s aggressive sequence-level clipping effectively filters out high-variance samples."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#the-gspo-length-bias-issue",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#the-gspo-length-bias-issue",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "The GSPO Length Bias Issue",
    "text": "The GSPO Length Bias Issue\nWhile GSPO addresses GRPO’s variance problems, it introduces its own limitation: length bias. The geometric mean normalization in GSPO’s importance ratio:\n\\[s_i(\\theta) = \\left(\\frac{\\pi_\\theta(y_i \\mid x)}{\\pi_{\\theta_{\\text{old}}}(y_i \\mid x)}\\right)^{\\frac{1}{\\lvert y_i \\rvert}}\\]\ncan create systematic biases in how the model treats responses of different lengths. Specifically:\n\nShort sequences receive disproportionately large importance weights when they deviate from the old policy, potentially leading to over-optimization on brief responses\nLong sequences have their importance ratios dampened even when they represent significant policy changes, potentially under-weighting important long-form improvements\nLength-dependent convergence: The effective learning rate becomes implicitly coupled to sequence length, which may not align with the true importance of different responses\n\nThis length bias can manifest in practice as models that either truncate responses prematurely (to exploit the short-sequence advantage) or fail to learn from long chains of reasoning (due to dampened signals). The issue is particularly problematic for tasks requiring variable-length reasoning where the optimal response length is itself a learned quantity."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#dynamic-reward-grpo-the-current-state-of-the-art",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#dynamic-reward-grpo-the-current-state-of-the-art",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "Dynamic Reward GRPO: The Current State-of-the-Art",
    "text": "Dynamic Reward GRPO: The Current State-of-the-Art\nRecent work has shown that Dynamic Reward GRPO (DR GRPO) addresses both GRPO’s variance issues and GSPO’s length bias, emerging as the current best-performing method in practice. DR GRPO introduces several key innovations:\n\nKey Improvements in DR GRPO\n\nDynamic advantage normalization: Instead of using fixed rewards, DR GRPO adaptively normalizes advantages based on sequence statistics, reducing the impact of length-dependent variance\nToken-level variance reduction: Implements sophisticated variance reduction techniques that maintain token-level granularity while controlling noise accumulation\nHybrid importance weighting: Combines elements of token-level and sequence-level importance sampling, dynamically adjusting based on sequence characteristics\nLength-agnostic optimization: Explicitly corrects for length bias through adaptive clipping ranges and normalization schemes\n\n\n\nEmpirical Performance\nCurrent benchmarks suggest DR GRPO achieves: - Stability comparable to GSPO on MoE architectures - Performance exceeding both vanilla GRPO and GSPO on mathematical reasoning tasks - No length bias in learned policies - Better sample efficiency than GSPO in many scenarios\nThe success of DR GRPO demonstrates that the token-level vs sequence-level debate may have been asking the wrong question. Rather than choosing between these extremes, the optimal approach appears to be a carefully engineered middle ground that preserves token-level signal while controlling variance through dynamic mechanisms."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#why-grpo-works-despite-being-wrong",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#why-grpo-works-despite-being-wrong",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "Why GRPO Works Despite Being Wrong",
    "text": "Why GRPO Works Despite Being Wrong\nGiven the theoretical concerns and empirical evidence of failure in specific contexts, why does GRPO succeed in others? Several mechanisms explain its continued effectiveness.\n\nSmall Divergence Regime\nWhen clipping keeps \\(\\pi_\\theta\\) close to \\(\\pi_{\\theta_{\\text{old}}}\\), the token-level approximation may be adequate. If policies are similar, we can write the token-level importance ratio as approximately \\(1 + \\epsilon_t\\) where \\(\\epsilon_t\\) is small. The product over all tokens becomes \\(\\prod_t (1 + \\epsilon_t) \\approx \\exp(\\sum_t \\epsilon_t)\\). If the per-token errors \\(\\epsilon_t\\) are roughly independent and average to a reasonable value, accumulated error may not be catastrophic.\nThis approximation breaks down in two scenarios. First, long sequences (1000+ tokens) accumulate many small errors into large total error. Second, MoE models violate the small divergence assumption because expert routing changes create large per-token probability shifts even when the overall policy changes moderately. The volatility of individual \\(w_{i,t}\\) values exceeds what clipping can control.\n\n\nEmpirical Risk Minimization\nThe theoretical objective may not be what matters for practical optimization. What matters is whether updates improve measured performance. GRPO’s updates are high variance and theoretically unjustified, yet they may still point in a productive direction on average. Deep learning is replete with methods whose theoretical justification was incorrect but which nonetheless work: the original explanation for batch normalization’s effectiveness was wrong, yet batch normalization remains standard practice.\nThe question becomes whether GRPO provides a sufficiently strong learning signal despite its flaws. For dense models on shorter sequences, the answer appears to be yes, conditional on careful hyperparameter tuning. For MoE models on longer sequences, the answer is definitively no.\n\n\nEngineering as Theory Compensation\nDAPO adds four modifications to vanilla GRPO: asymmetric clipping (Clip-Higher), dynamic sampling, token-level policy gradient loss, and overlong reward shaping. These are not mere optimizations but compensations for theoretical deficiencies. Clip-Higher allows rare but important tokens to be explored by decoupling upper and lower clipping bounds. Dynamic sampling filters out samples that produce zero gradients, improving sample efficiency. Token-level loss reweights contributions to prevent length bias. Overlong reward shaping penalizes excessive length in a smooth manner.\nEach modification addresses a specific pathology caused by token-level importance weighting. The fact that extensive engineering can rescue GRPO demonstrates two points. First, the theoretical problems are real and manifest as practical issues. Second, the problems are not insurmountable for dense models with sufficient effort. However, the engineering complexity represents hidden cost that GSPO avoids.\n\n\nTask Structure and Forgiveness\nSome tasks may be more tolerant of algorithmic approximation errors. Dense models with shorter sequences provide fewer opportunities for token-level noise to accumulate. The task structure matters: if critical information is concentrated in a few key tokens rather than distributed evenly, token-level importance reweighting might accidentally emphasize those key tokens despite lacking theoretical justification.\nConversely, tasks requiring precise long-range reasoning over 1000+ token chains of thought expose GRPO’s flaws maximally. The empirical pattern aligns with this hypothesis: GRPO struggles most on MoE models with long sequences, performs acceptably on dense models with shorter sequences, and falls between these extremes on intermediate scenarios.\n\n\nThe DeepSeek-R1 Puzzle\nGRPO’s success in DeepSeek-R1 deserves careful examination rather than dismissal. DeepSeek-R1 achieved remarkable performance on mathematical reasoning benchmarks using GRPO, raising important questions: What conditions allowed GRPO to succeed there? Was it the dense (non-MoE) architecture? Shorter effective sequence lengths during critical training phases? Exceptional hyperparameter tuning? Or does the task structure of mathematical reasoning provide some robustness to GRPO’s approximation errors?\nThe absence of public details about DeepSeek-R1’s training process makes definitive conclusions difficult. However, the empirical success suggests that for certain combinations of architecture, task, and sequence length, GRPO’s approximations remain within acceptable bounds. This doesn’t invalidate concerns about GRPO’s theoretical foundations, but it does highlight that the practical impact depends heavily on deployment context.\nWhy didn’t DeepSeek use GSPO or DR GRPO? Possible explanations include: (1) GRPO was more mature when DeepSeek-R1 was developed, (2) their specific infrastructure was optimized for GRPO, (3) newer methods like DR GRPO may have implementation subtleties not captured in papers, or (4) their dense architecture and tuning made GRPO sufficient. The choice between algorithms involves engineering tradeoffs beyond pure theoretical optimality."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#the-stability-analysis",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#the-stability-analysis",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "The Stability Analysis",
    "text": "The Stability Analysis\nTraining stability metrics reveal GSPO’s robustness advantage:\n\n\n\nThe stability difference is qualitative, not quantitative. GRPO training exhibits high variance reward curves with frequent drops. Some drops recover, but others lead to irreversible collapse where even reverting to earlier checkpoints fails to restore training. GSPO training shows monotonic improvement with smooth reward curves. The absence of catastrophic failures enables longer training runs and more aggressive scaling of compute.\nKey metrics comparison:\n\n\n\n\n\n\n\n\n\nMetric\nGRPO\nGSPO\nDR GRPO\n\n\n\n\nClipping Rate\n0.13%\n15%\n~5-10%\n\n\nExpert Routing Volatility\n~10% change per update\nImmune\nReduced\n\n\nFailure Mode\nCatastrophic collapse\nLength bias\nRare\n\n\nRecovery\nOften irreversible\nN/A\nGood"
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#production-deployment",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#production-deployment",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "Production Deployment",
    "text": "Production Deployment\nQwen3 models trained with GSPO demonstrate the algorithm’s scalability to production systems. The flagship Qwen3-235B-A22B achieves 85.7 on AIME’24 and 81.5 on AIME’25, substantially exceeding models trained with GRPO variants. On LiveCodeBench v5, it scores 70.7. On CodeForces, it achieves 2056 Elo rating. These results come from extended training runs that would be infeasible with GRPO’s instability.\nInfrastructure requirements differ significantly. GRPO requires Routing Replay for MoE models, adding memory overhead and communication cost. Routing Replay caches the expert routes from the old policy and replays them when computing importance ratios under the new policy. This ensures consistent expert activation but restricts the model’s capacity and complicates the training pipeline. GSPO eliminates this requirement entirely, simplifying infrastructure and allowing full utilization of model capacity.\nPrecision tolerance also favors GSPO. Training engines and inference engines often have subtle numerical differences due to optimization choices. GRPO needs exact token-level likelihoods, requiring recomputation in the training engine even when likelihoods were already computed during inference. GSPO’s sequence-level optimization is robust to small numerical differences, potentially allowing direct use of inference engine likelihoods without recomputation. This matters for efficiency in partial rollout and multi-turn RL scenarios.\nDR GRPO appears to offer similar infrastructure simplifications while maintaining better performance characteristics, though widespread production deployments are still emerging."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#when-each-method-wins",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#when-each-method-wins",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "When Each Method Wins",
    "text": "When Each Method Wins\nThe empirical evidence suggests updated guidelines:\nDR GRPO is the recommended default for new implementations given its strong empirical performance, lack of length bias, and reasonable stability. It represents the current state-of-the-art for most scenarios.\nGSPO remains a strong choice when maximum training stability is critical, particularly for MoE models where GRPO’s failure is catastrophic. GSPO is also preferable when implementation simplicity matters more than peak performance, or when length bias is manageable for the specific task.\nGRPO can still be viable for dense models with shorter sequences where its flaws are less exposed, or in legacy systems where migration cost exceeds the performance benefit. However, new implementations should strongly prefer DR GRPO or GSPO unless there are compelling infrastructure constraints."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#the-theoretical-lesson",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#the-theoretical-lesson",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "The Theoretical Lesson",
    "text": "The Theoretical Lesson\nThis case study illuminates the relationship between theory and practice in deep learning optimization. Theoretical correctness provides robustness guarantees but does not preclude success of theoretically flawed methods in restricted domains. GRPO violates importance sampling principles yet achieves competitive results on specific tasks with sufficient engineering. The violation matters in extreme regimes (MoE, long sequences) where theoretical predictions become empirically manifest.\nThe emergence of DR GRPO suggests that the most successful approaches may synthesize insights from both extremes rather than adhering dogmatically to either token-level or sequence-level formulations. The pattern resembles other instances where theory and practice iterate: initial methods have theoretical issues, theoretically-motivated alternatives address some issues but introduce new limitations, and eventually sophisticated engineering produces methods that work well in practice while being more theoretically grounded."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#practical-recommendations",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#practical-recommendations",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "Practical Recommendations",
    "text": "Practical Recommendations\nFor new implementations, DR GRPO is the recommended starting point given current empirical evidence. If DR GRPO is not available or well-supported in your framework, GSPO is the next best choice. The implementation for GSPO is straightforward:\nfrom trl import GRPOConfig\n\nconfig = GRPOConfig(\n    importance_sampling_level=\"sequence\",\n    loss_type=\"grpo\",\n    beta=0.0,\n    epsilon=3e-4,\n    epsilon_high=4e-4,\n    gradient_accumulation_steps=1,\n    steps_per_generation=4,\n)\nThe key parameter is importance_sampling_level=\"sequence\" which enables GSPO’s sequence-level importance weighting. The clipping ranges (epsilon=3e-4, epsilon_high=4e-4) are two orders of magnitude smaller than typical GRPO ranges because sequence-level ratios have different numerical scales than token-level ratios. Setting beta=0.0 removes KL regularization, which GSPO authors found unnecessary for long chain-of-thought reasoning.\nFor existing GRPO implementations, migration should be prioritized based on: - Critical: MoE architectures (GRPO fails catastrophically) - High priority: Long sequences (&gt;500 tokens), tasks where length bias matters - Medium priority: Production systems requiring high stability - Lower priority: Dense models with short sequences showing acceptable performance\nFor MoE models specifically, GSPO or DR GRPO are non-negotiable. The empirical evidence shows that vanilla GRPO fails catastrophically on MoE, and even heavily engineered GRPO variants require complex infrastructure like Routing Replay."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#limitations-and-caveats",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#limitations-and-caveats",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "Limitations and Caveats",
    "text": "Limitations and Caveats\nThis analysis has several limitations worth noting:\nPotential GSPO Tradeoffs: The length bias issue in GSPO is a real concern that may impact certain applications. Tasks requiring variable-length reasoning or where optimal response length must be learned may suffer from GSPO’s geometric mean normalization.\nDR GRPO Maturity: While DR GRPO shows promise, it’s a newer method with less production validation than GRPO or GSPO. Implementation details may vary, and hyperparameter sensitivity is not yet fully characterized.\nExperimental Reproducibility: The empirical comparisons rely on results from published papers without independent replication. Training stability claims would benefit from open-source implementations and shared training curves.\nEvolving Landscape: All three methods continue to evolve. Enhanced variants and potential refinements may shift the practical tradeoffs. The “optimal” choice may depend on rapidly changing infrastructure and tooling ecosystems.\nPublication Bias: Papers naturally emphasize scenarios where their proposed method excels. The broader landscape of deployments (many in proprietary settings) may include success cases not reflected in academic publications."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#conclusion",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#conclusion",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "Conclusion",
    "text": "Conclusion\nThe landscape of policy gradient methods for LLM alignment has evolved significantly:\n\nGRPO offered simplicity but suffers from theoretical issues that manifest as catastrophic failures in MoE architectures and high variance in long sequences\nGSPO corrected GRPO’s variance problems with sound theoretical foundations but introduced length bias\nDR GRPO appears to address both sets of issues, representing the current state-of-the-art\n\nFor practitioners, the choice hierarchy is clear: 1. First choice: DR GRPO if available and well-supported 2. Strong alternative: GSPO for stability-critical applications or when DR GRPO is unavailable 3. Legacy only: GRPO for existing systems where migration cost outweighs benefits\nThe broader lesson is that theory and practice exist in productive tension. Theory predicts failure modes that may not be immediately visible. Practice reveals which theoretical concerns matter most and which can be addressed through engineering. DR GRPO exemplifies an algorithm where theory and practice iterate to produce methods that are both theoretically motivated and empirically superior."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#references",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#references",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "References",
    "text": "References\n\nGSPO Paper - Group Sequence Policy Optimization\nGRPO Paper - DeepSeekMath (introduced GRPO)\nDAPO Paper - Improvements to GRPO\nSRPO Paper - Two-stage GRPO variant\nRSPO Paper - Router-shift aware optimization"
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html",
    "href": "posts/2025-10-18-jax_distributed/index.html",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "Training large language models like GPT-3 (175B parameters) requires distributing computation across dozens or hundreds of GPUs. JAX makes this remarkably elegant through its functional programming paradigm and sharding primitives. However, the mental model required differs significantly from PyTorch’s imperative style. This post demystifies JAX’s distributed training by addressing the key conceptual hurdles that arise when learning the framework.\n\n\nJAX excels at distributed training for three fundamental reasons:\n1. Functional paradigm: Parameters are data structures, not hidden object state. This makes sharding trivial—just split the data structure across devices.\n2. Explicit state management: No global random state or hidden device placement. Everything is passed explicitly.\n3. Automatic communication: Given sharding specifications, JAX’s compiler (XLA) figures out optimal communication patterns.\nFor comparison:\nPyTorch (DDP):\n# ~50+ lines of boilerplate\ndist.init_process_group(backend='nccl')\nrank = dist.get_rank()\nmodel = DDP(model, device_ids=[rank])\nsampler = DistributedSampler(dataset, rank=rank)\n# ... manual device management, rank checks, cleanup\nJAX:\n@jax.pmap\ndef train_step(params, batch):\n    return compute_grads(params, batch)\n\n\n\nJAX’s functional approach is the first conceptual hurdle. Unlike PyTorch where parameters live inside model objects, JAX separates computation from state.\nPyTorch:\nmodel = Model()  # Parameters hidden inside\nloss = model(x)  # Uses internal state\nloss.backward()  # Modifies internal .grad\nJAX:\nmodel = Model()  # Just defines computation\nparams = model.init(key, x)  # Parameters are separate data\nlogits = model.apply(params, x)  # Explicit parameter passing\ngrads = grad(loss_fn)(params, x)  # Explicit differentiation\nWhy does this matter? Because parameters being “just data” means you can trivially split them:\nsharded_params = jax.device_put(params, sharding_spec)\nNo DDP wrappers, no process groups, no manual device management.\n\n\n\nThe device mesh is JAX’s fundamental abstraction for organizing GPUs. Understanding this thoroughly is critical—most confusion in JAX distributed training stems from misunderstanding the mesh.\n\n\nA device mesh is a multi-dimensional array of devices with named axes:\n# Physical: 8x8 grid = 64 GPUs\ndevices = mesh_utils.create_device_mesh((8, 8))\n\n# Logical: Give axes semantic names\nmesh = Mesh(devices, axis_names=('data', 'model'))\nKey insight: axis names define how work is distributed, not the physical layout.\n        model axis (8 devices) →\ndata  ┌────┬────┬────┬────┬────┬────┬────┬────┐\naxis  │ 0  │ 1  │ 2  │ 3  │ 4  │ 5  │ 6  │ 7  │\n(8)   ├────┼────┼────┼────┼────┼────┼────┼────┤\n↓     │ 8  │ 9  │ 10 │ 11 │ 12 │ 13 │ 14 │ 15 │\n      ├────┼────┼────┼────┼────┼────┼────┼────┤\n      │ ... (64 GPUs total)                    │\n      └────┴────┴────┴────┴────┴────┴────┴────┘\nSemantics: - Same row: Process different batch slices with same model piece - Same column: Process same batch slice with different model pieces\nThis enables hybrid parallelism: 8-way data parallelism × 8-way model parallelism = 64-way total parallelism.\n\n\n\n\nPartitionSpec specifies tensor distribution across the mesh. The critical insight: PartitionSpec dimensions match tensor dimensions, not mesh dimensions.\nTensor shape:      (batch=64, seq=2048, embed=12288)\nPartitionSpec:     ('data',    None,    'model')\n                     ↑          ↑         ↑\n                     │          │         └─ Tensor dim 2: use 'model' axis\n                     │          └─────────── Tensor dim 1: replicate\n                     └────────────────────── Tensor dim 0: use 'data' axis\nThe mesh has 2 axes, but the tensor has 3 dimensions. PartitionSpec provides 3 entries, each referencing a mesh axis name or None.\n\n\ninput_batch = jnp.ones((64, 2048, 12288))\nspec = PartitionSpec('data', None, 'model')\n\n# What happens:\n# - Dim 0 (batch=64): Split 8 ways along 'data' axis → 8 per device\n# - Dim 1 (seq=2048): Replicate (all devices get full sequence)\n# - Dim 2 (embed=12288): Split 8 ways along 'model' axis → 1536 per device\n\n# Result per GPU: (8, 2048, 1536)\n\n\n\n\nUnderstanding memory layout is crucial for two reasons: correctness and performance. This is where reshape vs. transpose becomes important.\n\n\nIn multi-head attention, we perform:\nq = nn.Dense(num_heads * head_dim)(x)  # Shape: (batch, seq, 512)\n\nq = q.reshape(batch, seq, num_heads, head_dim)    # Step 1\nq = jnp.transpose(q, (0, 2, 1, 3))                # Step 2\nWhy not directly reshape to (batch, num_heads, seq, head_dim)?\nAnswer: Memory layout. After the Dense layer, the 512 dimensions are laid out in memory as:\n[head0_dim0, head0_dim1, ..., head0_dim63,    # First 64: head 0\n head1_dim0, head1_dim1, ..., head1_dim63,    # Next 64: head 1\n ...\n head7_dim0, head7_dim1, ..., head7_dim63]    # Last 64: head 7\nReshape changes how we interpret the data without moving it. Reshaping to (batch, seq, 8, 64) naturally groups the 512 dimensions into 8 groups of 64, which matches the memory layout.\nTranspose actually reorders data in memory. We need it to put num_heads before seq_len for efficient attention computation.\nAttempting to directly reshape to (batch, num_heads, seq, head_dim) would create a view where the data interpretation doesn’t match the underlying memory layout, resulting in incorrect groupings.\nKey principle: Reshape operations must respect the underlying memory layout. You can only reshape in ways that maintain the contiguity of data in memory.\n\n\n\n\nThe most common error is applying data parallelism to model weights:\n# WRONG\nweight = jnp.ones((12288, 49152))\nspec = PartitionSpec('data', None)  # Split first dim on data axis\nLet’s trace what happens in memory:\nWeight split along 'data' axis (8 ways):\n\nData position 0 (GPUs 0-7):   Rows 0-1535\nData position 1 (GPUs 8-15):  Rows 1536-3071\nData position 2 (GPUs 16-23): Rows 3072-4607\n...\n\nDuring training:\n- Batch slice 0 → Data position 0 → Uses weight rows 0-1535\n- Batch slice 1 → Data position 1 → Uses weight rows 1536-3071\n\nEach data replica has DIFFERENT weights = different models!\nTraining is broken.\nCorrect approach:\nspec = PartitionSpec(None, 'model')  # Replicate rows, split columns\n\n# All data replicas get all 12288 rows (same model)\n# Columns split 8 ways: each device gets 6144 columns\n\n\n\nThis reveals a critical insight: sharding happens at device_put time, not during computation.\n# Original batch in CPU/main memory\nbatch = jnp.ones((64, 2048, 12288))\n\n# Apply sharding - data is NOW physically distributed\ninput_spec = PartitionSpec('data', None, None)\nsharded_batch = jax.device_put(batch, NamedSharding(mesh, input_spec))\n\n# At this moment, batch is split across devices:\n# Data position 0 → batch[0:8] on GPUs 0-7\n# Data position 1 → batch[8:16] on GPUs 8-15\n# ...\nThe split happens before the forward pass. Each GPU already has its slice when computation begins. This is fundamentally different from PyTorch’s DistributedSampler which creates different batches per process.\n\n\n\nConsider this seemingly reasonable sharding:\ninput_spec = PartitionSpec('model', None, None)   # Batch on model axis\nweight_spec = PartitionSpec(None, 'model')        # Weights on model axis\nThis is mathematically correct but computationally wasteful:\nGPU 0:  Batch 0-7  × Weight cols 0-6143    → Result₀\nGPU 8:  Batch 0-7  × Weight cols 0-6143    → Result₀ (IDENTICAL!)\nGPU 16: Batch 0-7  × Weight cols 0-6143    → Result₀ (IDENTICAL!)\n...\nWhy? Both batch and weights are split along the model axis. GPUs in the same column (same model axis position) receive: - Same batch slice (model position 0 → batch 0-7) - Same weight slice (model position 0 → cols 0-6143) - Therefore: Identical computation\nAll GPUs in each column duplicate work. Only 12.5% of compute power is utilized (8 unique computations across 64 GPUs).\nSolution: Orthogonal splits:\ninput_spec = PartitionSpec('data', None, None)    # Different batches per row\nweight_spec = PartitionSpec(None, 'model')        # Different weights per column\nNow every GPU does unique work: 8 data replicas × 8 model pieces = true 64-way parallelism.\n\n\n\nPutting it together:\n# 1. Setup mesh\ndevices = mesh_utils.create_device_mesh((8, 8))\nmesh = Mesh(devices, axis_names=('data', 'model'))\n\n# 2. Initialize and shard parameters\nparams = model.init(key, dummy_input)\n\ndef shard_param(path, param):\n    name = '/'.join(path)\n    \n    # Large embeddings: split vocab\n    if 'embedding' in name and param.shape[0] &gt; 10000:\n        return jax.device_put(param, NamedSharding(mesh, PartitionSpec('model', None)))\n    \n    # Large matrices: split hidden dimension\n    if 'kernel' in name and param.shape[1] &gt; 1000:\n        return jax.device_put(param, NamedSharding(mesh, PartitionSpec(None, 'model')))\n    \n    # Small params: replicate\n    return jax.device_put(param, NamedSharding(mesh, PartitionSpec()))\n\nsharded_params = jax.tree_util.tree_map_with_path(shard_param, params)\n\n# 3. Training step\n@jax.jit\ndef train_step(params, batch, opt_state):\n    def loss_fn(params):\n        logits = model.apply(params, batch['input_ids'])\n        return cross_entropy(logits, batch['labels'])\n    \n    loss, grads = jax.value_and_grad(loss_fn)(params)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss\n\n# 4. Main loop\nfor batch in dataloader:\n    # Shard input\n    batch = jax.device_put(batch, NamedSharding(mesh, PartitionSpec('data', None)))\n    \n    # Train (all communication automatic)\n    sharded_params, opt_state, loss = train_step(sharded_params, batch, opt_state)\n\n\n\nWith 8-way model parallelism on 64 A100 GPUs:\nTotal parameters: 175B\nPer device: 175B / 8 = 22B params\n\nMemory per GPU (FP16):\n- Parameters:        22B × 2 bytes = 44 GB → 11 GB (with optimizations)\n- Gradients:         Same as parameters = 11 GB\n- Optimizer (Adam):  2× parameters = 22 GB\n- Activations:       ~20 GB\n─────────────────────────────────────────\nTotal: ~64 GB ✓ Fits on 80GB A100\nWithout sharding: 175B × 4 bytes = 700 GB for parameters alone. Impossible on single GPU.\n\n\n\n\nJAX’s functional paradigm makes parameters explicit data structures that can be trivially split across devices.\nDevice mesh with named axes provides semantic organization. The ‘data’ axis represents different data batches, the ‘model’ axis represents different model pieces.\nPartitionSpec dimensions match tensor dimensions, not mesh dimensions. Each entry references a named mesh axis or None.\nMemory layout matters: Reshape operations must respect contiguous memory layout. This is why attention requires both reshape and transpose.\nWeights use model parallelism (split along model axis), inputs use data parallelism (split along data axis). Mixing these causes either incorrect training (different models per replica) or redundant computation (wasted GPUs).\nSharding happens at device_put time, not during computation. Once sharded, JAX/XLA handles all communication automatically.\nEfficiency requires orthogonal splits: batch along data axis, model along model axis. This achieves true N×M parallelism on an N×M mesh.\n\nUnderstanding these principles, particularly the memory layout considerations and the distinction between physical device arrangement and logical axis semantics, demystifies JAX’s sharding and reveals why it’s particularly elegant for large-scale training.\n\n\n\n\nJAX Documentation\nJAX Sharding Guide\nFlax Documentation"
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#why-jax-for-distributed-training",
    "href": "posts/2025-10-18-jax_distributed/index.html#why-jax-for-distributed-training",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "JAX excels at distributed training for three fundamental reasons:\n1. Functional paradigm: Parameters are data structures, not hidden object state. This makes sharding trivial—just split the data structure across devices.\n2. Explicit state management: No global random state or hidden device placement. Everything is passed explicitly.\n3. Automatic communication: Given sharding specifications, JAX’s compiler (XLA) figures out optimal communication patterns.\nFor comparison:\nPyTorch (DDP):\n# ~50+ lines of boilerplate\ndist.init_process_group(backend='nccl')\nrank = dist.get_rank()\nmodel = DDP(model, device_ids=[rank])\nsampler = DistributedSampler(dataset, rank=rank)\n# ... manual device management, rank checks, cleanup\nJAX:\n@jax.pmap\ndef train_step(params, batch):\n    return compute_grads(params, batch)"
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#the-functional-foundation",
    "href": "posts/2025-10-18-jax_distributed/index.html#the-functional-foundation",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "JAX’s functional approach is the first conceptual hurdle. Unlike PyTorch where parameters live inside model objects, JAX separates computation from state.\nPyTorch:\nmodel = Model()  # Parameters hidden inside\nloss = model(x)  # Uses internal state\nloss.backward()  # Modifies internal .grad\nJAX:\nmodel = Model()  # Just defines computation\nparams = model.init(key, x)  # Parameters are separate data\nlogits = model.apply(params, x)  # Explicit parameter passing\ngrads = grad(loss_fn)(params, x)  # Explicit differentiation\nWhy does this matter? Because parameters being “just data” means you can trivially split them:\nsharded_params = jax.device_put(params, sharding_spec)\nNo DDP wrappers, no process groups, no manual device management."
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#understanding-device-mesh-the-core-abstraction",
    "href": "posts/2025-10-18-jax_distributed/index.html#understanding-device-mesh-the-core-abstraction",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "The device mesh is JAX’s fundamental abstraction for organizing GPUs. Understanding this thoroughly is critical—most confusion in JAX distributed training stems from misunderstanding the mesh.\n\n\nA device mesh is a multi-dimensional array of devices with named axes:\n# Physical: 8x8 grid = 64 GPUs\ndevices = mesh_utils.create_device_mesh((8, 8))\n\n# Logical: Give axes semantic names\nmesh = Mesh(devices, axis_names=('data', 'model'))\nKey insight: axis names define how work is distributed, not the physical layout.\n        model axis (8 devices) →\ndata  ┌────┬────┬────┬────┬────┬────┬────┬────┐\naxis  │ 0  │ 1  │ 2  │ 3  │ 4  │ 5  │ 6  │ 7  │\n(8)   ├────┼────┼────┼────┼────┼────┼────┼────┤\n↓     │ 8  │ 9  │ 10 │ 11 │ 12 │ 13 │ 14 │ 15 │\n      ├────┼────┼────┼────┼────┼────┼────┼────┤\n      │ ... (64 GPUs total)                    │\n      └────┴────┴────┴────┴────┴────┴────┴────┘\nSemantics: - Same row: Process different batch slices with same model piece - Same column: Process same batch slice with different model pieces\nThis enables hybrid parallelism: 8-way data parallelism × 8-way model parallelism = 64-way total parallelism."
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#partitionspec-mapping-tensors-to-mesh",
    "href": "posts/2025-10-18-jax_distributed/index.html#partitionspec-mapping-tensors-to-mesh",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "PartitionSpec specifies tensor distribution across the mesh. The critical insight: PartitionSpec dimensions match tensor dimensions, not mesh dimensions.\nTensor shape:      (batch=64, seq=2048, embed=12288)\nPartitionSpec:     ('data',    None,    'model')\n                     ↑          ↑         ↑\n                     │          │         └─ Tensor dim 2: use 'model' axis\n                     │          └─────────── Tensor dim 1: replicate\n                     └────────────────────── Tensor dim 0: use 'data' axis\nThe mesh has 2 axes, but the tensor has 3 dimensions. PartitionSpec provides 3 entries, each referencing a mesh axis name or None.\n\n\ninput_batch = jnp.ones((64, 2048, 12288))\nspec = PartitionSpec('data', None, 'model')\n\n# What happens:\n# - Dim 0 (batch=64): Split 8 ways along 'data' axis → 8 per device\n# - Dim 1 (seq=2048): Replicate (all devices get full sequence)\n# - Dim 2 (embed=12288): Split 8 ways along 'model' axis → 1536 per device\n\n# Result per GPU: (8, 2048, 1536)"
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#memory-layout-the-hidden-complexity",
    "href": "posts/2025-10-18-jax_distributed/index.html#memory-layout-the-hidden-complexity",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "Understanding memory layout is crucial for two reasons: correctness and performance. This is where reshape vs. transpose becomes important.\n\n\nIn multi-head attention, we perform:\nq = nn.Dense(num_heads * head_dim)(x)  # Shape: (batch, seq, 512)\n\nq = q.reshape(batch, seq, num_heads, head_dim)    # Step 1\nq = jnp.transpose(q, (0, 2, 1, 3))                # Step 2\nWhy not directly reshape to (batch, num_heads, seq, head_dim)?\nAnswer: Memory layout. After the Dense layer, the 512 dimensions are laid out in memory as:\n[head0_dim0, head0_dim1, ..., head0_dim63,    # First 64: head 0\n head1_dim0, head1_dim1, ..., head1_dim63,    # Next 64: head 1\n ...\n head7_dim0, head7_dim1, ..., head7_dim63]    # Last 64: head 7\nReshape changes how we interpret the data without moving it. Reshaping to (batch, seq, 8, 64) naturally groups the 512 dimensions into 8 groups of 64, which matches the memory layout.\nTranspose actually reorders data in memory. We need it to put num_heads before seq_len for efficient attention computation.\nAttempting to directly reshape to (batch, num_heads, seq, head_dim) would create a view where the data interpretation doesn’t match the underlying memory layout, resulting in incorrect groupings.\nKey principle: Reshape operations must respect the underlying memory layout. You can only reshape in ways that maintain the contiguity of data in memory."
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#critical-mistake-wrong-sharding-for-weights",
    "href": "posts/2025-10-18-jax_distributed/index.html#critical-mistake-wrong-sharding-for-weights",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "The most common error is applying data parallelism to model weights:\n# WRONG\nweight = jnp.ones((12288, 49152))\nspec = PartitionSpec('data', None)  # Split first dim on data axis\nLet’s trace what happens in memory:\nWeight split along 'data' axis (8 ways):\n\nData position 0 (GPUs 0-7):   Rows 0-1535\nData position 1 (GPUs 8-15):  Rows 1536-3071\nData position 2 (GPUs 16-23): Rows 3072-4607\n...\n\nDuring training:\n- Batch slice 0 → Data position 0 → Uses weight rows 0-1535\n- Batch slice 1 → Data position 1 → Uses weight rows 1536-3071\n\nEach data replica has DIFFERENT weights = different models!\nTraining is broken.\nCorrect approach:\nspec = PartitionSpec(None, 'model')  # Replicate rows, split columns\n\n# All data replicas get all 12288 rows (same model)\n# Columns split 8 ways: each device gets 6144 columns"
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#when-does-batch-splitting-actually-happen",
    "href": "posts/2025-10-18-jax_distributed/index.html#when-does-batch-splitting-actually-happen",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "This reveals a critical insight: sharding happens at device_put time, not during computation.\n# Original batch in CPU/main memory\nbatch = jnp.ones((64, 2048, 12288))\n\n# Apply sharding - data is NOW physically distributed\ninput_spec = PartitionSpec('data', None, None)\nsharded_batch = jax.device_put(batch, NamedSharding(mesh, input_spec))\n\n# At this moment, batch is split across devices:\n# Data position 0 → batch[0:8] on GPUs 0-7\n# Data position 1 → batch[8:16] on GPUs 8-15\n# ...\nThe split happens before the forward pass. Each GPU already has its slice when computation begins. This is fundamentally different from PyTorch’s DistributedSampler which creates different batches per process."
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#redundant-computation-a-subtle-pitfall",
    "href": "posts/2025-10-18-jax_distributed/index.html#redundant-computation-a-subtle-pitfall",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "Consider this seemingly reasonable sharding:\ninput_spec = PartitionSpec('model', None, None)   # Batch on model axis\nweight_spec = PartitionSpec(None, 'model')        # Weights on model axis\nThis is mathematically correct but computationally wasteful:\nGPU 0:  Batch 0-7  × Weight cols 0-6143    → Result₀\nGPU 8:  Batch 0-7  × Weight cols 0-6143    → Result₀ (IDENTICAL!)\nGPU 16: Batch 0-7  × Weight cols 0-6143    → Result₀ (IDENTICAL!)\n...\nWhy? Both batch and weights are split along the model axis. GPUs in the same column (same model axis position) receive: - Same batch slice (model position 0 → batch 0-7) - Same weight slice (model position 0 → cols 0-6143) - Therefore: Identical computation\nAll GPUs in each column duplicate work. Only 12.5% of compute power is utilized (8 unique computations across 64 GPUs).\nSolution: Orthogonal splits:\ninput_spec = PartitionSpec('data', None, None)    # Different batches per row\nweight_spec = PartitionSpec(None, 'model')        # Different weights per column\nNow every GPU does unique work: 8 data replicas × 8 model pieces = true 64-way parallelism."
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#complete-training-loop",
    "href": "posts/2025-10-18-jax_distributed/index.html#complete-training-loop",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "Putting it together:\n# 1. Setup mesh\ndevices = mesh_utils.create_device_mesh((8, 8))\nmesh = Mesh(devices, axis_names=('data', 'model'))\n\n# 2. Initialize and shard parameters\nparams = model.init(key, dummy_input)\n\ndef shard_param(path, param):\n    name = '/'.join(path)\n    \n    # Large embeddings: split vocab\n    if 'embedding' in name and param.shape[0] &gt; 10000:\n        return jax.device_put(param, NamedSharding(mesh, PartitionSpec('model', None)))\n    \n    # Large matrices: split hidden dimension\n    if 'kernel' in name and param.shape[1] &gt; 1000:\n        return jax.device_put(param, NamedSharding(mesh, PartitionSpec(None, 'model')))\n    \n    # Small params: replicate\n    return jax.device_put(param, NamedSharding(mesh, PartitionSpec()))\n\nsharded_params = jax.tree_util.tree_map_with_path(shard_param, params)\n\n# 3. Training step\n@jax.jit\ndef train_step(params, batch, opt_state):\n    def loss_fn(params):\n        logits = model.apply(params, batch['input_ids'])\n        return cross_entropy(logits, batch['labels'])\n    \n    loss, grads = jax.value_and_grad(loss_fn)(params)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss\n\n# 4. Main loop\nfor batch in dataloader:\n    # Shard input\n    batch = jax.device_put(batch, NamedSharding(mesh, PartitionSpec('data', None)))\n    \n    # Train (all communication automatic)\n    sharded_params, opt_state, loss = train_step(sharded_params, batch, opt_state)"
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#memory-calculation-for-gpt-3",
    "href": "posts/2025-10-18-jax_distributed/index.html#memory-calculation-for-gpt-3",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "With 8-way model parallelism on 64 A100 GPUs:\nTotal parameters: 175B\nPer device: 175B / 8 = 22B params\n\nMemory per GPU (FP16):\n- Parameters:        22B × 2 bytes = 44 GB → 11 GB (with optimizations)\n- Gradients:         Same as parameters = 11 GB\n- Optimizer (Adam):  2× parameters = 22 GB\n- Activations:       ~20 GB\n─────────────────────────────────────────\nTotal: ~64 GB ✓ Fits on 80GB A100\nWithout sharding: 175B × 4 bytes = 700 GB for parameters alone. Impossible on single GPU."
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#key-principles",
    "href": "posts/2025-10-18-jax_distributed/index.html#key-principles",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "JAX’s functional paradigm makes parameters explicit data structures that can be trivially split across devices.\nDevice mesh with named axes provides semantic organization. The ‘data’ axis represents different data batches, the ‘model’ axis represents different model pieces.\nPartitionSpec dimensions match tensor dimensions, not mesh dimensions. Each entry references a named mesh axis or None.\nMemory layout matters: Reshape operations must respect contiguous memory layout. This is why attention requires both reshape and transpose.\nWeights use model parallelism (split along model axis), inputs use data parallelism (split along data axis). Mixing these causes either incorrect training (different models per replica) or redundant computation (wasted GPUs).\nSharding happens at device_put time, not during computation. Once sharded, JAX/XLA handles all communication automatically.\nEfficiency requires orthogonal splits: batch along data axis, model along model axis. This achieves true N×M parallelism on an N×M mesh.\n\nUnderstanding these principles, particularly the memory layout considerations and the distinction between physical device arrangement and logical axis semantics, demystifies JAX’s sharding and reveals why it’s particularly elegant for large-scale training."
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#resources",
    "href": "posts/2025-10-18-jax_distributed/index.html#resources",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "JAX Documentation\nJAX Sharding Guide\nFlax Documentation"
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html",
    "href": "posts/2025-3-20-cudp/index.html",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "",
    "text": "So you have been thinking about CUDA programming from a while and now want to learn but are confused how to proceed and know about memory models, etc. in CUDA. Well, even I am confused, so let’s learn together. This is the first blog of the CUDA series where the end goal is to code DQN in C and accelerate training and inference with CUDA.\nWith AI rapidly advancing, its integration into high-stakes environments is becoming inevitable. In this landscape, efficient deployment is arguably more critical than the pace of new research. As the number of AI tools continues to grow, efficiency may well become the defining moat for the tools that endure.\nFor nearly every use case, there’s potential to unlock significant performance gains — often by writing custom CUDA kernels tailored to the task.\nRL lives in my heart. Thus, the end goal of this learning experience is to write a DQN completely in C and accelerate the training using CUDA kernels.\nSo let’s go…!\nLet’s start off with memory organization in CUDA, then intuitively I will explain my first program: matrix multiplication in CUDA."
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#first-principles-memory-organization-in-cuda",
    "href": "posts/2025-3-20-cudp/index.html#first-principles-memory-organization-in-cuda",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "First Principles: Memory Organization in CUDA",
    "text": "First Principles: Memory Organization in CUDA\nBefore writing a single kernel, let’s zoom into the GPU’s memory layout. This is where most CUDA bugs (and performance pitfalls) are born.\nIn CUDA, like in C/C++, matrices are stored in row-major order.\n\nWhat does row-major mean?\nImagine a 2D matrix:\nA = [[1, 2, 3],\n     [4, 5, 6]]\nEven though it’s 2D logically, it’s stored in a flat 1D array as:\n[1, 2, 3, 4, 5, 6]\nTo access the element at A[row][col], the index becomes:\nindex = row * numCols + col\nFor the above matrix, A[1][2] is 1*3 + 2 = 5, and sure enough, A[5] = 6.\nThis little indexing trick — row * numCols + col — is foundational. We’ll use it repeatedly in our CUDA code to compute where each value lives in memory."
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#matrix-multiplication-recap",
    "href": "posts/2025-3-20-cudp/index.html#matrix-multiplication-recap",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "Matrix Multiplication Recap",
    "text": "Matrix Multiplication Recap\nWe have two matrices:\n\nA of size M × K\nB of size K × N\n\nWe want to compute: - C = A × B → a matrix of size M × N\nThe formula for each element of C is:\nC[row][col] = sum over i of A[row][i] * B[i][col]\nThat’s a dot product between a row from A and a column from B."
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#writing-the-cuda-kernel",
    "href": "posts/2025-3-20-cudp/index.html#writing-the-cuda-kernel",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "Writing the CUDA Kernel",
    "text": "Writing the CUDA Kernel\nHere’s our minimal working kernel:\n__global__ void matMulKernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row &lt; M && col &lt; N) {\n        float sum = 0.0f;\n        for (int i = 0; i &lt; K; ++i) {\n            sum += A[row * K + i] * B[i * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}"
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#thread-indexing-why-.y-for-row-and-.x-for-col",
    "href": "posts/2025-3-20-cudp/index.html#thread-indexing-why-.y-for-row-and-.x-for-col",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "Thread Indexing: Why .y for Row and .x for Col?",
    "text": "Thread Indexing: Why .y for Row and .x for Col?\nint row = blockIdx.y * blockDim.y + threadIdx.y;\nint col = blockIdx.x * blockDim.x + threadIdx.x;\n\nThreads are arranged in a 2D grid\nEach thread computes one element of the result matrix C[row][col]\nRows → vertical → Y-axis\nColumns → horizontal → X-axis\n\nHence, row is computed using .y components, and col using .x.\nMakes your mental model clean and your code correct."
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#memory-access-why-row-n-col",
    "href": "posts/2025-3-20-cudp/index.html#memory-access-why-row-n-col",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "Memory Access: Why row * N + col?",
    "text": "Memory Access: Why row * N + col?\nThis was one of the core doubts that tripped me up early on:\n\n“Shouldn’t it be row * M + col since the matrix has M rows?”\n\nTurns out — no. It’s row * N + col because:\n\nWe’re accessing matrix C, which is M × N\nEach row of C has N elements\nSo to access the start of row, we skip row * N elements\nThen move col steps in → row * N + col\n\nAlways remember: it’s about how many columns per row, not how many rows in total."
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#what-about-bs-indexing-bi-n-col",
    "href": "posts/2025-3-20-cudp/index.html#what-about-bs-indexing-bi-n-col",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "What about B’s indexing: B[i * N + col]?",
    "text": "What about B’s indexing: B[i * N + col]?\nAnother “aha!” moment:\n\n“Aren’t we accessing B column-wise here? Shouldn’t it be column-major?”\n\nGood instinct — but even though we’re accessing down a column, we’re still treating B as row-major.\n\nWe’re accessing B[i][col]\nSince B has N columns, row-major indexing says:\n\nB[i][col] = i * N + col\nSo B[i * N + col] is still perfectly valid row-major indexing, even if the access pattern feels “column-ish”."
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#edge-case-gotcha-tiny-matrices",
    "href": "posts/2025-3-20-cudp/index.html#edge-case-gotcha-tiny-matrices",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "Edge Case Gotcha: Tiny Matrices",
    "text": "Edge Case Gotcha: Tiny Matrices\nSay we multiply:\n\nA = [1 2 3] → 1×3\nB = [1, 2, 3]ᵗ → 3×1\n\nThe result should be:\n1×1 = [1×1 + 2×2 + 3×3] = [14]\nBut your code might crash or give garbage. Why?\nBecause your kernel might launch a full grid of threads, and many of them will access memory out-of-bounds.\nSolution:\nif (row &lt; M && col &lt; N)\nAlways bound your threads. Especially with small matrices."
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#test-case",
    "href": "posts/2025-3-20-cudp/index.html#test-case",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "Test Case",
    "text": "Test Case\nHere’s a minimal test:\nint M = 1, K = 3, N = 1;\nfloat A[] = {1, 2, 3};   // 1x3\nfloat B[] = {1, 2, 3};   // 3x1\nfloat C[1];              // 1x1 output\n\n// Launch with grid/block of 1\ndim3 blockSize(1, 1);\ndim3 gridSize(1, 1);\n\n// Run the kernel → expect C[0] = 14"
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#the-end-goal-dqn-in-pure-c-cuda",
    "href": "posts/2025-3-20-cudp/index.html#the-end-goal-dqn-in-pure-c-cuda",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "The End Goal: DQN in Pure C + CUDA",
    "text": "The End Goal: DQN in Pure C + CUDA\nThis matrix multiplication was a warm-up.\nMy goal? To build Deep Q-Networks (DQN) completely in C, and accelerate training with CUDA — no PyTorch, no TensorFlow. Just raw speed and full control.\nReinforcement learning lives in my heart. And this journey is all about learning from first principles."
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#whats-next",
    "href": "posts/2025-3-20-cudp/index.html#whats-next",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "What’s Next?",
    "text": "What’s Next?\n\nTiled matrix multiplication (for speed!)\nShared memory optimization\nWriting ReLU, Softmax, Linear layers in CUDA\nCustom CUDA-based experience replay\nCUDA kernels for DQN forward/backward pass\n\nStay tuned — this is just the start!"
  },
  {
    "objectID": "projects/sporsprayig/index.html",
    "href": "projects/sporsprayig/index.html",
    "title": "Spot Spraying",
    "section": "",
    "text": "My project at DFKI with HYDAC. Built a robotic system for identifying & localizing weeds/crops for spot-spraying. More about the project:"
  },
  {
    "objectID": "projects/sporsprayig/index.html#spot-spraying",
    "href": "projects/sporsprayig/index.html#spot-spraying",
    "title": "Spot Spraying",
    "section": "",
    "text": "My project at DFKI with HYDAC. Built a robotic system for identifying & localizing weeds/crops for spot-spraying. More about the project:"
  },
  {
    "objectID": "projects/ganvo/index.html",
    "href": "projects/ganvo/index.html",
    "title": "Generative Adversarial Networks for Visual Odometry",
    "section": "",
    "text": "Resources\n\nCode Repository: GitHub Link"
  },
  {
    "objectID": "projects/deeplidar/index.html",
    "href": "projects/deeplidar/index.html",
    "title": "DeepLiDARFlow",
    "section": "",
    "text": "Abstract\nScene flow is the dense 3D reconstruction of motion and geometry of a scene. Most state-of-the-art methods use a pair of stereo images as input for full scene reconstruction. These methods depend a lot on the quality of the RGB images and perform poorly in regions with reflective objects, shadows, ill-conditioned light environment and so on. LiDAR measurements are much less sensitive to the aforementioned conditions but LiDAR features are in general unsuitable for matching tasks due to their sparse nature. Hence, using both LiDAR and RGB can potentially overcome the individual disadvantages of each sensor by mutual improvement and yield robust features which can improve the matching process. In this paper, we present DeepLiDARFlow, a novel deep learning architecture which fuses high level RGB and LiDAR features at multiple scales in a monocular setup to predict dense scene flow. Its performance is much better in the critical regions where image-only and LiDAR-only methods are inaccurate. We verify our DeepLiDARFlow using the established data sets KITTI and FlyingThings3D and we show strong robustness compared to several state-of-the-art methods which used other input modalities.\n\n\nResources\n\nCode Repository: GitHub Link\nPaper: ArXiV"
  },
  {
    "objectID": "cn.html",
    "href": "cn.html",
    "title": "何钢",
    "section": "",
    "text": "深度能源与气候政策实验室\n  \n  \n      CUNY主页\n  \n  \n    \n     领英\n  \n  \n    \n     优兔\n  \n  \n     {{&lt; ai orcid &gt;}} ORCID\n  \n  \n     {{&lt; ai clarivate &gt;}} Web of Science\n  \n  \n     {{&lt; ai scopus &gt;}} Scopus\n  \n  \n     {{&lt; ai semantic-scholar &gt;}} 语义学者\n  \n  \n     {{&lt; ai google-scholar &gt;}} 谷歌学术\n  \n  \n     {{&lt; ai researchgate &gt;}} 研究之门\n  \n\n  \n  \n何钢是纽约市立大学巴鲁克学院公共和国际事务学院副教授，也是纽约市立大学研究生院和大学中心的博导及人口研究所的兼职教授。他主要从事能源系统、能源和气候政策及能源转型方面的研究和教学。他创建了深度能源和气候政策实验室，致力于为深度减排实现碳中和提供深度分析与深度见解。\n他的研究探索能源转型实现碳中和的技术路径和政策选择，及其对气候、环境与社会经济的影响，旨在实现可持续、韧性及公正的能源转型。聚焦领域包括清洁能源全球供应链、电力系统脱碳建模、气候变化对能源系统的影响、能源-水-碳关联关系、能源与气候公正等。他主导和合作的研究发表在高影响力的跨学科和专业期刊上，包括《自然》、《自然-通讯》、《自然-能源》、《自然-水》、《一个地球》、《环境科学与技术》和《能源政策》。\n他的研究得到了联邦政府、州级级地方政府、非政府组织、基金会和工业界的支持。研究成果被《自然》、《经济学人》、《纽约时报》、《科学美国人》、《碳简报》、《国家地理》、《能源与环境新闻》等媒体报道，并被国际应用系统分析研究所、政府间气候变化专门委员会（IPCC AR6）、世界银行、世界贸易组织、联合国环境规划署、国际可再生能源署等国际组织发布的报告所引用。\n他的工作为能源与气候政策制定提供了参考。例如，他曾受邀为《纽约州气候领导力和社区保护法案》提供证词，并担任纽约州气候行动委员会的气候行动计划综合报告技术顾问委员会成员为其提供建议。他还作为中美“21世纪20年代强化气候行动工作组”下能源转型专家组成员参与了中美在能源和气候变化方面的合作。\n他曾被选为联合国气候变化框架公约第11届缔约国会议青年代表（2005年）、亚洲21世纪青年领导力（2007年）、阿斯彭环境论坛学者（2011年）、新经济思维研究所青年学者（2013年）、ITIF能源创新政策与管理学者（2019年），美中关系全国委员会公共知识分子项目学者（2025年），并入选斯坦福/爱思唯尔全球前2%科学家名单（2025, 2024年）。\n他曾任职于石溪大学技术与社会系、劳伦斯伯克利国家实验室能源技术部，和斯坦福大学能源与可持续发展研究中心。他毕业于加州大学伯克利分校获得能源与资源博士学位，哥伦比亚大学气候与社会硕士学位及北京大学地理学学士和硕士学位。"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "My research and implementation projects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpot Spraying\n\n\nMy project at DFKI with HYDAC. Built a robotic system for identifying & localizing weeds/crops for spot-spraying.\n\n\n\n\n\n\n\n\n\n\n\n\nHSID-CNN\n\n\nDeep Learning for denoising hyperspectral images (AVIRIS). Work completed at Pixxel, Bengaluru.\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative Adversarial Networks for Visual Odometry\n\n\nUnofficial PyTorch implementation for GANVO - a GAN-based visual odometry depth estimation.\n\n\n\n\n\n\n\n\n\n\n\n\nUnsupervised Cross Spectral Stereo Matching\n\n\nPyTorch implementation of ‘Unsupervised Cross-spectral Stereo Matching by Learning to Synthesize’.\n\n\n\n\n\n\n\n\n\n\n\n\nDeepLiDARFlow\n\n\nBachelor thesis; worked on scene flow estimation using monocular camera & sparse LiDAR. Work accepted at two conferences.\n\n\n\n\n\n\n\n\n\n\n\n\nCompiler Construction\n\n\nConstructed a compiler for a given language specification in C language. A project at BITS Pilani.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "media.html",
    "href": "media.html",
    "title": "Media",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nChina Faces the Cost of Dismantling the World’s Biggest Coal Sector\n\n\n\nBloomberg\n\n\n\n“Coal remains deeply embedded in the economic and social fabric of many Chinese communities,” said Gang He, associate professor at Baruch College, City University of New York, who specialises in energy and climate policies. “A transition away from coal is therefore not only an energy transformation, but also an economic and employment transition that must be managed with care.” “The ability of China to manage this transition in a rapid and just manner will have a significant impact on how it and, to a large extent, the world use energy and address climate change,” Gang said.\n\n\n\n\n\nDec 14, 2025\n\n\nDan Murtaugh\n\n\n\n\n\n\n\n\n\n\n\n\nAlbuquerque’s Array Technologies Powers Ahead Despite Trump’s Tougher Stance on Renewables\n\n\n\nAlbuquerque Journal\n\n\n\n“It’s encouraging to see domestic solar manufacturing gaining ground,” said Gang He, an associate professor of energy and climate policy at Baruch College in New York. “But it’s equally important to keep the flow of knowledge, talent and capital open, so U.S. markets can continue to benefit from global learning, driving down costs and speeding up solar deployment.”\n\n\n\n\n\nNov 15, 2025\n\n\nHannah García\n\n\n\n\n\n\n\n\n\n\n\n\nUS solar tariffs linked to preventable deaths, claim researchers\n\n\n\nPV Magazine\n\n\n\nA more rigorous—and encouraging—analysis from researchers at the City University of New York found that solar panel imports into the United States displaced 305 TWh of fossil generation, avoided 178 million tons of CO2 emissions, and prevented 595 premature deaths between 2014 and 2022.\n\n\n\n\n\nNov 4, 2025\n\n\nJohn Weaver\n\n\n\n\n\n\n\n\n\n\n\n\nChina’s New Coal Language Leaves More Room for Rising Use\n\n\n\nBloomberg\n\n\n\nGang He, an associate professor at CUNY Baruch College, said, Coal’s share in China’s electricity generation is set to fall to about half by 2030 from 58% in 2024. … it reflects a managed transition—recognizing coal’s diminishing but still critical role in maintaining grid stability as renewables expand rapidly.\n\n\n\n\n\nOct 29, 2025\n\n\nLili Pike, Dan Murtaugh\n\n\n\n\n\n\n\n\n\n\n\n\nHow China Raced Ahead of the U.S. on Nuclear Power\n\n\n\nThe New York Times\n\n\n\nThe New York Times covered U.S. China’s nuclear power race and highlighted our work—reproducing our figure and citing our Nature Comment article on nuclear costs.\n\n\n\n\n\nOct 23, 2025\n\n\nBrad Plumer, Harry Stevens\n\n\n\n\n\n\n\n\n\n\n\n\nWhy nuclear is now a booming industry\n\n\n\nThe Economist\n\n\n\nThe Economist covered the nuclear industry’s resurgence and highlighted our work—reproducing our figure and citing our Nature Comment article on nuclear costs.\n\n\n\n\n\nSep 4, 2025\n\n\nThe Economist\n\n\n\n\n\n\n\n\n\n\n\n\nBeyond the Grid: Cost Overload\n\n\n\nThe Highlands Current\n\n\n\nDriven by such factors as economies of scale, global supply chains and advances in technology, the average cost of solar power dropped by over 90 percent between 2010 and 2023, and for onshore and offshore wind by 73 percent and 65 percent, respectively, said Gang He, an energy policy expert from CUNY Baruch College.\n\n\n\n\n\nJul 4, 2025\n\n\nLeonard Sparks\n\n\n\n\n\n\n\n\n\n\n\n\nIn major shift, China moves toward hard carbon targets\n\n\n\nE&E News\n\n\n\n“China has long been hesitant to specify a carbon cap,” said Gang He, “with all the data and implementations in place, China will feel more comfortable to say it will peak its emissions by 2030 at [XX] billion tons and aim to reduce emissions by [XX] percent by 2035.”\n\n\n\n\n\nAug 5, 2024\n\n\nSara Schonhardt\n\n\n\n\n\n\n\n\n\n\n\n\nChina–US climate collaboration concerns as Xie and Kerry step down\n\n\n\nNature\n\n\n\nGang He on the importance to continue the research exchange and colloboration: “If the river freezes over, water will still flow underneath.”\n\n\n\n\n\nMar 12, 2024\n\n\nSmriti Mallapaty\n\n\n\n\n\n\n\n\n\n\n\n\nWhat the U.S.-China Agreement Means for Greenhouse Gas Emissions\n\n\n\nScientific American\n\nE&E News\n\n\n\nThe two nations announced limited steps to address climate change. But even a modest agreement could have far-reaching effects.\n\n\n\n\n\nNov 16, 2023\n\n\nBenjamin Storrow, Sara Schonhardt\n\n\n\n\n\n\n\n\n\n\n\n\nCan the U.S. and China Cooperate on Green Technology Again?\n\n\n\nForeign Policy\n\n\n\nA study published in Nature last year found that if the United States, Germany, and China all decided to shift to fully domestic solar production starting in 2020, by 2030 global solar costs would be 20 percent to 30 percent higher.\n\n\n\n\n\nSep 21, 2023\n\n\nLili Pike\n\n\n\n\n\n\n\n\n\n\n\n\nHow Biden’s made-in-America solar strategy may backfire\n\n\n\nE&E News\n\n\n\nA new study in Nature concludes that policies similar to ones the administration is considering could make solar panels 30 percent more expensive by the end of the decade.\n\n\n\n\n\nOct 27, 2022\n\n\nDavid Iaconangelo\n\n\n\n\n\n\n\n\n\n\n\n\nAsia’s nuclear power dilemma: Ukraine war drives energy turnarounds\n\n\n\nNikkei Asia\n\n\n\nFrom Japan to Singapore, Russia sanctions and carbon-zero targets push states to reconsider nuclear energy.\n\n\n\n\n\nApr 20, 2022\n\n\nDominic Faulder\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis: What does China’s coal push mean for its climate goals?\n\n\n\nCarbon Brief\n\n\n\nDr. He tells Carbon Brief that China has made clear its long-term climate pledges, but “when and at what scale” China peaks its emissions will have a “large” impact on how it and the world would achieve carbon neutrality.\n\n\n\n\n\nMar 29, 2022\n\n\nXiaoying You\n\n\n\n\n\n\n\n\n\n\n\n\nNew energy transition guidance; Five coal plants approved; Energy-efficiency targets raised\n\n\n\nCarbon Brief\n\n\n\nGang He told Carbon Brief: “There are other and better options: renewables are achieving grid parity which could accelerate a renewable-dominant pathway. Such a pathway is both technically feasible and economically beneficial to China’s development.”\n\n\n\n\n\nFeb 17, 2022\n\n\nCarbon Brief Staff\n\n\n\n\n\n\n\n\n\n\n\n\nChina creates vast research infrastructure to support ambitious climate goals\n\n\n\nNature\n\n\n\nCarbon-neutrality institutes, and other initiatives to support a pledge to achieve net zero by 2060, are popping up like mushrooms across China.\n\n\n\n\n\nNov 22, 2021\n\n\nSmriti Mallapaty\n\n\n\n\n\n\n\n\n\n\n\n\nHow China could be carbon neutral by mid-century\n\n\n\nNature\n\n\n\nNature special report examines the role of renewables, nuclear power and carbon capture in reaching this ambitious goal.\n\n\n\n\n\nOct 19, 2020\n\n\nSmriti Mallapaty\n\n\n\n\n\n\n\n\n\n\n\n\nChina Says It Will Stop Releasing CO2 within 40 Years\n\n\n\nScientific American\n\nE&E News\n\n\n\nThe surprise announcement vaults the country ahead of U.S. climate ambitions and could encourage developing countries to follow suit.\n\n\n\n\n\nSep 23, 2020\n\n\nJean Chemnick, Benjamin Storrow\n\n\n\n\n\n\n\n\n\n\n\n\nPlummeting Renewable Energy, Battery Prices Mean China Could Hit 62% Clean Power And Cut Costs 11% By 2030\n\n\n\nForbes\n\n\n\nNew research shows plummeting clean energy prices mean China could reliably run its grids on at least 62% non-fossil electricity generation by 2030, while cutting costs 11% compared to a business-as-usual approach. Once again, it’s cheaper to save the climate than destroy it.\n\n\n\n\n\nAug 10, 2020\n\n\nEnergy Innovation\n\n\n\n\n\n\n\n\n\n\n\n\nSurging coal use in China threatens global CO2 goals\n\n\n\nE&E News\n\n\n\nThe fast decrease in the cost of solar, wind and storage, and technological innovation has fundamentally changed the economics of renewables, said Gang He, our analysis shows that such a fast decarbonization and clean power transition is both technically feasible and economicallybeneficial.\n\n\n\n\n\nJun 9, 2020\n\n\nBenjamin Storrow\n\n\n\n\n\n\n\n\n\n\n\n\nChina’s Path to Clean Energy May Be Smoother than We Previously Thought\n\n\n\nInside Climate News\n\n\n\nA new paper in the journal Nature Communications says the probable reality is much better than we previously thought, largely because of falling costs of wind, solar and battery storage.\n\n\n\n\n\nJun 4, 2020\n\n\nDan Gearino\n\n\n\n\n\n\n\n\n\n\n\n\nBill Calls For An Emissions-Free NY By 2050\n\n\n\nWSHU Public Radio\n\n\n\nIn addition to policy, technology, there are also behavior components to that. How we incentivize people to change their behavior and their lifestyle.\n\n\n\n\n\nFeb 18, 2019\n\n\nJay Shah\n\n\n\n\n\n\n\n\n\n\n\n\nWhere is the world’s greenest city?\n\n\n\nThe Guardian\n\n\n\nIn a 2015 study published in the journal Ecological Indicators, scientists based at the Lawrence Berkeley National Laboratory in California have fine-tuned a potential method for assessing Chinese “eco-cities” using 33 key indicators.\n\n\n\n\n\nApr 2, 2015\n\n\nHayley Birch\n\n\n\n\n\n\n\n\n\n\n\n\nWestern companies gave China power projects a boost\n\n\n\nThe Seattle Times\n\n\n\nThe rejections exposed cracks at the core of how carbon credits are verified in developing economies, according to a study of the Chinese wind-power industry by Gang He and Richard Morse of Stanford University.\n\n\n\n\n\nMay 5, 2014\n\n\nHal Bernton\n\n\n\n\n\n\n\n\n\n\n\n\nBeijing’s record smog poses health nightmare as China plans ‘green’ energy future\n\n\n\nE&E News\n\n\n\nI see it as a system failure exposed in a bad weather condition, said Gang He. The government has to first figure out the sources of air pollutants in order to adopt the right measures.\n\n\n\n\n\nMar 25, 2013\n\n\nKandy Wong\n\n\n\n\n\n\n\n\n\n\n\n\nGovernment conflicts could slow shale gas development\n\n\n\nE&E News\n\n\n\nThe priorities between the central and local governments are often different, said Gang He, the top concern of the central government is energy security while economic growth matters most for local governments.\n\n\n\n\n\nMay 9, 2012\n\n\nKandy Wong\n\n\n\n\n\n\n\n\n\n\n\n\nBeijing Emission Cuts May Underestimate Use of Coal\n\n\n\nScientific American\n\nE&E News\n\n\n\nIt should be noted that the coal industry is gradualtly being opened to market, while the electricity industry is still highly regulated. The scattered management of energy issues within the central government makes the energy administration’s mission very difficult one.\n\n\n\n\n\nMay 7, 2012\n\n\nKandy Wong\n\n\n\n\n\n\n\n\n\n\n\n\nSeeking a Pacific Northwest Gateway for U.S. Coal\n\n\n\nNational Geographic\n\n\n\nThe middle kingdom’s appetite for imported coal seems insatiable, wrote researchers Richard Morse and Gang He in a 2010 working paperm refering to China. And the China Factor appears to have ushered in a new paradigm for the global coal market.\n\n\n\n\n\nOct 20, 2011\n\n\nStacey Schultz\n\n\n\n\n\n\n\n\n\n\n\n\nDrop in CO2 in U.S. and Power Use in China – for Now\n\n\n\nThe New York Times\n\n\n\nResearchers at Stanford University say blazing growth in the generation of electricity in China ended last year as the global recession struck.\n\n\n\n\n\nMay 21, 2009\n\n\nAndrew Revkin\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Anokhin, Ivan, Rishav Rishav, Matthew Riemer, Stephen Chung, Irina Rish, and Samira Ebrahimi Kahou. 2025. “Handling Delay in Reinforcement Learning Caused by Parallel Computations of Neurons.” In The Thirteenth International Conference on Learning Representations.\n\n\nAzimi, Rambod, Rishav Rishav, Marek Teichmann, and Samira Ebrahimi Kahou. 2024. “KD-LoRA: A Hybrid Approach to Efficient Fine-Tuning with LoRA and Knowledge Distillation.” In NeurIPS ENLSP Workshop. https://arxiv.org/abs/2410.20777.\n\n\nNath, Somjit, Rishav Rishav, Gopeshh Subbaraj, Derek Nowrouzezahrai, and Samira Ebrahimi Kahou. 2025. “Behavioral Suite Analysis of Self-Supervised Learning in Atari.” In Reinforcement Learning and Video Games Workshop @ RLC 2025.\n\n\nRishav, Rishav, Ramy Battrawy, René Schuster, Oliver Wasenmüller, and Didier Stricker. 2020. “DeepLiDARFlow: A Deep Learning Architecture for Scene Flow Estimation Using Monocular Camera and Sparse LiDAR.” In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 10460–67. https://doi.org/10.1109/IROS45743.2020.9341077.\n\n\nRishav, Rishav, Somjit Nath, Vincent Michalski, and Samira Ebrahimi Kahou. 2025. “Behaviour Discovery and Attribution for Explainable Reinforcement Learning.” In Transactions on Machine Learning Research. https://arxiv.org/abs/2503.14973.\n\n\nRishav, Rishav, René Schuster, Ramy Battrawy, Oliver Wasenmüller, and Didier Stricker. 2021. “ResFPN: Residual Skip Connections in Multi-Resolution Feature Pyramid Networks for Accurate Dense Pixel Matching.” In 25th International Conference on Pattern Recognition (ICPR), 180–87. https://doi.org/10.1109/ICPR48806.2021.9412750."
  },
  {
    "objectID": "notable-policy-citations.html",
    "href": "notable-policy-citations.html",
    "title": "Notable Policy Citations",
    "section": "",
    "text": "Helveston, John, Gang He*, and Michael Davidson. 2022. Quantifying the Cost Savings of Global Solar Photovoltaic Supply Chains. Nature 612: 83–87. doi: 10.1038/s41586-022-05316-6. Cited by:\n\nUN, OECD, The World Bank, WTO and IMF. 2024. Working Together for Better Climate Action: Carbon pricing, policy spillovers, and global climate goals.\nUK Parliament Select Committee Publications. 2024. Written evidence submitted by UK Trade for Net Zero.\nWorld Bank. 2024. World Development Report 2024: The Middle-Income Trap.\nInternational Renewable Energy Agency. 2024. Renewable energy and jobs: Annual review 2024.\nWorld Trade Organization. 2023. Trade Policy Tools for Climate Action.\nWorld Trade Organization. 2023. Global Value Chain Development Report 2023.\n\nSource: Altmetric and PlumX"
  },
  {
    "objectID": "notable-policy-citations.html#nature-2022",
    "href": "notable-policy-citations.html#nature-2022",
    "title": "Notable Policy Citations",
    "section": "",
    "text": "Helveston, John, Gang He*, and Michael Davidson. 2022. Quantifying the Cost Savings of Global Solar Photovoltaic Supply Chains. Nature 612: 83–87. doi: 10.1038/s41586-022-05316-6. Cited by:\n\nUN, OECD, The World Bank, WTO and IMF. 2024. Working Together for Better Climate Action: Carbon pricing, policy spillovers, and global climate goals.\nUK Parliament Select Committee Publications. 2024. Written evidence submitted by UK Trade for Net Zero.\nWorld Bank. 2024. World Development Report 2024: The Middle-Income Trap.\nInternational Renewable Energy Agency. 2024. Renewable energy and jobs: Annual review 2024.\nWorld Trade Organization. 2023. Trade Policy Tools for Climate Action.\nWorld Trade Organization. 2023. Global Value Chain Development Report 2023.\n\nSource: Altmetric and PlumX"
  },
  {
    "objectID": "notable-policy-citations.html#nature-energy-2023",
    "href": "notable-policy-citations.html#nature-energy-2023",
    "title": "Notable Policy Citations",
    "section": "Nature Energy 2023",
    "text": "Nature Energy 2023\nLiu, Laibao*, Gang He, Mengxi Wu*, Gang Liu, Haoran Zhang, Ying Chen, Jiashu Shen, and Shuangcheng Li. 2023. Climate Change Impacts on Planned Supply–Demand Match in Global Wind and Solar Energy Systems. Nature Energy 8(8): 870–880. doi: 10.1038/s41560-023-01304-w. Cited by:\n\nUNEP. 2024. Emissions gap report 2024: no more hot air please: with a massive gap between rhetoric and reality, countries draft new climate commitments.\n\nSource: PlumX"
  },
  {
    "objectID": "notable-policy-citations.html#one-earth-2020",
    "href": "notable-policy-citations.html#one-earth-2020",
    "title": "Notable Policy Citations",
    "section": "One Earth 2020",
    "text": "One Earth 2020\nHe, Gang*, Jiang Lin, Ying Zhang, Wenhua Zhang, Guilherme Larangeira, Chao Zhang, Wei Peng, Manzhi Liu, and Fuqiang Yang. 2020. Enabling a Rapid and Just Transition Away from Coal in China. One Earth 3(2): 187–94. doi: 10.1016/j.oneear.2020.07.012. Cited by:\n\nBrookings Institution. 2023. Power play: How the US benefits if China greens the Global South.\nIPCC. 2022. Chapter 6 Energy Systems, Climate Change 2022: mitigation of climate change: Working Group III Contribution to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change.\nWorld Bank. 2022. China Country Climate and Development Report.\nCanadian Centre for Policy Alternatives. 2022. Bet Big: A citizen’s guide to green industrial policy in Canada.\nCenter for Strategic and International Studies. 2021. Why Won’t Major Coal-Dependent Countries Sign on to a Coal Phaseout Deal?.\n\nSource: Altmetric and PlumX"
  },
  {
    "objectID": "notable-policy-citations.html#nature-communications-2020",
    "href": "notable-policy-citations.html#nature-communications-2020",
    "title": "Notable Policy Citations",
    "section": "Nature Communications 2020",
    "text": "Nature Communications 2020\nHe, Gang*, Jiang Lin*, Froylan Sifuentes, Xu Liu, Nikit Abhyankar, and Amol Phadke*. 2020. Rapid Cost Decrease of Renewables and Storage Accelerates the Decarbonization of China’s Power System. Nature Communications 11(1): 2486. doi: 10.1038/s41467-020-16184-x. Cited by:\n\nIIASA. 2021. Understanding the employment and fiscal consequences of coal phase-out in China.\nGrantham Institute. 2020. From rescue to recovery: towards a sustainable transition for China after the COVID-19 pandemic – Centre for Climate Change Economics and Policy.\nInstitute of International and European Affairs. 2020. Course Correction: China is going Carbon Neutral.\n\nSource: PlumX"
  },
  {
    "objectID": "notable-policy-citations.html#energy-policy-2017",
    "href": "notable-policy-citations.html#energy-policy-2017",
    "title": "Notable Policy Citations",
    "section": "Energy Policy 2017",
    "text": "Energy Policy 2017\nHe, Gang*, David Victor. 2017. Experiences and lessons from China’s success in providing electricity for all. Resources, Conservation and Recycling 122: 335–338. doi: 10.1016/j.resconrec.2017.03.011. Cited by:\n\nMekong River Commission. 2019. State of the Basin Report 2018.\nWorld Bank. 2017. A Retrospective Analysis of the Role of Isolated and Mini Grids in Power System Development.\n\nSource: PlumX"
  },
  {
    "objectID": "notable-policy-citations.html#environmental-science-and-technology-2016",
    "href": "notable-policy-citations.html#environmental-science-and-technology-2016",
    "title": "Notable Policy Citations",
    "section": "Environmental Science and Technology 2016",
    "text": "Environmental Science and Technology 2016\nHe, Gang*, Anne-Perrine Avrin, James H. Nelson, Josiah Johnston, Ana Mileva, Jianwei Tian, and Daniel M. Kammen*. 2016. SWITCH-China: A Systems Approach to Decarbonizing China’s Power System. Environmental Science & Technology 50(11): 5467–5473. doi: 10.1021/acs.est.6b01345. Cited by:\n\nHouse Science, Space, and Technology Subcommittee on Energy. 2021. Fostering Equity in Energy Innovation.\nIRENA. 2017. Planning for the renewable future: Long-term modelling and tools to expand variable renewable power in emerging economies.\n\nSource: PlumX"
  },
  {
    "objectID": "notable-policy-citations.html#energy-policy-2013",
    "href": "notable-policy-citations.html#energy-policy-2013",
    "title": "Notable Policy Citations",
    "section": "Energy Policy 2013",
    "text": "Energy Policy 2013\nHe, Gang*, Richard Morse. 2013. Addressing Carbon Offsetters’ Paradox: Lessons from Chinese Wind CDM. Energy Policy 63: 1051–1055. doi: 10.1016/j.enpol.2013.09.021. Cited by:\n\nThe ifo Institute (Leibniz Institute for Economic Research at the University of Munich). 2024. Sequencing Carbon Dioxide Removal into the EU ETS.\nUNFCCC. 2023. Call for input 2023 - Issues included in the annotated agenda and related annexes of the eighth meeting of the Article 6.4.\nStockholm Environment Institute. 2017. International transfers under Article 6 of the Paris Agreement.\nStockholm Environment Institute. 2017. Using the Clean Development Mechanism for nationally determined contributions and international aviation.\n\nSource: PlumX\nNote: Data as of December 2024"
  },
  {
    "objectID": "notes/cuny-baruch-visiting-scholar-paperwork-guide.html",
    "href": "notes/cuny-baruch-visiting-scholar-paperwork-guide.html",
    "title": "CUNY Baruch College Visiting Scholar Paperwork Guide",
    "section": "",
    "text": "flowchart LR\n  A[Conditional Invitation] --&gt; \n  B(Appointment Letter) --&gt;\n  C{{DS-2019}} --&gt;\n  D{{Visa Application}}  --&gt;\n  E(Visiting Baruch)\n\n\n\n\n\n\n\n\nTo begin, you need to secure a conditional invitation letter from a faculty member who will support your scholarship application. The following documents are usually helpful:\n\nCV\nResearch Plan\nEnglish Proficiency Certificate\n\nYour potential host may schedule an interview to discuss your proposal and assess the possibilities.\n\n\n\nOnce you receive the fellowship or scholarship certificate, forward it to your host faculty member. The certificate is useful for the department or school to issue an official invitation letter. You may ignore the first step if you were hired or support by your host faculty.\n\nThe official letter, similar to the conditional invitation, will be signed by the Department Chair or Dean.\nThis step formalizes your appointment and enables the next stages of paperwork.\n\n\n\n\nThe DS-2019 form involves several internal approval processes before the designated office can issue it. These include:\n\nApprovals from the Department Chair, Dean, and Provost\n\nExport Control Clearance\n\nOverview of the Process from the Baruch College Export Control Clearance for J-1 Research Scholar\n\nTo comply with the “Guidance on Onboarding J-1 Researchers and Scholars Exchange Visitors” memorandum dated July 26, 2021 from Robert T. Maruca, CUNY Associate University Provost for Planning, and Tamera Schneider, CUNY Associate Vice Chancellor & University Provost for Research, the Weissman Center for International Business has devised the following procedure for obtaining export control clearance for all prospective J-1 Research Scholars.*\nThe process involves several steps.\nThe process begins whenever a Baruch department or other College office (“Department”) receives an inquiry from someone requesting to be hosted as a prospective international J-1 Research Scholar (“Scholar”). If the Department decides it wishes to host this Scholar, these steps must be taken:\nPhase I: Departmental Preparation\n\nThe Department sends two forms to the Scholar:\n\n\n\nApplication for Prospective International J-1 Scholar\nCUNY J-1 Export-Control Questionnaire\n\n\nThe Scholar returns the completed and signed forms along with certified scanned copies of the required documentation, plus a current CV/Resume, to the Department.\nThe Department completes and signs these two forms:\n\n\n\nBaruch College Application to Host International J-1 Scholar\n\nCUNY Foreign Influence due Diligence Form for J1-Visiting Scholar\n\n\nThe Department submits the following materials (completed and with all required signatures) to the SEVIS Responsible Officer (RO) for this J-1 program (currently Dr. Richard Mitten):\n\n\nApplication for Prospective International J-1 Scholar and accompanying documentation (completed by Scholar)\n\nCUNY J-1 Export-Control Questionnaire (completed by Scholar)\n\nCV/Resume of the Scholar (completed by Scholar)\n\nBaruch College Application to Host International J-1 Scholar with required signatures (completed by Department)\n\nCUNY Foreign Influence due Diligence Form for J1-Visiting Scholar (completed by Department)\nAn original official letter of invitation (completed by Department)\n\nPhase II: Preliminary Campus Review\n\nThe RO reviews documents and submits them to the Baruch College Provost for preliminary campus review.\nAfter consulting with the “appropriate stakeholders” (per CUNY regulations), the Provost returns the entire dossier to the SEVIS RO, recording the decision with a signature and date on page 2 of this form (Provisional Approval-Provost).\n\nPhase III: CUNY Screening\n\nIf the Provost approves the request to host the Scholar, the RO forwards the dossier to the CUNY Office of International Student and Scholar Services (“CUNY ISSS”) for “Visual Compliance Screening” (per CUNY regulations).\nAfter completing its screening, the CUNY ISSS returns the dossier to the RO with its findings.\nThe RO forwards the dossier to the Provost for final campus review.\n\nPhase IV: Final Campus Review\n\nThe Provost approves, or withholds final approval to the request to host the Scholar, and conveys this decision, along with the dossier of documents, to the RO, recording the decision with a signature and date on Export Control Clearance Form for Prospective J-1 Research Scholar.\nIf the Provost approves the request, the RO notifies the Department, issues the DS-2019 for the Scholar, and sends it and additional documentation needed to apply for a J-1 visa to the Scholar, with a copy to the Department.\n\n*Please note: The process for described below applies to any prospective J-1 Research Scholar only (Short-Term Scholars are processed by the International Student Service Center).\n\nAdditional documents:\nFinancial ability to cover the living expenses for J-1 exchange visitors & J-2 dependents:\n\nJ-1 exchange visitor $32,000/year; $2,700/month (research scholar, professor, short-term scholar & student)\n\nJ-2 spouse $7,200/year; $600/month\n\nJ-2 child (under 21) $4,800/year per child; $400/month per child\n\nIf the scholarship amount is less than the listed requirement, you may need to provide a bank certificate of personal savings to cover the difference.\nMinimum insurance coverage must provide:\n\nMedical benefits of at least $100,000 per accident or illness;\n\nRepatriation of remains in the amount of $25,000;\n\nExpenses associated with the medical evacuation of exchange visitors to his or her home country in the amount of $50,000; and\n\nDeductibles not to exceed $500 per accident or illness.\n\n\n\n\nCheck the latest information from the U.S. Embassy website to understand the requirements and materials needed for your visa application. Make sure to complete this step promptly after receiving your DS-2019 form.\n\n\n\nOnce all paperwork is completed, you can prepare for your arrival at Baruch College in New York City! New York City is Your Classroom and Office!\n\n\n\n\nTimeline\n\nThe entire process may take several months, particularly if additional documentation is requested. Begin early and allow flexibility in your start date to accommodate any delays.\n\nProactive Communication\n\nKeep in close contact with your host faculty and administrative offices to ensure you stay updated on requirements and approvals.\n\nUnofficial Guide\n\nThis is an information page for your reference only. You should always follow the official guide to prepare your documents."
  },
  {
    "objectID": "notes/cuny-baruch-visiting-scholar-paperwork-guide.html#paperwork-flow",
    "href": "notes/cuny-baruch-visiting-scholar-paperwork-guide.html#paperwork-flow",
    "title": "CUNY Baruch College Visiting Scholar Paperwork Guide",
    "section": "",
    "text": "flowchart LR\n  A[Conditional Invitation] --&gt; \n  B(Appointment Letter) --&gt;\n  C{{DS-2019}} --&gt;\n  D{{Visa Application}}  --&gt;\n  E(Visiting Baruch)\n\n\n\n\n\n\n\n\nTo begin, you need to secure a conditional invitation letter from a faculty member who will support your scholarship application. The following documents are usually helpful:\n\nCV\nResearch Plan\nEnglish Proficiency Certificate\n\nYour potential host may schedule an interview to discuss your proposal and assess the possibilities.\n\n\n\nOnce you receive the fellowship or scholarship certificate, forward it to your host faculty member. The certificate is useful for the department or school to issue an official invitation letter. You may ignore the first step if you were hired or support by your host faculty.\n\nThe official letter, similar to the conditional invitation, will be signed by the Department Chair or Dean.\nThis step formalizes your appointment and enables the next stages of paperwork.\n\n\n\n\nThe DS-2019 form involves several internal approval processes before the designated office can issue it. These include:\n\nApprovals from the Department Chair, Dean, and Provost\n\nExport Control Clearance\n\nOverview of the Process from the Baruch College Export Control Clearance for J-1 Research Scholar\n\nTo comply with the “Guidance on Onboarding J-1 Researchers and Scholars Exchange Visitors” memorandum dated July 26, 2021 from Robert T. Maruca, CUNY Associate University Provost for Planning, and Tamera Schneider, CUNY Associate Vice Chancellor & University Provost for Research, the Weissman Center for International Business has devised the following procedure for obtaining export control clearance for all prospective J-1 Research Scholars.*\nThe process involves several steps.\nThe process begins whenever a Baruch department or other College office (“Department”) receives an inquiry from someone requesting to be hosted as a prospective international J-1 Research Scholar (“Scholar”). If the Department decides it wishes to host this Scholar, these steps must be taken:\nPhase I: Departmental Preparation\n\nThe Department sends two forms to the Scholar:\n\n\n\nApplication for Prospective International J-1 Scholar\nCUNY J-1 Export-Control Questionnaire\n\n\nThe Scholar returns the completed and signed forms along with certified scanned copies of the required documentation, plus a current CV/Resume, to the Department.\nThe Department completes and signs these two forms:\n\n\n\nBaruch College Application to Host International J-1 Scholar\n\nCUNY Foreign Influence due Diligence Form for J1-Visiting Scholar\n\n\nThe Department submits the following materials (completed and with all required signatures) to the SEVIS Responsible Officer (RO) for this J-1 program (currently Dr. Richard Mitten):\n\n\nApplication for Prospective International J-1 Scholar and accompanying documentation (completed by Scholar)\n\nCUNY J-1 Export-Control Questionnaire (completed by Scholar)\n\nCV/Resume of the Scholar (completed by Scholar)\n\nBaruch College Application to Host International J-1 Scholar with required signatures (completed by Department)\n\nCUNY Foreign Influence due Diligence Form for J1-Visiting Scholar (completed by Department)\nAn original official letter of invitation (completed by Department)\n\nPhase II: Preliminary Campus Review\n\nThe RO reviews documents and submits them to the Baruch College Provost for preliminary campus review.\nAfter consulting with the “appropriate stakeholders” (per CUNY regulations), the Provost returns the entire dossier to the SEVIS RO, recording the decision with a signature and date on page 2 of this form (Provisional Approval-Provost).\n\nPhase III: CUNY Screening\n\nIf the Provost approves the request to host the Scholar, the RO forwards the dossier to the CUNY Office of International Student and Scholar Services (“CUNY ISSS”) for “Visual Compliance Screening” (per CUNY regulations).\nAfter completing its screening, the CUNY ISSS returns the dossier to the RO with its findings.\nThe RO forwards the dossier to the Provost for final campus review.\n\nPhase IV: Final Campus Review\n\nThe Provost approves, or withholds final approval to the request to host the Scholar, and conveys this decision, along with the dossier of documents, to the RO, recording the decision with a signature and date on Export Control Clearance Form for Prospective J-1 Research Scholar.\nIf the Provost approves the request, the RO notifies the Department, issues the DS-2019 for the Scholar, and sends it and additional documentation needed to apply for a J-1 visa to the Scholar, with a copy to the Department.\n\n*Please note: The process for described below applies to any prospective J-1 Research Scholar only (Short-Term Scholars are processed by the International Student Service Center).\n\nAdditional documents:\nFinancial ability to cover the living expenses for J-1 exchange visitors & J-2 dependents:\n\nJ-1 exchange visitor $32,000/year; $2,700/month (research scholar, professor, short-term scholar & student)\n\nJ-2 spouse $7,200/year; $600/month\n\nJ-2 child (under 21) $4,800/year per child; $400/month per child\n\nIf the scholarship amount is less than the listed requirement, you may need to provide a bank certificate of personal savings to cover the difference.\nMinimum insurance coverage must provide:\n\nMedical benefits of at least $100,000 per accident or illness;\n\nRepatriation of remains in the amount of $25,000;\n\nExpenses associated with the medical evacuation of exchange visitors to his or her home country in the amount of $50,000; and\n\nDeductibles not to exceed $500 per accident or illness.\n\n\n\n\nCheck the latest information from the U.S. Embassy website to understand the requirements and materials needed for your visa application. Make sure to complete this step promptly after receiving your DS-2019 form.\n\n\n\nOnce all paperwork is completed, you can prepare for your arrival at Baruch College in New York City! New York City is Your Classroom and Office!\n\n\n\n\nTimeline\n\nThe entire process may take several months, particularly if additional documentation is requested. Begin early and allow flexibility in your start date to accommodate any delays.\n\nProactive Communication\n\nKeep in close contact with your host faculty and administrative offices to ensure you stay updated on requirements and approvals.\n\nUnofficial Guide\n\nThis is an information page for your reference only. You should always follow the official guide to prepare your documents."
  },
  {
    "objectID": "notes/cuny-baruch-visiting-scholar-paperwork-guide.html#links",
    "href": "notes/cuny-baruch-visiting-scholar-paperwork-guide.html#links",
    "title": "CUNY Baruch College Visiting Scholar Paperwork Guide",
    "section": "Links",
    "text": "Links\n\nBaruch Faculty Profiles\nStudy Abroad Office\nCUNY International Student & Scholar Services (ISSS)\nCUNY Export Control\n\nCreated: Januaray 3, 2025"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "What’s New & Updated",
    "section": "",
    "text": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections\n\n\n\nai\n\n\n\nmHC\n\n\n\n\n\nJan 3, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nReal-Time Reinforcement Learning\n\n\n\nai\n\n\n\nArticle on real-time reinforcement learning\n\n\n\n\n\nJun 20, 2025\n\n\nIvan Anokhin, Matthew Riemer, Rishav, Gopeshh Subbaraj, Glen Berseth\n\n\n\n\n\n\n\n\n\n\n\n\nGSPO vs GRPO - Theory, Practice, and the Limits of Approximation\n\n\n\nai\n\n\n\nGSPO vs GRPO\n\n\n\n\n\nNov 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDistributed Training with JAX Simplified\n\n\n\nai\n\n\n\nTraining a large model like GPT-3 with JAX.\n\n\n\n\n\nOct 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration\n\n\n\nai\n\n\n\nCuda Programming\n\n\n\n\n\nMar 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPrecision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System\n\n\n\nai\n\n\n\nSpot Spraying Project\n\n\n\n\n\nDec 24, 2024\n\n\n\n\n\nNo matching items"
  }
]