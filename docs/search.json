[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Anokhin, Ivan, Rishav Rishav, Matthew Riemer, Stephen Chung, Irina Rish, and Samira Ebrahimi Kahou. 2025. “Handling Delay in Reinforcement Learning Caused by Parallel Computations of Neurons.” In The Thirteenth International Conference on Learning Representations.\n\n\nAzimi, Rambod, Rishav Rishav, Marek Teichmann, and Samira Ebrahimi Kahou. 2024. “KD-LoRA: A Hybrid Approach to Efficient Fine-Tuning with LoRA and Knowledge Distillation.” In NeurIPS ENLSP Workshop. https://arxiv.org/abs/2410.20777.\n\n\nNath, Somjit, Rishav Rishav, Gopeshh Subbaraj, Derek Nowrouzezahrai, and Samira Ebrahimi Kahou. 2025. “Behavioral Suite Analysis of Self-Supervised Learning in Atari.” In Reinforcement Learning and Video Games Workshop @ RLC 2025.\n\n\nRishav, Rishav, Ramy Battrawy, René Schuster, Oliver Wasenmüller, and Didier Stricker. 2020. “DeepLiDARFlow: A Deep Learning Architecture for Scene Flow Estimation Using Monocular Camera and Sparse LiDAR.” In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 10460–67. https://doi.org/10.1109/IROS45743.2020.9341077.\n\n\nRishav, Rishav, Somjit Nath, Vincent Michalski, and Samira Ebrahimi Kahou. 2025. “Behaviour Discovery and Attribution for Explainable Reinforcement Learning.” In Transactions on Machine Learning Research. https://arxiv.org/abs/2503.14973.\n\n\nRishav, Rishav, René Schuster, Ramy Battrawy, Oliver Wasenmüller, and Didier Stricker. 2021. “ResFPN: Residual Skip Connections in Multi-Resolution Feature Pyramid Networks for Accurate Dense Pixel Matching.” In 25th International Conference on Pattern Recognition (ICPR), 180–87. https://doi.org/10.1109/ICPR48806.2021.9412750."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "My research and implementation projects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpot Spraying\n\n\nMy project at DFKI with HYDAC. Built a robotic system for identifying & localizing weeds/crops for spot-spraying.\n\n\n\n\n\n\n\n\n\n\n\n\nHSID-CNN\n\n\nDeep Learning for denoising hyperspectral images (AVIRIS). Work completed at Pixxel, Bengaluru.\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative Adversarial Networks for Visual Odometry\n\n\nUnofficial PyTorch implementation for GANVO - a GAN-based visual odometry depth estimation.\n\n\n\n\n\n\n\n\n\n\n\n\nUnsupervised Cross Spectral Stereo Matching\n\n\nPyTorch implementation of ‘Unsupervised Cross-spectral Stereo Matching by Learning to Synthesize’.\n\n\n\n\n\n\n\n\n\n\n\n\nDeepLiDARFlow\n\n\nBachelor thesis; worked on scene flow estimation using monocular camera & sparse LiDAR. Work accepted at two conferences.\n\n\n\n\n\n\n\n\n\n\n\n\nCompiler Construction\n\n\nConstructed a compiler for a given language specification in C language. A project at BITS Pilani.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/deeplidar/index.html",
    "href": "projects/deeplidar/index.html",
    "title": "DeepLiDARFlow",
    "section": "",
    "text": "Abstract\nScene flow is the dense 3D reconstruction of motion and geometry of a scene. Most state-of-the-art methods use a pair of stereo images as input for full scene reconstruction. These methods depend a lot on the quality of the RGB images and perform poorly in regions with reflective objects, shadows, ill-conditioned light environment and so on. LiDAR measurements are much less sensitive to the aforementioned conditions but LiDAR features are in general unsuitable for matching tasks due to their sparse nature. Hence, using both LiDAR and RGB can potentially overcome the individual disadvantages of each sensor by mutual improvement and yield robust features which can improve the matching process. In this paper, we present DeepLiDARFlow, a novel deep learning architecture which fuses high level RGB and LiDAR features at multiple scales in a monocular setup to predict dense scene flow. Its performance is much better in the critical regions where image-only and LiDAR-only methods are inaccurate. We verify our DeepLiDARFlow using the established data sets KITTI and FlyingThings3D and we show strong robustness compared to several state-of-the-art methods which used other input modalities.\n\n\nResources\n\nCode Repository: GitHub Link\nPaper: ArXiV"
  },
  {
    "objectID": "projects/ganvo/index.html",
    "href": "projects/ganvo/index.html",
    "title": "Generative Adversarial Networks for Visual Odometry",
    "section": "",
    "text": "Resources\n\nCode Repository: GitHub Link"
  },
  {
    "objectID": "projects/sporsprayig/index.html",
    "href": "projects/sporsprayig/index.html",
    "title": "Spot Spraying",
    "section": "",
    "text": "My project at DFKI with HYDAC. Built a robotic system for identifying & localizing weeds/crops for spot-spraying. More about the project:"
  },
  {
    "objectID": "projects/sporsprayig/index.html#spot-spraying",
    "href": "projects/sporsprayig/index.html#spot-spraying",
    "title": "Spot Spraying",
    "section": "",
    "text": "My project at DFKI with HYDAC. Built a robotic system for identifying & localizing weeds/crops for spot-spraying. More about the project:"
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html",
    "href": "posts/2025-3-20-cudp/index.html",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "",
    "text": "So you have been thinking about CUDA programming from a while and now want to learn but are confused how to proceed and know about memory models, etc. in CUDA. Well, even I am confused, so let’s learn together. This is the first blog of the CUDA series where the end goal is to code DQN in C and accelerate training and inference with CUDA.\nWith AI rapidly advancing, its integration into high-stakes environments is becoming inevitable. In this landscape, efficient deployment is arguably more critical than the pace of new research. As the number of AI tools continues to grow, efficiency may well become the defining moat for the tools that endure.\nFor nearly every use case, there’s potential to unlock significant performance gains — often by writing custom CUDA kernels tailored to the task.\nRL lives in my heart. Thus, the end goal of this learning experience is to write a DQN completely in C and accelerate the training using CUDA kernels.\nSo let’s go…!\nLet’s start off with memory organization in CUDA, then intuitively I will explain my first program: matrix multiplication in CUDA."
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#first-principles-memory-organization-in-cuda",
    "href": "posts/2025-3-20-cudp/index.html#first-principles-memory-organization-in-cuda",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "First Principles: Memory Organization in CUDA",
    "text": "First Principles: Memory Organization in CUDA\nBefore writing a single kernel, let’s zoom into the GPU’s memory layout. This is where most CUDA bugs (and performance pitfalls) are born.\nIn CUDA, like in C/C++, matrices are stored in row-major order.\n\nWhat does row-major mean?\nImagine a 2D matrix:\nA = [[1, 2, 3],\n     [4, 5, 6]]\nEven though it’s 2D logically, it’s stored in a flat 1D array as:\n[1, 2, 3, 4, 5, 6]\nTo access the element at A[row][col], the index becomes:\nindex = row * numCols + col\nFor the above matrix, A[1][2] is 1*3 + 2 = 5, and sure enough, A[5] = 6.\nThis little indexing trick — row * numCols + col — is foundational. We’ll use it repeatedly in our CUDA code to compute where each value lives in memory."
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#matrix-multiplication-recap",
    "href": "posts/2025-3-20-cudp/index.html#matrix-multiplication-recap",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "Matrix Multiplication Recap",
    "text": "Matrix Multiplication Recap\nWe have two matrices:\n\nA of size M × K\nB of size K × N\n\nWe want to compute: - C = A × B → a matrix of size M × N\nThe formula for each element of C is:\nC[row][col] = sum over i of A[row][i] * B[i][col]\nThat’s a dot product between a row from A and a column from B."
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#writing-the-cuda-kernel",
    "href": "posts/2025-3-20-cudp/index.html#writing-the-cuda-kernel",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "Writing the CUDA Kernel",
    "text": "Writing the CUDA Kernel\nHere’s our minimal working kernel:\n__global__ void matMulKernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row &lt; M && col &lt; N) {\n        float sum = 0.0f;\n        for (int i = 0; i &lt; K; ++i) {\n            sum += A[row * K + i] * B[i * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}"
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#thread-indexing-why-.y-for-row-and-.x-for-col",
    "href": "posts/2025-3-20-cudp/index.html#thread-indexing-why-.y-for-row-and-.x-for-col",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "Thread Indexing: Why .y for Row and .x for Col?",
    "text": "Thread Indexing: Why .y for Row and .x for Col?\nint row = blockIdx.y * blockDim.y + threadIdx.y;\nint col = blockIdx.x * blockDim.x + threadIdx.x;\n\nThreads are arranged in a 2D grid\nEach thread computes one element of the result matrix C[row][col]\nRows → vertical → Y-axis\nColumns → horizontal → X-axis\n\nHence, row is computed using .y components, and col using .x.\nMakes your mental model clean and your code correct."
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#memory-access-why-row-n-col",
    "href": "posts/2025-3-20-cudp/index.html#memory-access-why-row-n-col",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "Memory Access: Why row * N + col?",
    "text": "Memory Access: Why row * N + col?\nThis was one of the core doubts that tripped me up early on:\n\n“Shouldn’t it be row * M + col since the matrix has M rows?”\n\nTurns out — no. It’s row * N + col because:\n\nWe’re accessing matrix C, which is M × N\nEach row of C has N elements\nSo to access the start of row, we skip row * N elements\nThen move col steps in → row * N + col\n\nAlways remember: it’s about how many columns per row, not how many rows in total."
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#what-about-bs-indexing-bi-n-col",
    "href": "posts/2025-3-20-cudp/index.html#what-about-bs-indexing-bi-n-col",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "What about B’s indexing: B[i * N + col]?",
    "text": "What about B’s indexing: B[i * N + col]?\nAnother “aha!” moment:\n\n“Aren’t we accessing B column-wise here? Shouldn’t it be column-major?”\n\nGood instinct — but even though we’re accessing down a column, we’re still treating B as row-major.\n\nWe’re accessing B[i][col]\nSince B has N columns, row-major indexing says:\n\nB[i][col] = i * N + col\nSo B[i * N + col] is still perfectly valid row-major indexing, even if the access pattern feels “column-ish”."
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#edge-case-gotcha-tiny-matrices",
    "href": "posts/2025-3-20-cudp/index.html#edge-case-gotcha-tiny-matrices",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "Edge Case Gotcha: Tiny Matrices",
    "text": "Edge Case Gotcha: Tiny Matrices\nSay we multiply:\n\nA = [1 2 3] → 1×3\nB = [1, 2, 3]ᵗ → 3×1\n\nThe result should be:\n1×1 = [1×1 + 2×2 + 3×3] = [14]\nBut your code might crash or give garbage. Why?\nBecause your kernel might launch a full grid of threads, and many of them will access memory out-of-bounds.\nSolution:\nif (row &lt; M && col &lt; N)\nAlways bound your threads. Especially with small matrices."
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#test-case",
    "href": "posts/2025-3-20-cudp/index.html#test-case",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "Test Case",
    "text": "Test Case\nHere’s a minimal test:\nint M = 1, K = 3, N = 1;\nfloat A[] = {1, 2, 3};   // 1x3\nfloat B[] = {1, 2, 3};   // 3x1\nfloat C[1];              // 1x1 output\n\n// Launch with grid/block of 1\ndim3 blockSize(1, 1);\ndim3 gridSize(1, 1);\n\n// Run the kernel → expect C[0] = 14"
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#the-end-goal-dqn-in-pure-c-cuda",
    "href": "posts/2025-3-20-cudp/index.html#the-end-goal-dqn-in-pure-c-cuda",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "The End Goal: DQN in Pure C + CUDA",
    "text": "The End Goal: DQN in Pure C + CUDA\nThis matrix multiplication was a warm-up.\nMy goal? To build Deep Q-Networks (DQN) completely in C, and accelerate training with CUDA — no PyTorch, no TensorFlow. Just raw speed and full control.\nReinforcement learning lives in my heart. And this journey is all about learning from first principles."
  },
  {
    "objectID": "posts/2025-3-20-cudp/index.html#whats-next",
    "href": "posts/2025-3-20-cudp/index.html#whats-next",
    "title": "CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration",
    "section": "What’s Next?",
    "text": "What’s Next?\n\nTiled matrix multiplication (for speed!)\nShared memory optimization\nWriting ReLU, Softmax, Linear layers in CUDA\nCustom CUDA-based experience replay\nCUDA kernels for DQN forward/backward pass\n\nStay tuned — this is just the start!"
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html",
    "href": "posts/2025-10-18-jax_distributed/index.html",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "Training large language models like GPT-3 (175B parameters) requires distributing computation across dozens or hundreds of GPUs. JAX makes this remarkably elegant through its functional programming paradigm and sharding primitives. However, the mental model required differs significantly from PyTorch’s imperative style. This post demystifies JAX’s distributed training by addressing the key conceptual hurdles that arise when learning the framework.\n\n\nJAX excels at distributed training for three fundamental reasons:\n1. Functional paradigm: Parameters are data structures, not hidden object state. This makes sharding trivial—just split the data structure across devices.\n2. Explicit state management: No global random state or hidden device placement. Everything is passed explicitly.\n3. Automatic communication: Given sharding specifications, JAX’s compiler (XLA) figures out optimal communication patterns.\nFor comparison:\nPyTorch (DDP):\n# ~50+ lines of boilerplate\ndist.init_process_group(backend='nccl')\nrank = dist.get_rank()\nmodel = DDP(model, device_ids=[rank])\nsampler = DistributedSampler(dataset, rank=rank)\n# ... manual device management, rank checks, cleanup\nJAX:\n@jax.pmap\ndef train_step(params, batch):\n    return compute_grads(params, batch)\n\n\n\nJAX’s functional approach is the first conceptual hurdle. Unlike PyTorch where parameters live inside model objects, JAX separates computation from state.\nPyTorch:\nmodel = Model()  # Parameters hidden inside\nloss = model(x)  # Uses internal state\nloss.backward()  # Modifies internal .grad\nJAX:\nmodel = Model()  # Just defines computation\nparams = model.init(key, x)  # Parameters are separate data\nlogits = model.apply(params, x)  # Explicit parameter passing\ngrads = grad(loss_fn)(params, x)  # Explicit differentiation\nWhy does this matter? Because parameters being “just data” means you can trivially split them:\nsharded_params = jax.device_put(params, sharding_spec)\nNo DDP wrappers, no process groups, no manual device management.\n\n\n\nThe device mesh is JAX’s fundamental abstraction for organizing GPUs. Understanding this thoroughly is critical—most confusion in JAX distributed training stems from misunderstanding the mesh.\n\n\nA device mesh is a multi-dimensional array of devices with named axes:\n# Physical: 8x8 grid = 64 GPUs\ndevices = mesh_utils.create_device_mesh((8, 8))\n\n# Logical: Give axes semantic names\nmesh = Mesh(devices, axis_names=('data', 'model'))\nKey insight: axis names define how work is distributed, not the physical layout.\n        model axis (8 devices) →\ndata  ┌────┬────┬────┬────┬────┬────┬────┬────┐\naxis  │ 0  │ 1  │ 2  │ 3  │ 4  │ 5  │ 6  │ 7  │\n(8)   ├────┼────┼────┼────┼────┼────┼────┼────┤\n↓     │ 8  │ 9  │ 10 │ 11 │ 12 │ 13 │ 14 │ 15 │\n      ├────┼────┼────┼────┼────┼────┼────┼────┤\n      │ ... (64 GPUs total)                    │\n      └────┴────┴────┴────┴────┴────┴────┴────┘\nSemantics: - Same row: Process different batch slices with same model piece - Same column: Process same batch slice with different model pieces\nThis enables hybrid parallelism: 8-way data parallelism × 8-way model parallelism = 64-way total parallelism.\n\n\n\n\nPartitionSpec specifies tensor distribution across the mesh. The critical insight: PartitionSpec dimensions match tensor dimensions, not mesh dimensions.\nTensor shape:      (batch=64, seq=2048, embed=12288)\nPartitionSpec:     ('data',    None,    'model')\n                     ↑          ↑         ↑\n                     │          │         └─ Tensor dim 2: use 'model' axis\n                     │          └─────────── Tensor dim 1: replicate\n                     └────────────────────── Tensor dim 0: use 'data' axis\nThe mesh has 2 axes, but the tensor has 3 dimensions. PartitionSpec provides 3 entries, each referencing a mesh axis name or None.\n\n\ninput_batch = jnp.ones((64, 2048, 12288))\nspec = PartitionSpec('data', None, 'model')\n\n# What happens:\n# - Dim 0 (batch=64): Split 8 ways along 'data' axis → 8 per device\n# - Dim 1 (seq=2048): Replicate (all devices get full sequence)\n# - Dim 2 (embed=12288): Split 8 ways along 'model' axis → 1536 per device\n\n# Result per GPU: (8, 2048, 1536)\n\n\n\n\nUnderstanding memory layout is crucial for two reasons: correctness and performance. This is where reshape vs. transpose becomes important.\n\n\nIn multi-head attention, we perform:\nq = nn.Dense(num_heads * head_dim)(x)  # Shape: (batch, seq, 512)\n\nq = q.reshape(batch, seq, num_heads, head_dim)    # Step 1\nq = jnp.transpose(q, (0, 2, 1, 3))                # Step 2\nWhy not directly reshape to (batch, num_heads, seq, head_dim)?\nAnswer: Memory layout. After the Dense layer, the 512 dimensions are laid out in memory as:\n[head0_dim0, head0_dim1, ..., head0_dim63,    # First 64: head 0\n head1_dim0, head1_dim1, ..., head1_dim63,    # Next 64: head 1\n ...\n head7_dim0, head7_dim1, ..., head7_dim63]    # Last 64: head 7\nReshape changes how we interpret the data without moving it. Reshaping to (batch, seq, 8, 64) naturally groups the 512 dimensions into 8 groups of 64, which matches the memory layout.\nTranspose actually reorders data in memory. We need it to put num_heads before seq_len for efficient attention computation.\nAttempting to directly reshape to (batch, num_heads, seq, head_dim) would create a view where the data interpretation doesn’t match the underlying memory layout, resulting in incorrect groupings.\nKey principle: Reshape operations must respect the underlying memory layout. You can only reshape in ways that maintain the contiguity of data in memory.\n\n\n\n\nThe most common error is applying data parallelism to model weights:\n# WRONG\nweight = jnp.ones((12288, 49152))\nspec = PartitionSpec('data', None)  # Split first dim on data axis\nLet’s trace what happens in memory:\nWeight split along 'data' axis (8 ways):\n\nData position 0 (GPUs 0-7):   Rows 0-1535\nData position 1 (GPUs 8-15):  Rows 1536-3071\nData position 2 (GPUs 16-23): Rows 3072-4607\n...\n\nDuring training:\n- Batch slice 0 → Data position 0 → Uses weight rows 0-1535\n- Batch slice 1 → Data position 1 → Uses weight rows 1536-3071\n\nEach data replica has DIFFERENT weights = different models!\nTraining is broken.\nCorrect approach:\nspec = PartitionSpec(None, 'model')  # Replicate rows, split columns\n\n# All data replicas get all 12288 rows (same model)\n# Columns split 8 ways: each device gets 6144 columns\n\n\n\nThis reveals a critical insight: sharding happens at device_put time, not during computation.\n# Original batch in CPU/main memory\nbatch = jnp.ones((64, 2048, 12288))\n\n# Apply sharding - data is NOW physically distributed\ninput_spec = PartitionSpec('data', None, None)\nsharded_batch = jax.device_put(batch, NamedSharding(mesh, input_spec))\n\n# At this moment, batch is split across devices:\n# Data position 0 → batch[0:8] on GPUs 0-7\n# Data position 1 → batch[8:16] on GPUs 8-15\n# ...\nThe split happens before the forward pass. Each GPU already has its slice when computation begins. This is fundamentally different from PyTorch’s DistributedSampler which creates different batches per process.\n\n\n\nConsider this seemingly reasonable sharding:\ninput_spec = PartitionSpec('model', None, None)   # Batch on model axis\nweight_spec = PartitionSpec(None, 'model')        # Weights on model axis\nThis is mathematically correct but computationally wasteful:\nGPU 0:  Batch 0-7  × Weight cols 0-6143    → Result₀\nGPU 8:  Batch 0-7  × Weight cols 0-6143    → Result₀ (IDENTICAL!)\nGPU 16: Batch 0-7  × Weight cols 0-6143    → Result₀ (IDENTICAL!)\n...\nWhy? Both batch and weights are split along the model axis. GPUs in the same column (same model axis position) receive: - Same batch slice (model position 0 → batch 0-7) - Same weight slice (model position 0 → cols 0-6143) - Therefore: Identical computation\nAll GPUs in each column duplicate work. Only 12.5% of compute power is utilized (8 unique computations across 64 GPUs).\nSolution: Orthogonal splits:\ninput_spec = PartitionSpec('data', None, None)    # Different batches per row\nweight_spec = PartitionSpec(None, 'model')        # Different weights per column\nNow every GPU does unique work: 8 data replicas × 8 model pieces = true 64-way parallelism.\n\n\n\nPutting it together:\n# 1. Setup mesh\ndevices = mesh_utils.create_device_mesh((8, 8))\nmesh = Mesh(devices, axis_names=('data', 'model'))\n\n# 2. Initialize and shard parameters\nparams = model.init(key, dummy_input)\n\ndef shard_param(path, param):\n    name = '/'.join(path)\n    \n    # Large embeddings: split vocab\n    if 'embedding' in name and param.shape[0] &gt; 10000:\n        return jax.device_put(param, NamedSharding(mesh, PartitionSpec('model', None)))\n    \n    # Large matrices: split hidden dimension\n    if 'kernel' in name and param.shape[1] &gt; 1000:\n        return jax.device_put(param, NamedSharding(mesh, PartitionSpec(None, 'model')))\n    \n    # Small params: replicate\n    return jax.device_put(param, NamedSharding(mesh, PartitionSpec()))\n\nsharded_params = jax.tree_util.tree_map_with_path(shard_param, params)\n\n# 3. Training step\n@jax.jit\ndef train_step(params, batch, opt_state):\n    def loss_fn(params):\n        logits = model.apply(params, batch['input_ids'])\n        return cross_entropy(logits, batch['labels'])\n    \n    loss, grads = jax.value_and_grad(loss_fn)(params)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss\n\n# 4. Main loop\nfor batch in dataloader:\n    # Shard input\n    batch = jax.device_put(batch, NamedSharding(mesh, PartitionSpec('data', None)))\n    \n    # Train (all communication automatic)\n    sharded_params, opt_state, loss = train_step(sharded_params, batch, opt_state)\n\n\n\nWith 8-way model parallelism on 64 A100 GPUs:\nTotal parameters: 175B\nPer device: 175B / 8 = 22B params\n\nMemory per GPU (FP16):\n- Parameters:        22B × 2 bytes = 44 GB → 11 GB (with optimizations)\n- Gradients:         Same as parameters = 11 GB\n- Optimizer (Adam):  2× parameters = 22 GB\n- Activations:       ~20 GB\n─────────────────────────────────────────\nTotal: ~64 GB ✓ Fits on 80GB A100\nWithout sharding: 175B × 4 bytes = 700 GB for parameters alone. Impossible on single GPU.\n\n\n\n\nJAX’s functional paradigm makes parameters explicit data structures that can be trivially split across devices.\nDevice mesh with named axes provides semantic organization. The ‘data’ axis represents different data batches, the ‘model’ axis represents different model pieces.\nPartitionSpec dimensions match tensor dimensions, not mesh dimensions. Each entry references a named mesh axis or None.\nMemory layout matters: Reshape operations must respect contiguous memory layout. This is why attention requires both reshape and transpose.\nWeights use model parallelism (split along model axis), inputs use data parallelism (split along data axis). Mixing these causes either incorrect training (different models per replica) or redundant computation (wasted GPUs).\nSharding happens at device_put time, not during computation. Once sharded, JAX/XLA handles all communication automatically.\nEfficiency requires orthogonal splits: batch along data axis, model along model axis. This achieves true N×M parallelism on an N×M mesh.\n\nUnderstanding these principles, particularly the memory layout considerations and the distinction between physical device arrangement and logical axis semantics, demystifies JAX’s sharding and reveals why it’s particularly elegant for large-scale training.\n\n\n\n\nJAX Documentation\nJAX Sharding Guide\nFlax Documentation"
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#why-jax-for-distributed-training",
    "href": "posts/2025-10-18-jax_distributed/index.html#why-jax-for-distributed-training",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "JAX excels at distributed training for three fundamental reasons:\n1. Functional paradigm: Parameters are data structures, not hidden object state. This makes sharding trivial—just split the data structure across devices.\n2. Explicit state management: No global random state or hidden device placement. Everything is passed explicitly.\n3. Automatic communication: Given sharding specifications, JAX’s compiler (XLA) figures out optimal communication patterns.\nFor comparison:\nPyTorch (DDP):\n# ~50+ lines of boilerplate\ndist.init_process_group(backend='nccl')\nrank = dist.get_rank()\nmodel = DDP(model, device_ids=[rank])\nsampler = DistributedSampler(dataset, rank=rank)\n# ... manual device management, rank checks, cleanup\nJAX:\n@jax.pmap\ndef train_step(params, batch):\n    return compute_grads(params, batch)"
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#the-functional-foundation",
    "href": "posts/2025-10-18-jax_distributed/index.html#the-functional-foundation",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "JAX’s functional approach is the first conceptual hurdle. Unlike PyTorch where parameters live inside model objects, JAX separates computation from state.\nPyTorch:\nmodel = Model()  # Parameters hidden inside\nloss = model(x)  # Uses internal state\nloss.backward()  # Modifies internal .grad\nJAX:\nmodel = Model()  # Just defines computation\nparams = model.init(key, x)  # Parameters are separate data\nlogits = model.apply(params, x)  # Explicit parameter passing\ngrads = grad(loss_fn)(params, x)  # Explicit differentiation\nWhy does this matter? Because parameters being “just data” means you can trivially split them:\nsharded_params = jax.device_put(params, sharding_spec)\nNo DDP wrappers, no process groups, no manual device management."
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#understanding-device-mesh-the-core-abstraction",
    "href": "posts/2025-10-18-jax_distributed/index.html#understanding-device-mesh-the-core-abstraction",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "The device mesh is JAX’s fundamental abstraction for organizing GPUs. Understanding this thoroughly is critical—most confusion in JAX distributed training stems from misunderstanding the mesh.\n\n\nA device mesh is a multi-dimensional array of devices with named axes:\n# Physical: 8x8 grid = 64 GPUs\ndevices = mesh_utils.create_device_mesh((8, 8))\n\n# Logical: Give axes semantic names\nmesh = Mesh(devices, axis_names=('data', 'model'))\nKey insight: axis names define how work is distributed, not the physical layout.\n        model axis (8 devices) →\ndata  ┌────┬────┬────┬────┬────┬────┬────┬────┐\naxis  │ 0  │ 1  │ 2  │ 3  │ 4  │ 5  │ 6  │ 7  │\n(8)   ├────┼────┼────┼────┼────┼────┼────┼────┤\n↓     │ 8  │ 9  │ 10 │ 11 │ 12 │ 13 │ 14 │ 15 │\n      ├────┼────┼────┼────┼────┼────┼────┼────┤\n      │ ... (64 GPUs total)                    │\n      └────┴────┴────┴────┴────┴────┴────┴────┘\nSemantics: - Same row: Process different batch slices with same model piece - Same column: Process same batch slice with different model pieces\nThis enables hybrid parallelism: 8-way data parallelism × 8-way model parallelism = 64-way total parallelism."
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#partitionspec-mapping-tensors-to-mesh",
    "href": "posts/2025-10-18-jax_distributed/index.html#partitionspec-mapping-tensors-to-mesh",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "PartitionSpec specifies tensor distribution across the mesh. The critical insight: PartitionSpec dimensions match tensor dimensions, not mesh dimensions.\nTensor shape:      (batch=64, seq=2048, embed=12288)\nPartitionSpec:     ('data',    None,    'model')\n                     ↑          ↑         ↑\n                     │          │         └─ Tensor dim 2: use 'model' axis\n                     │          └─────────── Tensor dim 1: replicate\n                     └────────────────────── Tensor dim 0: use 'data' axis\nThe mesh has 2 axes, but the tensor has 3 dimensions. PartitionSpec provides 3 entries, each referencing a mesh axis name or None.\n\n\ninput_batch = jnp.ones((64, 2048, 12288))\nspec = PartitionSpec('data', None, 'model')\n\n# What happens:\n# - Dim 0 (batch=64): Split 8 ways along 'data' axis → 8 per device\n# - Dim 1 (seq=2048): Replicate (all devices get full sequence)\n# - Dim 2 (embed=12288): Split 8 ways along 'model' axis → 1536 per device\n\n# Result per GPU: (8, 2048, 1536)"
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#memory-layout-the-hidden-complexity",
    "href": "posts/2025-10-18-jax_distributed/index.html#memory-layout-the-hidden-complexity",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "Understanding memory layout is crucial for two reasons: correctness and performance. This is where reshape vs. transpose becomes important.\n\n\nIn multi-head attention, we perform:\nq = nn.Dense(num_heads * head_dim)(x)  # Shape: (batch, seq, 512)\n\nq = q.reshape(batch, seq, num_heads, head_dim)    # Step 1\nq = jnp.transpose(q, (0, 2, 1, 3))                # Step 2\nWhy not directly reshape to (batch, num_heads, seq, head_dim)?\nAnswer: Memory layout. After the Dense layer, the 512 dimensions are laid out in memory as:\n[head0_dim0, head0_dim1, ..., head0_dim63,    # First 64: head 0\n head1_dim0, head1_dim1, ..., head1_dim63,    # Next 64: head 1\n ...\n head7_dim0, head7_dim1, ..., head7_dim63]    # Last 64: head 7\nReshape changes how we interpret the data without moving it. Reshaping to (batch, seq, 8, 64) naturally groups the 512 dimensions into 8 groups of 64, which matches the memory layout.\nTranspose actually reorders data in memory. We need it to put num_heads before seq_len for efficient attention computation.\nAttempting to directly reshape to (batch, num_heads, seq, head_dim) would create a view where the data interpretation doesn’t match the underlying memory layout, resulting in incorrect groupings.\nKey principle: Reshape operations must respect the underlying memory layout. You can only reshape in ways that maintain the contiguity of data in memory."
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#critical-mistake-wrong-sharding-for-weights",
    "href": "posts/2025-10-18-jax_distributed/index.html#critical-mistake-wrong-sharding-for-weights",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "The most common error is applying data parallelism to model weights:\n# WRONG\nweight = jnp.ones((12288, 49152))\nspec = PartitionSpec('data', None)  # Split first dim on data axis\nLet’s trace what happens in memory:\nWeight split along 'data' axis (8 ways):\n\nData position 0 (GPUs 0-7):   Rows 0-1535\nData position 1 (GPUs 8-15):  Rows 1536-3071\nData position 2 (GPUs 16-23): Rows 3072-4607\n...\n\nDuring training:\n- Batch slice 0 → Data position 0 → Uses weight rows 0-1535\n- Batch slice 1 → Data position 1 → Uses weight rows 1536-3071\n\nEach data replica has DIFFERENT weights = different models!\nTraining is broken.\nCorrect approach:\nspec = PartitionSpec(None, 'model')  # Replicate rows, split columns\n\n# All data replicas get all 12288 rows (same model)\n# Columns split 8 ways: each device gets 6144 columns"
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#when-does-batch-splitting-actually-happen",
    "href": "posts/2025-10-18-jax_distributed/index.html#when-does-batch-splitting-actually-happen",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "This reveals a critical insight: sharding happens at device_put time, not during computation.\n# Original batch in CPU/main memory\nbatch = jnp.ones((64, 2048, 12288))\n\n# Apply sharding - data is NOW physically distributed\ninput_spec = PartitionSpec('data', None, None)\nsharded_batch = jax.device_put(batch, NamedSharding(mesh, input_spec))\n\n# At this moment, batch is split across devices:\n# Data position 0 → batch[0:8] on GPUs 0-7\n# Data position 1 → batch[8:16] on GPUs 8-15\n# ...\nThe split happens before the forward pass. Each GPU already has its slice when computation begins. This is fundamentally different from PyTorch’s DistributedSampler which creates different batches per process."
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#redundant-computation-a-subtle-pitfall",
    "href": "posts/2025-10-18-jax_distributed/index.html#redundant-computation-a-subtle-pitfall",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "Consider this seemingly reasonable sharding:\ninput_spec = PartitionSpec('model', None, None)   # Batch on model axis\nweight_spec = PartitionSpec(None, 'model')        # Weights on model axis\nThis is mathematically correct but computationally wasteful:\nGPU 0:  Batch 0-7  × Weight cols 0-6143    → Result₀\nGPU 8:  Batch 0-7  × Weight cols 0-6143    → Result₀ (IDENTICAL!)\nGPU 16: Batch 0-7  × Weight cols 0-6143    → Result₀ (IDENTICAL!)\n...\nWhy? Both batch and weights are split along the model axis. GPUs in the same column (same model axis position) receive: - Same batch slice (model position 0 → batch 0-7) - Same weight slice (model position 0 → cols 0-6143) - Therefore: Identical computation\nAll GPUs in each column duplicate work. Only 12.5% of compute power is utilized (8 unique computations across 64 GPUs).\nSolution: Orthogonal splits:\ninput_spec = PartitionSpec('data', None, None)    # Different batches per row\nweight_spec = PartitionSpec(None, 'model')        # Different weights per column\nNow every GPU does unique work: 8 data replicas × 8 model pieces = true 64-way parallelism."
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#complete-training-loop",
    "href": "posts/2025-10-18-jax_distributed/index.html#complete-training-loop",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "Putting it together:\n# 1. Setup mesh\ndevices = mesh_utils.create_device_mesh((8, 8))\nmesh = Mesh(devices, axis_names=('data', 'model'))\n\n# 2. Initialize and shard parameters\nparams = model.init(key, dummy_input)\n\ndef shard_param(path, param):\n    name = '/'.join(path)\n    \n    # Large embeddings: split vocab\n    if 'embedding' in name and param.shape[0] &gt; 10000:\n        return jax.device_put(param, NamedSharding(mesh, PartitionSpec('model', None)))\n    \n    # Large matrices: split hidden dimension\n    if 'kernel' in name and param.shape[1] &gt; 1000:\n        return jax.device_put(param, NamedSharding(mesh, PartitionSpec(None, 'model')))\n    \n    # Small params: replicate\n    return jax.device_put(param, NamedSharding(mesh, PartitionSpec()))\n\nsharded_params = jax.tree_util.tree_map_with_path(shard_param, params)\n\n# 3. Training step\n@jax.jit\ndef train_step(params, batch, opt_state):\n    def loss_fn(params):\n        logits = model.apply(params, batch['input_ids'])\n        return cross_entropy(logits, batch['labels'])\n    \n    loss, grads = jax.value_and_grad(loss_fn)(params)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss\n\n# 4. Main loop\nfor batch in dataloader:\n    # Shard input\n    batch = jax.device_put(batch, NamedSharding(mesh, PartitionSpec('data', None)))\n    \n    # Train (all communication automatic)\n    sharded_params, opt_state, loss = train_step(sharded_params, batch, opt_state)"
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#memory-calculation-for-gpt-3",
    "href": "posts/2025-10-18-jax_distributed/index.html#memory-calculation-for-gpt-3",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "With 8-way model parallelism on 64 A100 GPUs:\nTotal parameters: 175B\nPer device: 175B / 8 = 22B params\n\nMemory per GPU (FP16):\n- Parameters:        22B × 2 bytes = 44 GB → 11 GB (with optimizations)\n- Gradients:         Same as parameters = 11 GB\n- Optimizer (Adam):  2× parameters = 22 GB\n- Activations:       ~20 GB\n─────────────────────────────────────────\nTotal: ~64 GB ✓ Fits on 80GB A100\nWithout sharding: 175B × 4 bytes = 700 GB for parameters alone. Impossible on single GPU."
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#key-principles",
    "href": "posts/2025-10-18-jax_distributed/index.html#key-principles",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "JAX’s functional paradigm makes parameters explicit data structures that can be trivially split across devices.\nDevice mesh with named axes provides semantic organization. The ‘data’ axis represents different data batches, the ‘model’ axis represents different model pieces.\nPartitionSpec dimensions match tensor dimensions, not mesh dimensions. Each entry references a named mesh axis or None.\nMemory layout matters: Reshape operations must respect contiguous memory layout. This is why attention requires both reshape and transpose.\nWeights use model parallelism (split along model axis), inputs use data parallelism (split along data axis). Mixing these causes either incorrect training (different models per replica) or redundant computation (wasted GPUs).\nSharding happens at device_put time, not during computation. Once sharded, JAX/XLA handles all communication automatically.\nEfficiency requires orthogonal splits: batch along data axis, model along model axis. This achieves true N×M parallelism on an N×M mesh.\n\nUnderstanding these principles, particularly the memory layout considerations and the distinction between physical device arrangement and logical axis semantics, demystifies JAX’s sharding and reveals why it’s particularly elegant for large-scale training."
  },
  {
    "objectID": "posts/2025-10-18-jax_distributed/index.html#resources",
    "href": "posts/2025-10-18-jax_distributed/index.html#resources",
    "title": "Distributed Training with JAX Simplified",
    "section": "",
    "text": "JAX Documentation\nJAX Sharding Guide\nFlax Documentation"
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "",
    "text": "When Qwen released their GSPO paper questioning GRPO’s theoretical foundations, they ignited a debate about theoretical correctness versus empirical success. GSPO offers stronger theoretical guarantees. GRPO makes approximations in its importance sampling approach that can be problematic. Yet GRPO powers state-of-the-art models like DeepSeek-R1, which achieved breakthrough performance on mathematical reasoning tasks. This post examines why both perspectives have merit and what this reveals about the theory-practice gap in deep learning."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#the-theoretical-problem",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#the-theoretical-problem",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "The Theoretical Problem",
    "text": "The Theoretical Problem\nGRPO’s objective computes token-level importance weights using single samples from each token distribution:\n\\[J_{\\text{GRPO}}(\\theta) = \\mathbb{E}\\left[\\frac{1}{G} \\sum_{i=1}^{G} \\frac{1}{\\lvert y_i \\rvert} \\sum_{t=1}^{\\lvert y_i \\rvert} \\min\\left(w_{i,t}(\\theta)\\hat{A}_i, \\text{clip}(w_{i,t}(\\theta), 1-\\varepsilon, 1+\\varepsilon)\\hat{A}_i\\right)\\right]\\]\nwhere the importance ratio at each token is:\n\\[w_{i,t}(\\theta) = \\frac{\\pi_\\theta(y_{i,t} \\mid x, y_{i,&lt;t})}{\\pi_{\\theta_{\\text{old}}}(y_{i,t} \\mid x, y_{i,&lt;t})}\\]\nThe importance sampling principle requires multiple samples to make reweighting valid. For a random variable \\(z\\), proper importance sampling states:\n\\[\\mathbb{E}_{\\pi_{\\text{target}}}[f(z)] = \\mathbb{E}_{\\pi_{\\text{behavior}}}\\left[\\frac{\\pi_{\\text{target}}(z)}{\\pi_{\\text{behavior}}(z)} \\cdot f(z)\\right]\\]\nThis equality holds asymptotically as the number of samples from \\(\\pi_{\\text{behavior}}\\) approaches infinity. GRPO computes \\(w_{i,t}\\) using a single sample \\(y_{i,t}\\) from \\(\\pi_{\\theta_{\\text{old}}}(\\cdot \\mid x, y_{i,&lt;t})\\). With only one sample per token position, the importance weight cannot perform valid distribution correction. Instead, it introduces high-variance noise that accumulates multiplicatively across the sequence length.\nIt’s important to note that using single samples with importance weights is not inherently invalid. Many policy gradient methods do this. The question is whether GRPO’s specific token-level aggregation of these weights introduces problematic variance, particularly as sequence length increases. The concern is less about “violating” importance sampling and more about whether this approximation remains reasonable under various conditions.\nConsider a 1000-token response. GRPO computes 1000 independent importance weights, each based on a single sample. If we denote the estimation error at token \\(t\\) as \\(\\epsilon_t\\), the accumulated effect scales as \\(\\exp(\\sum_t \\epsilon_t)\\) or \\(\\prod_t (1 + \\epsilon_t)\\). Small per-token errors compound into large sequence-level errors. Qwen’s experiments show this accumulation leads to “catastrophic and irreversible model collapse” particularly in Mixture-of-Experts models and long-context scenarios.\nGSPO corrects this by computing importance ratios at the sequence level:\n\\[J_{\\text{GSPO}}(\\theta) = \\mathbb{E}\\left[\\frac{1}{G} \\sum_{i=1}^{G} \\min\\left(s_i(\\theta)\\hat{A}_i, \\text{clip}(s_i(\\theta), 1-\\varepsilon, 1+\\varepsilon)\\hat{A}_i\\right)\\right]\\]\nwhere:\n\\[s_i(\\theta) = \\left(\\frac{\\pi_\\theta(y_i \\mid x)}{\\pi_{\\theta_{\\text{old}}}(y_i \\mid x)}\\right)^{\\frac{1}{\\lvert y_i \\rvert}}\\]\nThe exponent \\(\\frac{1}{\\lvert y_i \\rvert}\\) applies length normalization via geometric mean, preventing longer sequences from dominating the importance ratio. All tokens in sequence \\(y_i\\) share the same weight \\(s_i(\\theta)\\), eliminating token-level fluctuations. This aligns the optimization unit with the reward unit, since rewards are assigned to complete sequences rather than individual tokens."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#gradient-analysis",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#gradient-analysis",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "Gradient Analysis",
    "text": "Gradient Analysis\nThe gradient expressions reveal the fundamental difference. For GRPO:\n\\[\\nabla_\\theta J_{\\text{GRPO}} = \\mathbb{E}\\left[\\frac{1}{G} \\sum_{i=1}^{G} \\hat{A}_i \\cdot \\frac{1}{\\lvert y_i \\rvert} \\sum_{t=1}^{\\lvert y_i \\rvert} \\frac{\\pi_\\theta(y_{i,t} \\mid x, y_{i,&lt;t})}{\\pi_{\\theta_{\\text{old}}}(y_{i,t} \\mid x, y_{i,&lt;t})} \\cdot \\nabla_\\theta \\log \\pi_\\theta(y_{i,t} \\mid x, y_{i,&lt;t})\\right]\\]\nFor GSPO:\n\\[\\nabla_\\theta J_{\\text{GSPO}} = \\mathbb{E}\\left[\\frac{1}{G} \\sum_{i=1}^{G} \\left(\\frac{\\pi_\\theta(y_i \\mid x)}{\\pi_{\\theta_{\\text{old}}}(y_i \\mid x)}\\right)^{\\frac{1}{\\lvert y_i \\rvert}} \\cdot \\hat{A}_i \\cdot \\frac{1}{\\lvert y_i \\rvert} \\sum_{t=1}^{\\lvert y_i \\rvert} \\nabla_\\theta \\log \\pi_\\theta(y_{i,t} \\mid x, y_{i,&lt;t})\\right]\\]\nIn GRPO, each token \\(t\\) receives its own importance weight. These weights can vary arbitrarily: token 1 might get weight 0.5, token 2 weight 2.8, token 500 weight 0.09. The unequal weighting creates gradient variance that accumulates across the sequence. In GSPO, all tokens in sequence \\(i\\) share the same scalar weight, producing uniform treatment and stable gradients.\nThe following diagram illustrates the difference:\n\n\n\nToken-Level vs Sequence-Level Weighting\n\n\n\nGRPO Token-Level Weighting: Token: [ 1 ] [ 2 ] [ 3 ] … [ 999 ] [1000] Weight: [0.8 ] [2.1 ] [0.3 ] … [1.7 ] [0.1 ] → High variance, noisy gradients\nGSPO Sequence-Level Weighting: Token: [ 1 ] [ 2 ] [ 3 ] … [ 999 ] [1000] Weight: [1.05] [1.05] [1.05] … [1.05] [1.05] → Uniform, stable gradients"
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#empirical-evidence",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#empirical-evidence",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "Empirical Evidence",
    "text": "Empirical Evidence\nNote on experimental setup: The following results are reported from the RSPO paper. A fair comparison requires equivalent hyperparameter tuning effort for all methods. While these results suggest significant issues with GRPO on MoE architectures, the approximations (“~”) in GRPO’s scores and the specific experimental conditions warrant careful interpretation.\nThe evidence from training on Mixture-of-Experts architectures is striking. The RSPO paper evaluated Qwen3-30B-A3B across five mathematical reasoning benchmarks:\n\n\n\nBenchmark\nBase Model\nGRPO\nGSPO\nGMPO\nRSPO\n\n\n\n\nAIME24\n43.3\n~20\n74.1\n73.3\n80.0\n\n\nAMC23\n69.9\n~45\n77.1\n75.9\n79.5\n\n\nMATH500\n82.8\n~70\n88.2\n88.6\n88.4\n\n\nMinerva\n48.5\n~35\n58.1\n57.0\n61.8\n\n\nOlympiadBench\n44.7\n~40\n54.2\n54.8\n52.6\n\n\nAverage\n57.8\n35.0\n70.3\n69.9\n77.1\n\n\n\nGRPO not only underperforms GSPO but actually degrades below the base model. Training curves show pronounced collapse around 200 to 500 steps. The cause is expert activation volatility: after each gradient update in a 48-layer MoE model, approximately 10% of activated experts change for the same input. Token-level importance ratios \\(w_{i,t}\\) fluctuate drastically as different experts are selected, preventing convergence.\nThis expert volatility explanation is plausible and consistent with the observed failures, though definitively proving causation would require ablation studies isolating this factor from other potential causes like learning rates, batch sizes, or other architectural interactions.\nGSPO avoids this failure mode because sequence-level likelihoods remain stable even when individual token expert assignments shift. The sequence likelihood \\(\\pi_\\theta(y_i \\mid x)\\) aggregates over all token-level expert decisions, smoothing out routing variability. This eliminates the need for “Routing Replay,” a complex workaround that caches expert routes from the old policy and replays them during importance ratio computation.\nOn AIME 2024 using Qwen2.5-32B base, the performance gap is equally stark:\n\n\n\nMethod\nScore\nTraining Steps\n\n\n\n\nVanilla GRPO\n30\nBaseline\n\n\nGSPO\n70-80\nSame\n\n\nGRPO + engineering (DAPO)\n50\n50% of DeepSeek\n\n\nGRPO + engineering (SRPO)\n50\n10% of DeepSeek\n\n\n\nVanilla GRPO achieves only 30 points, while GSPO reaches 70 to 80 points with equivalent compute. The DAPO and SRPO variants improve GRPO by adding extensive engineering: asymmetric clipping, dynamic sampling, token-level loss modifications, and two-stage training. These modifications compensate for GRPO’s theoretical deficiencies but require significant implementation complexity.\nA counter-intuitive finding emerges from analyzing clipping statistics. GRPO clips approximately 0.13% of tokens during training, while GSPO clips 15% of tokens (two orders of magnitude more). Yet GSPO achieves superior performance. This demonstrates that GRPO’s token-level gradients are inherently noisy. Even unclipped gradients hurt rather than help. GSPO’s aggressive sequence-level clipping effectively filters out high-variance samples."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#the-gspo-length-bias-issue",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#the-gspo-length-bias-issue",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "The GSPO Length Bias Issue",
    "text": "The GSPO Length Bias Issue\nWhile GSPO addresses GRPO’s variance problems, it introduces its own limitation: length bias. The geometric mean normalization in GSPO’s importance ratio:\n\\[s_i(\\theta) = \\left(\\frac{\\pi_\\theta(y_i \\mid x)}{\\pi_{\\theta_{\\text{old}}}(y_i \\mid x)}\\right)^{\\frac{1}{\\lvert y_i \\rvert}}\\]\ncan create systematic biases in how the model treats responses of different lengths. Specifically:\n\nShort sequences receive disproportionately large importance weights when they deviate from the old policy, potentially leading to over-optimization on brief responses\nLong sequences have their importance ratios dampened even when they represent significant policy changes, potentially under-weighting important long-form improvements\nLength-dependent convergence: The effective learning rate becomes implicitly coupled to sequence length, which may not align with the true importance of different responses\n\nThis length bias can manifest in practice as models that either truncate responses prematurely (to exploit the short-sequence advantage) or fail to learn from long chains of reasoning (due to dampened signals). The issue is particularly problematic for tasks requiring variable-length reasoning where the optimal response length is itself a learned quantity."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#dynamic-reward-grpo-the-current-state-of-the-art",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#dynamic-reward-grpo-the-current-state-of-the-art",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "Dynamic Reward GRPO: The Current State-of-the-Art",
    "text": "Dynamic Reward GRPO: The Current State-of-the-Art\nRecent work has shown that Dynamic Reward GRPO (DR GRPO) addresses both GRPO’s variance issues and GSPO’s length bias, emerging as the current best-performing method in practice. DR GRPO introduces several key innovations:\n\nKey Improvements in DR GRPO\n\nDynamic advantage normalization: Instead of using fixed rewards, DR GRPO adaptively normalizes advantages based on sequence statistics, reducing the impact of length-dependent variance\nToken-level variance reduction: Implements sophisticated variance reduction techniques that maintain token-level granularity while controlling noise accumulation\nHybrid importance weighting: Combines elements of token-level and sequence-level importance sampling, dynamically adjusting based on sequence characteristics\nLength-agnostic optimization: Explicitly corrects for length bias through adaptive clipping ranges and normalization schemes\n\n\n\nEmpirical Performance\nCurrent benchmarks suggest DR GRPO achieves: - Stability comparable to GSPO on MoE architectures - Performance exceeding both vanilla GRPO and GSPO on mathematical reasoning tasks - No length bias in learned policies - Better sample efficiency than GSPO in many scenarios\nThe success of DR GRPO demonstrates that the token-level vs sequence-level debate may have been asking the wrong question. Rather than choosing between these extremes, the optimal approach appears to be a carefully engineered middle ground that preserves token-level signal while controlling variance through dynamic mechanisms."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#why-grpo-works-despite-being-wrong",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#why-grpo-works-despite-being-wrong",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "Why GRPO Works Despite Being Wrong",
    "text": "Why GRPO Works Despite Being Wrong\nGiven the theoretical concerns and empirical evidence of failure in specific contexts, why does GRPO succeed in others? Several mechanisms explain its continued effectiveness.\n\nSmall Divergence Regime\nWhen clipping keeps \\(\\pi_\\theta\\) close to \\(\\pi_{\\theta_{\\text{old}}}\\), the token-level approximation may be adequate. If policies are similar, we can write the token-level importance ratio as approximately \\(1 + \\epsilon_t\\) where \\(\\epsilon_t\\) is small. The product over all tokens becomes \\(\\prod_t (1 + \\epsilon_t) \\approx \\exp(\\sum_t \\epsilon_t)\\). If the per-token errors \\(\\epsilon_t\\) are roughly independent and average to a reasonable value, accumulated error may not be catastrophic.\nThis approximation breaks down in two scenarios. First, long sequences (1000+ tokens) accumulate many small errors into large total error. Second, MoE models violate the small divergence assumption because expert routing changes create large per-token probability shifts even when the overall policy changes moderately. The volatility of individual \\(w_{i,t}\\) values exceeds what clipping can control.\n\n\nEmpirical Risk Minimization\nThe theoretical objective may not be what matters for practical optimization. What matters is whether updates improve measured performance. GRPO’s updates are high variance and theoretically unjustified, yet they may still point in a productive direction on average. Deep learning is replete with methods whose theoretical justification was incorrect but which nonetheless work: the original explanation for batch normalization’s effectiveness was wrong, yet batch normalization remains standard practice.\nThe question becomes whether GRPO provides a sufficiently strong learning signal despite its flaws. For dense models on shorter sequences, the answer appears to be yes, conditional on careful hyperparameter tuning. For MoE models on longer sequences, the answer is definitively no.\n\n\nEngineering as Theory Compensation\nDAPO adds four modifications to vanilla GRPO: asymmetric clipping (Clip-Higher), dynamic sampling, token-level policy gradient loss, and overlong reward shaping. These are not mere optimizations but compensations for theoretical deficiencies. Clip-Higher allows rare but important tokens to be explored by decoupling upper and lower clipping bounds. Dynamic sampling filters out samples that produce zero gradients, improving sample efficiency. Token-level loss reweights contributions to prevent length bias. Overlong reward shaping penalizes excessive length in a smooth manner.\nEach modification addresses a specific pathology caused by token-level importance weighting. The fact that extensive engineering can rescue GRPO demonstrates two points. First, the theoretical problems are real and manifest as practical issues. Second, the problems are not insurmountable for dense models with sufficient effort. However, the engineering complexity represents hidden cost that GSPO avoids.\n\n\nTask Structure and Forgiveness\nSome tasks may be more tolerant of algorithmic approximation errors. Dense models with shorter sequences provide fewer opportunities for token-level noise to accumulate. The task structure matters: if critical information is concentrated in a few key tokens rather than distributed evenly, token-level importance reweighting might accidentally emphasize those key tokens despite lacking theoretical justification.\nConversely, tasks requiring precise long-range reasoning over 1000+ token chains of thought expose GRPO’s flaws maximally. The empirical pattern aligns with this hypothesis: GRPO struggles most on MoE models with long sequences, performs acceptably on dense models with shorter sequences, and falls between these extremes on intermediate scenarios.\n\n\nThe DeepSeek-R1 Puzzle\nGRPO’s success in DeepSeek-R1 deserves careful examination rather than dismissal. DeepSeek-R1 achieved remarkable performance on mathematical reasoning benchmarks using GRPO, raising important questions: What conditions allowed GRPO to succeed there? Was it the dense (non-MoE) architecture? Shorter effective sequence lengths during critical training phases? Exceptional hyperparameter tuning? Or does the task structure of mathematical reasoning provide some robustness to GRPO’s approximation errors?\nThe absence of public details about DeepSeek-R1’s training process makes definitive conclusions difficult. However, the empirical success suggests that for certain combinations of architecture, task, and sequence length, GRPO’s approximations remain within acceptable bounds. This doesn’t invalidate concerns about GRPO’s theoretical foundations, but it does highlight that the practical impact depends heavily on deployment context.\nWhy didn’t DeepSeek use GSPO or DR GRPO? Possible explanations include: (1) GRPO was more mature when DeepSeek-R1 was developed, (2) their specific infrastructure was optimized for GRPO, (3) newer methods like DR GRPO may have implementation subtleties not captured in papers, or (4) their dense architecture and tuning made GRPO sufficient. The choice between algorithms involves engineering tradeoffs beyond pure theoretical optimality."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#the-stability-analysis",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#the-stability-analysis",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "The Stability Analysis",
    "text": "The Stability Analysis\nTraining stability metrics reveal GSPO’s robustness advantage:\n\n\n\nThe stability difference is qualitative, not quantitative. GRPO training exhibits high variance reward curves with frequent drops. Some drops recover, but others lead to irreversible collapse where even reverting to earlier checkpoints fails to restore training. GSPO training shows monotonic improvement with smooth reward curves. The absence of catastrophic failures enables longer training runs and more aggressive scaling of compute.\nKey metrics comparison:\n\n\n\n\n\n\n\n\n\nMetric\nGRPO\nGSPO\nDR GRPO\n\n\n\n\nClipping Rate\n0.13%\n15%\n~5-10%\n\n\nExpert Routing Volatility\n~10% change per update\nImmune\nReduced\n\n\nFailure Mode\nCatastrophic collapse\nLength bias\nRare\n\n\nRecovery\nOften irreversible\nN/A\nGood"
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#production-deployment",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#production-deployment",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "Production Deployment",
    "text": "Production Deployment\nQwen3 models trained with GSPO demonstrate the algorithm’s scalability to production systems. The flagship Qwen3-235B-A22B achieves 85.7 on AIME’24 and 81.5 on AIME’25, substantially exceeding models trained with GRPO variants. On LiveCodeBench v5, it scores 70.7. On CodeForces, it achieves 2056 Elo rating. These results come from extended training runs that would be infeasible with GRPO’s instability.\nInfrastructure requirements differ significantly. GRPO requires Routing Replay for MoE models, adding memory overhead and communication cost. Routing Replay caches the expert routes from the old policy and replays them when computing importance ratios under the new policy. This ensures consistent expert activation but restricts the model’s capacity and complicates the training pipeline. GSPO eliminates this requirement entirely, simplifying infrastructure and allowing full utilization of model capacity.\nPrecision tolerance also favors GSPO. Training engines and inference engines often have subtle numerical differences due to optimization choices. GRPO needs exact token-level likelihoods, requiring recomputation in the training engine even when likelihoods were already computed during inference. GSPO’s sequence-level optimization is robust to small numerical differences, potentially allowing direct use of inference engine likelihoods without recomputation. This matters for efficiency in partial rollout and multi-turn RL scenarios.\nDR GRPO appears to offer similar infrastructure simplifications while maintaining better performance characteristics, though widespread production deployments are still emerging."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#when-each-method-wins",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#when-each-method-wins",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "When Each Method Wins",
    "text": "When Each Method Wins\nThe empirical evidence suggests updated guidelines:\nDR GRPO is the recommended default for new implementations given its strong empirical performance, lack of length bias, and reasonable stability. It represents the current state-of-the-art for most scenarios.\nGSPO remains a strong choice when maximum training stability is critical, particularly for MoE models where GRPO’s failure is catastrophic. GSPO is also preferable when implementation simplicity matters more than peak performance, or when length bias is manageable for the specific task.\nGRPO can still be viable for dense models with shorter sequences where its flaws are less exposed, or in legacy systems where migration cost exceeds the performance benefit. However, new implementations should strongly prefer DR GRPO or GSPO unless there are compelling infrastructure constraints."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#the-theoretical-lesson",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#the-theoretical-lesson",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "The Theoretical Lesson",
    "text": "The Theoretical Lesson\nThis case study illuminates the relationship between theory and practice in deep learning optimization. Theoretical correctness provides robustness guarantees but does not preclude success of theoretically flawed methods in restricted domains. GRPO violates importance sampling principles yet achieves competitive results on specific tasks with sufficient engineering. The violation matters in extreme regimes (MoE, long sequences) where theoretical predictions become empirically manifest.\nThe emergence of DR GRPO suggests that the most successful approaches may synthesize insights from both extremes rather than adhering dogmatically to either token-level or sequence-level formulations. The pattern resembles other instances where theory and practice iterate: initial methods have theoretical issues, theoretically-motivated alternatives address some issues but introduce new limitations, and eventually sophisticated engineering produces methods that work well in practice while being more theoretically grounded."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#practical-recommendations",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#practical-recommendations",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "Practical Recommendations",
    "text": "Practical Recommendations\nFor new implementations, DR GRPO is the recommended starting point given current empirical evidence. If DR GRPO is not available or well-supported in your framework, GSPO is the next best choice. The implementation for GSPO is straightforward:\nfrom trl import GRPOConfig\n\nconfig = GRPOConfig(\n    importance_sampling_level=\"sequence\",\n    loss_type=\"grpo\",\n    beta=0.0,\n    epsilon=3e-4,\n    epsilon_high=4e-4,\n    gradient_accumulation_steps=1,\n    steps_per_generation=4,\n)\nThe key parameter is importance_sampling_level=\"sequence\" which enables GSPO’s sequence-level importance weighting. The clipping ranges (epsilon=3e-4, epsilon_high=4e-4) are two orders of magnitude smaller than typical GRPO ranges because sequence-level ratios have different numerical scales than token-level ratios. Setting beta=0.0 removes KL regularization, which GSPO authors found unnecessary for long chain-of-thought reasoning.\nFor existing GRPO implementations, migration should be prioritized based on: - Critical: MoE architectures (GRPO fails catastrophically) - High priority: Long sequences (&gt;500 tokens), tasks where length bias matters - Medium priority: Production systems requiring high stability - Lower priority: Dense models with short sequences showing acceptable performance\nFor MoE models specifically, GSPO or DR GRPO are non-negotiable. The empirical evidence shows that vanilla GRPO fails catastrophically on MoE, and even heavily engineered GRPO variants require complex infrastructure like Routing Replay."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#limitations-and-caveats",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#limitations-and-caveats",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "Limitations and Caveats",
    "text": "Limitations and Caveats\nThis analysis has several limitations worth noting:\nPotential GSPO Tradeoffs: The length bias issue in GSPO is a real concern that may impact certain applications. Tasks requiring variable-length reasoning or where optimal response length must be learned may suffer from GSPO’s geometric mean normalization.\nDR GRPO Maturity: While DR GRPO shows promise, it’s a newer method with less production validation than GRPO or GSPO. Implementation details may vary, and hyperparameter sensitivity is not yet fully characterized.\nExperimental Reproducibility: The empirical comparisons rely on results from published papers without independent replication. Training stability claims would benefit from open-source implementations and shared training curves.\nEvolving Landscape: All three methods continue to evolve. Enhanced variants and potential refinements may shift the practical tradeoffs. The “optimal” choice may depend on rapidly changing infrastructure and tooling ecosystems.\nPublication Bias: Papers naturally emphasize scenarios where their proposed method excels. The broader landscape of deployments (many in proprietary settings) may include success cases not reflected in academic publications."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#conclusion",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#conclusion",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "Conclusion",
    "text": "Conclusion\nThe landscape of policy gradient methods for LLM alignment has evolved significantly:\n\nGRPO offered simplicity but suffers from theoretical issues that manifest as catastrophic failures in MoE architectures and high variance in long sequences\nGSPO corrected GRPO’s variance problems with sound theoretical foundations but introduced length bias\nDR GRPO appears to address both sets of issues, representing the current state-of-the-art\n\nFor practitioners, the choice hierarchy is clear: 1. First choice: DR GRPO if available and well-supported 2. Strong alternative: GSPO for stability-critical applications or when DR GRPO is unavailable 3. Legacy only: GRPO for existing systems where migration cost outweighs benefits\nThe broader lesson is that theory and practice exist in productive tension. Theory predicts failure modes that may not be immediately visible. Practice reveals which theoretical concerns matter most and which can be addressed through engineering. DR GRPO exemplifies an algorithm where theory and practice iterate to produce methods that are both theoretically motivated and empirically superior."
  },
  {
    "objectID": "posts/2025-11-23-GSPO_GRPO/index.html#references",
    "href": "posts/2025-11-23-GSPO_GRPO/index.html#references",
    "title": "GSPO vs GRPO - Theory, Practice, and the Limits of Approximation",
    "section": "References",
    "text": "References\n\nGSPO Paper - Group Sequence Policy Optimization\nGRPO Paper - DeepSeekMath (introduced GRPO)\nDAPO Paper - Improvements to GRPO\nSRPO Paper - Two-stage GRPO variant\nRSPO Paper - Router-shift aware optimization"
  },
  {
    "objectID": "posts/2024-12-24-spotspraying/index.html",
    "href": "posts/2024-12-24-spotspraying/index.html",
    "title": "Precision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System",
    "section": "",
    "text": "Conventional spraying methods apply herbicides uniformly across fields, resulting in excessive chemical use, environmental risks, and increased operational costs. This project implements a precision weeding system for sugarbeet fields by combining semantic segmentation, depth sensing, and advanced model optimization techniques. The system targets only weed-infested areas, reducing herbicide usage and minimizing environmental impact."
  },
  {
    "objectID": "posts/2024-12-24-spotspraying/index.html#introduction",
    "href": "posts/2024-12-24-spotspraying/index.html#introduction",
    "title": "Precision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System",
    "section": "",
    "text": "Conventional spraying methods apply herbicides uniformly across fields, resulting in excessive chemical use, environmental risks, and increased operational costs. This project implements a precision weeding system for sugarbeet fields by combining semantic segmentation, depth sensing, and advanced model optimization techniques. The system targets only weed-infested areas, reducing herbicide usage and minimizing environmental impact."
  },
  {
    "objectID": "posts/2024-12-24-spotspraying/index.html#system-overview",
    "href": "posts/2024-12-24-spotspraying/index.html#system-overview",
    "title": "Precision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System",
    "section": "System Overview",
    "text": "System Overview\nThe system pipeline is summarized in the following flowchart:\nRGB + Depth Images\n       │\n       ▼\nSemantic Segmentation \n(DeepLabV3+ with Channel Attention)\n       │\n       ▼\nDepth Projection & 3D Processing\n       │\n       ▼\nField Actuation (Coordinate Transformation & Control)\nThe process begins with the acquisition of synchronized RGB and depth images. It then uses semantic segmentation to distinguish sugarbeets from weeds, followed by processing to obtain 3D information. Finally, the system translates these 3D coordinates to guide precise actuation in the field."
  },
  {
    "objectID": "posts/2024-12-24-spotspraying/index.html#semantic-segmentation-with-channel-attention",
    "href": "posts/2024-12-24-spotspraying/index.html#semantic-segmentation-with-channel-attention",
    "title": "Precision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System",
    "section": "Semantic Segmentation with Channel Attention",
    "text": "Semantic Segmentation with Channel Attention\n\nOverview\nThe segmentation network is built on a modified DeepLabV3+ architecture that classifies each pixel as sugarbeet, weed, or background. A channel attention module enhances the network’s ability to differentiate between crops and weeds, even under challenging conditions such as variable illumination, occlusions, or subtle texture differences.\n\n\nBenefits of Channel Attention\n\nAdaptive Feature Recalibration:\nGlobal average pooling generates a channel descriptor that is then processed to produce channel-specific weights. This allows the network to emphasize important features while suppressing less informative ones.\nEnhanced Discrimination:\nRecalibrated feature maps improve the network’s ability to distinguish between visually similar classes, leading to a\n\\[\\textbf{19\\% increase in Intersection over Union (IoU)}\\].\nRobustness to Variability:\nDynamic adjustment of feature importance helps maintain consistent performance despite changes in weather, soil, or crop growth stages.\n\n\n\nMathematical Formulation\nThe channel attention module computes a channel descriptor via global average pooling:\n\\[\nz_c = \\frac{1}{H \\times W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} x_c(i,j),\n\\]\nwhich is transformed into channel-specific weights using:\n\\[\ns = \\sigma(W_2 \\cdot \\text{ReLU}(W_1 \\cdot z)),\n\\]\nand applied to rescale the feature maps:\n\\[\n\\tilde{x}_c = s_c \\cdot x_c.\n\\]\n\n\nPyTorch Implementation\nimport torch\nimport torch.nn as nn\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(in_channels, in_channels // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(in_channels // reduction, in_channels, bias=False),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y"
  },
  {
    "objectID": "posts/2024-12-24-spotspraying/index.html#real-time-deployment-and-model-optimization",
    "href": "posts/2024-12-24-spotspraying/index.html#real-time-deployment-and-model-optimization",
    "title": "Precision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System",
    "section": "Real-Time Deployment and Model Optimization",
    "text": "Real-Time Deployment and Model Optimization\n\nEmbedded System Architecture\nDue to remote field conditions and limited internet connectivity, the entire inference pipeline is implemented in C++ and deployed on an NXP i.MX8 board running Yocto Linux with a Hailo-8 accelerator. libtorch is used to integrate the PyTorch models into the C++ environment, ensuring seamless execution in the field.\n\n\nModel Optimization Techniques\n\nINT8 Quantization:\nThe model is optimized for real-time performance using INT8 precision. This involves:\n\nDynamic Quantization:\nimport torch.quantization as quant\n\nmodel_fp32 = CustomSegNet(num_classes=3)\nmodel_int8 = quant.quantize_dynamic(model_fp32, {nn.Conv2d, nn.Linear}, dtype=torch.qint8)\nQuantization Aware Training (QAT):\nDuring QAT, fake quantization layers simulate INT8 precision. The activation quantization function is defined as:\n\\[\n\\hat{x} = Q(x) = s \\cdot \\text{clip}\\left(\\left\\lfloor \\frac{x}{s} \\right\\rceil, -q_{\\min}, q_{\\max}\\right),\n\\]\nwhere \\[s\\] is the scale factor and \\[\\lfloor \\cdot \\rceil\\] denotes rounding. The gradient is approximated via:\n\\[\n\\frac{\\partial L}{\\partial x} \\approx \\frac{\\partial L}{\\partial \\hat{x}}.\n\\]\nThe optimal scale \\[s^*\\] minimizes the Kullback-Leibler divergence between the full-precision gradient distribution \\[P(g)\\] and the quantized distribution \\[Q(g; s)\\]:\n\\[\ns^* = \\arg\\min_{s} \\, \\mathrm{KL}(P(g) \\parallel Q(g; s)) = \\arg\\min_{s} \\sum_{i} P(g_i) \\log \\frac{P(g_i)}{Q(g_i; s)}.\n\\]\n\nModel Freezing and Tracing:\nThe optimized model is frozen and traced using libtorch, resulting in a static computation graph that is exported to the ONNX format:\nimport torch\n\nmodel.eval()  # Freeze layers like batch normalization and dropout\nexample_input = torch.randn(1, 3, 512, 512)\ntraced_model = torch.jit.trace(model, example_input)\ntraced_model.save(\"model_traced.pt\")\n\ntorch.onnx.export(\n    model, \n    example_input, \n    \"model_int8.onnx\", \n    export_params=True,\n    opset_version=11,\n    do_constant_folding=True,\n    input_names=['input'],\n    output_names=['output']\n)\nIntegration with Hailo AI:\nThe traced ONNX model is integrated into the C++ inference pipeline and executed using the Hailo AI inference server, providing optimized INT8 performance for real-time processing."
  },
  {
    "objectID": "posts/2024-12-24-spotspraying/index.html#field-actuation-and-3d-processing",
    "href": "posts/2024-12-24-spotspraying/index.html#field-actuation-and-3d-processing",
    "title": "Precision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System",
    "section": "Field Actuation and 3D Processing",
    "text": "Field Actuation and 3D Processing\nAfter real-time inference, the system must translate the segmentation output into actionable data for field actuation. This section integrates depth projection, 3D localization, and coordinate transformation.\n\nDepth Projection and 3D Localization\nThe 2D segmentation mask is projected into 3D space using the intrinsic camera matrix \\[K\\]:\n\\[\n\\begin{bmatrix}\nX \\\\\nY \\\\\nZ\n\\end{bmatrix} = d(u,v) \\cdot K^{-1}\n\\begin{bmatrix}\nu \\\\\nv \\\\\n1\n\\end{bmatrix},\n\\]\nwhere \\[d(u,v)\\] represents the depth at pixel \\[(u,v)\\]. Clustering the resulting 3D points enables the estimation of weed density and the computation of average spatial coordinates for each weed cluster.\n\n\nCoordinate Transformation for Field Actuation\nTo accurately guide the mechanical weeding nib, the computed 3D coordinates (initially defined in the camera frame) must be transformed into the machine’s coordinate system. This alignment is performed using a pre-computed transformation matrix.\n\nC++ Implementation\n#include &lt;Eigen/Dense&gt;\n\nEigen::Matrix4f getTransformMatrix();  // Retrieves the transformation matrix\n\nEigen::Vector4f getPlantCoordinates(float x, float y, float z) {\n    Eigen::Matrix4f camToActuator = getTransformMatrix();\n    Eigen::Vector4f plantCoordCam(x, y, z, 1);\n    return camToActuator * plantCoordCam;\n}\nThis transformation ensures that the machine’s location in the field is accurately adjusted for effective actuation."
  },
  {
    "objectID": "posts/2024-12-24-spotspraying/index.html#weedicide-usage-calculation-and-motivation",
    "href": "posts/2024-12-24-spotspraying/index.html#weedicide-usage-calculation-and-motivation",
    "title": "Precision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System",
    "section": "Weedicide Usage Calculation and Motivation",
    "text": "Weedicide Usage Calculation and Motivation\n\nWhy Precision Spot Spraying?\nConventional systems apply herbicides uniformly, often wasting chemicals and affecting non-target crops. By precisely identifying weed-infested areas using real-time semantic segmentation and depth estimation, the system applies herbicides only where necessary, reducing chemical waste and environmental impact.\n\n\nCalculating Weedicide Requirements\n\nSegmentation Map \\[S(u,v)\\]:\n\\[\nS(u,v) =\n\\begin{cases}\n1, & \\text{if pixel } (u,v) \\text{ is classified as weed} \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]\nReal-World Area Calculation:\nThe area corresponding to a pixel is approximated by:\n\\[\nA_{pixel}(u,v) = \\left(\\frac{d(u,v)}{f}\\right)^2,\n\\]\nwhere \\[d(u,v)\\] is the depth at pixel \\[(u,v)\\] and \\[f\\] is the focal length.\nTotal Weed-Covered Area \\[A_w\\]:\n\\[\nA_w = \\sum_{u,v} S(u,v) \\cdot \\left(\\frac{d(u,v)}{f}\\right)^2.\n\\]\nHerbicide Amount:\nGiven \\[\\beta\\] as the application rate (liters per square meter), the total herbicide required is:\n\\[\n\\text{Weedicide Amount} = \\beta \\cdot A_w.\n\\]\n\nThis calculation guarantees that herbicide is applied only where needed."
  },
  {
    "objectID": "posts/2024-12-24-spotspraying/index.html#key-results",
    "href": "posts/2024-12-24-spotspraying/index.html#key-results",
    "title": "Precision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System",
    "section": "Key Results",
    "text": "Key Results\n\nImproved Segmentation Accuracy:\nIntegration of the channel attention module resulted in a\n\\[\\textbf{19\\% increase in IoU}\\].\nEnhanced Processing Speed:\nOptimizations increased the frame rate from 1.3 fps to 32 fps on the NVIDIA Xavier AGX platform.\nReal-Time Deployment:\nThe embedded system on an NXP board achieved 23 fps with INT8 quantization."
  },
  {
    "objectID": "posts/2024-12-24-spotspraying/index.html#conclusion",
    "href": "posts/2024-12-24-spotspraying/index.html#conclusion",
    "title": "Precision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System",
    "section": "Conclusion",
    "text": "Conclusion\nThis project demonstrates a comprehensive approach to precision weeding in sugarbeet fields. By combining advanced semantic segmentation with robust model optimizations and efficient embedded deployment, the system offers a scalable solution for targeted weed treatment. The integrated field actuation module—comprising depth projection, 3D localization, and coordinate transformation—ensures that the machine’s location is accurately adjusted for precise herbicide application. This method not only reduces herbicide waste and environmental impact but also offers significant cost savings and improved operational efficiency—key benefits for modern precision agriculture."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html",
    "href": "posts/2026-01-03-mHC/index.html",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "",
    "text": "In the world of Deep Learning, we have a “Golden Rule” that has allowed models to evolve from the image classifiers of 2015 to the reasoning giants of today: the Identity Mapping. For a very long time, this was seen a standard which did not need any further engineering but DeepSeek’s team begs to differ here.\nThink of a standard Residual Network (ResNet) as a single-lane highway. The “Identity Mapping” is the rule that allows traffic (information) to flow straight through from start to finish without stopping. Mathematically, we express this as:\n\\[x_{l+1} = x_l + F(x_l, W_l)\\]\n Figure 1(a-c) from the paper - shows residual connection, HC architecture, and mHC architecture\n\n\nThis simple addition was a key enabler in scaling deep networks. The identity mapping allows gradients to flow cleanly through hundreds of layers, which became essential as architectures grew from models like ResNet-50 (~25 million parameters) to modern LLMs with hundreds of billions of parameters.\nThe Physics: During training, when the model looks backward to learn (backpropagation), the gradient flows through the identity path (the x_l term) without any modification, ensuring the signal doesn’t vanish or explode, even after traveling through hundreds of layers.\nConcrete Example: Imagine you’re training a 100-layer network. During backpropagation, gradients need to flow from layer 100 back to layer 1. Without the identity mapping, each layer might multiply the gradient by some value like 0.9. After 100 layers: 0.9^100 ≈ 0.0000266. Your gradient has essentially vanished - the early layers can’t learn anything. With the identity mapping, there’s always a direct gradient path back to early layers, preserving the signal."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#part-1-the-golden-rule-of-deep-learning",
    "href": "posts/2026-01-03-mHC/index.html#part-1-the-golden-rule-of-deep-learning",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "",
    "text": "In the world of Deep Learning, we have a “Golden Rule” that has allowed models to evolve from the image classifiers of 2015 to the reasoning giants of today: the Identity Mapping. For a very long time, this was seen a standard which did not need any further engineering but DeepSeek’s team begs to differ here.\nThink of a standard Residual Network (ResNet) as a single-lane highway. The “Identity Mapping” is the rule that allows traffic (information) to flow straight through from start to finish without stopping. Mathematically, we express this as:\n\\[x_{l+1} = x_l + F(x_l, W_l)\\]\n Figure 1(a-c) from the paper - shows residual connection, HC architecture, and mHC architecture\n\n\nThis simple addition was a key enabler in scaling deep networks. The identity mapping allows gradients to flow cleanly through hundreds of layers, which became essential as architectures grew from models like ResNet-50 (~25 million parameters) to modern LLMs with hundreds of billions of parameters.\nThe Physics: During training, when the model looks backward to learn (backpropagation), the gradient flows through the identity path (the x_l term) without any modification, ensuring the signal doesn’t vanish or explode, even after traveling through hundreds of layers.\nConcrete Example: Imagine you’re training a 100-layer network. During backpropagation, gradients need to flow from layer 100 back to layer 1. Without the identity mapping, each layer might multiply the gradient by some value like 0.9. After 100 layers: 0.9^100 ≈ 0.0000266. Your gradient has essentially vanished - the early layers can’t learn anything. With the identity mapping, there’s always a direct gradient path back to early layers, preserving the signal."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#part-2-the-innovation---hyper-connections-hc",
    "href": "posts/2026-01-03-mHC/index.html#part-2-the-innovation---hyper-connections-hc",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "Part 2: The Innovation - Hyper-Connections (HC)",
    "text": "Part 2: The Innovation - Hyper-Connections (HC)\nRecent research introduced “Hyper-Connections” (HC) to upgrade this highway. HC widens the road by an expansion rate of n (typically n=4).\n\nUnderstanding the Dimensions\nIt’s important to visualize this correctly. We aren’t just making the single vector 4 times longer. Instead, we are building 4 parallel streams (lanes) that run side-by-side.\nStandard: 1 Stream of size d (e.g., 4096)\nHC (n=4): 4 Streams, each of size d\n\\[M^{(t)} = T_r(T_c(M^{(t-1)}))\\]\nIntuition: Think of it like having 4 different “perspectives” on the same information. One stream might specialize in syntax, another in semantics, a third in world knowledge, and the fourth in reasoning patterns. By having these parallel streams, the model can maintain multiple specialized representations simultaneously. (See Figure 1(b) above)\nHC builds complex interchanges to mix the traffic between these lanes:\n\\[\\prod_{i=1}^{100} H^{res}_{100-i} \\text{ is ALSO doubly stochastic}\\]\nWhere:\n\nreads: \\(H^{pre}_l \\in \\mathbb{R}^{1 \\times n}\\) reads from the streams into the layer (aggregates 4 streams → 1 input)\nwrites: \\(H^{post}_l \\in \\mathbb{R}^{1 \\times n}\\) writes the layer output back to the streams (distributes 1 output → 4 streams)\nmixes: \\(H^{res}_l \\in \\mathbb{R}^{n \\times n}\\) mixes information between the parallel streams (4 streams → 4 streams)\n\nConcrete Example of Mixing: Suppose stream 1 contains grammatical information and stream 2 contains semantic information. The mixing matrix \\(H^{res}_l\\) might have learned that for certain tasks, you need 70% grammar + 30% semantics in the first output stream, and 20% grammar + 80% semantics in the second output stream. This is what “mixing” means - creating weighted combinations of the specialized streams.\nThis diversification drastically increases the model’s capacity to learn and reason by allowing different “lanes” to specialize in different features."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#part-3-the-problem---the-crash",
    "href": "posts/2026-01-03-mHC/index.html#part-3-the-problem---the-crash",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "Part 3: The Problem - The Crash",
    "text": "Part 3: The Problem - The Crash\nBut there was a catch. HC removed the traffic rules. Without the safety of the Identity Mapping, the mixing matrices could arbitrarily multiply the signal strength.\n\nVisualizing the Failure\nImagine a graph where the X-axis is the Layer Number (Depth) and the Y-axis is the Signal Variance (Energy).\nStable ResNet: The line is flat. The energy stays constant at 1.0 from Layer 1 to Layer 100, effectively following the “speed limit.”\nUnstable HC: The line looks like an exponential “rocket launch.” It starts small, but the compound effect causes it to explode.\nThe Math Behind the Explosion: In standard ResNet, after 100 layers you have:\n\\[\\|H^{res}_l x\\|_2 \\leq \\|x\\|_2\\]\nThe x_0 term is unchanged - it’s literally the same vector that entered layer 0.\nIn HC, after 100 layers you have:\n\\[\\mathcal{M}_{res} = \\{H^{res}_l \\in \\mathbb{R}^{n \\times n} \\mid H^{res}_l \\mathbf{1}_n = \\mathbf{1}_n, \\mathbf{1}_n^T H^{res}_l = \\mathbf{1}_n^T, H^{res}_l \\geq 0\\}\\]\nThat product of 100 matrices is the problem. Even if each H^{res}_l has a maximum eigenvalue of just 1.02 (only 2% above unity), after 100 layers: 1.02^100 ≈ 7.24. Your signal has amplified by 7×! And with uncontrolled matrices, you might see eigenvalues of 1.1 or higher, leading to: 1.1^100 ≈ 13,780 - complete explosion.\n Figure 3 from the paper - shows the dramatic explosion in gradient magnitude for HC reaching nearly 10^4\n Figure 2 from the paper - shows the training instability and loss spikes in HC\nWhy This Breaks Training: When gradients explode to magnitudes of 10^4, the optimizer (Adam, SGD, etc.) receives nonsensical update signals. It’s like trying to park a car when the speedometer randomly jumps between 5 mph and 5,000 mph - you have no reliable information to make good decisions."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#part-4-the-rules-of-the-road-the-math",
    "href": "posts/2026-01-03-mHC/index.html#part-4-the-rules-of-the-road-the-math",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "Part 4: The Rules of the Road (The Math)",
    "text": "Part 4: The Rules of the Road (The Math)\nTo fix the instability caused by Hyper-Connections, the authors had to impose strict “traffic rules” on the mixing matrices. They restrict \\(H^{res}_l\\) to the Birkhoff Polytope (\\(\\mathcal{M}_{res}\\)), the geometric set of all Doubly Stochastic matrices:\n\\[x_100 = \\left(\\prod_{i=1}^{100} H^{res}_{100-i}\\right) x_0 + ...\\]\nWhere \\(\\mathbf{1}_n\\) is a column vector of ones. While this looks abstract, it translates to two simple physical rules:\n\nRule 1: Row Stochasticity\n\\[H^{res}_l \\mathbf{1}_n = \\mathbf{1}_n\\]\nThe sum of weights for each outgoing stream is exactly 1.\nWhat this means: Imagine you have 100 “units of energy” in stream 1. Row stochasticity says: “You can redistribute this energy to streams 1, 2, 3, 4 in any proportion you want (e.g., 25% to each, or 70% to stream 1 and 10% to each of the others), BUT the total output must still be 100 units.” You cannot create energy out of nowhere.\nExample:\nStream 1 (100 units) → [0.7×100 → Stream 1, 0.1×100 → Stream 2, \n                         0.1×100 → Stream 3, 0.1×100 → Stream 4]\nTotal output = 70 + 10 + 10 + 10 = 100 units\nThis prevents the “Rocket Launch” effect by ensuring the total signal energy cannot be amplified.\n\n\nRule 2: Column Stochasticity\n\\[\\mathbf{1}_n^T H^{res}_l = \\mathbf{1}_n^T\\]\nThe sum of weights for each incoming stream is exactly 1.\nWhat this means: For any output stream, the contributions from all input streams must sum to exactly 1. This ensures every input feature is fully utilized and not “lost.”\nExample:\nOutput Stream 1 receives:\n  0.4 from Input Stream 1\n  0.3 from Input Stream 2\n  0.2 from Input Stream 3\n  0.1 from Input Stream 4\nTotal = 0.4 + 0.3 + 0.2 + 0.1 = 1.0\nThis prevents vanishing gradients by ensuring no stream is “forgotten” or “zeroed out.”\n\n\nThe Mathematical Guarantee: Conservation Properties\nWhen a matrix is doubly stochastic, it provides powerful conservation guarantees. The result is a convex combination - a weighted average where the weights sum to 1.\nWhy this matters:\n\\[\\|H^{res}_l x\\|_1 = \\|x\\|_1\\]\nDoubly stochastic matrices exactly preserve the 1-norm (sum of absolute values) of any vector. This is stronger than just bounding the norm - it’s perfect conservation. Additionally, they bound the 2-norm: \\(\\|H^{res}_l x\\|_2 \\leq \\|x\\|_2\\). This dual property ensures that the signal energy is conserved during propagation, preventing both explosions and vanishing.\nFurthermore - the crucial closure property: If you multiply two doubly stochastic matrices together, you get another doubly stochastic matrix! This means:\n\\[\\prod_{i=1}^{100} H^{res}_{100-i} \\text{ is ALSO doubly stochastic}\\]\nSo even after 100 layers, the composite mapping still respects the “speed limit” of 1.0.\nWhy doubly stochastic instead of spectral normalization? While spectral normalization (constraining maximum singular value to 1) also prevents explosions, doubly stochastic matrices offer a key advantage: they allow flexible mixing between streams while preserving total energy. Spectral normalization only preserves norm without enabling the rich cross-stream information exchange that makes multi-stream architectures powerful. The doubly stochastic constraint provides stability AND expressivity.\n\n\nThe Enforcer: Sinkhorn-Knopp\nWe cannot train these constrained parameters directly using standard gradient descent. Instead, we train a “messy,” unconstrained parameter matrix \\(\\tilde{H}^{res}_l\\) and force it to follow the rules during the forward pass using the Sinkhorn-Knopp algorithm.\nThis algorithm acts like a strict accountant. It alternates between normalizing rows and columns:\n\\[M^{(t)} = T_r(T_c(M^{(t-1)}))\\]\nWhere \\(T_r\\) normalizes rows (divide each row by its sum) and \\(T_c\\) normalizes columns (divide each column by its sum).\nStep-by-step example:\nStarting matrix (after exp to make it positive):\n[1.0  3.0]\n[2.0 10.0]\nRow sums: [4, 12], Column sums: [3, 13]\nIteration 1 - Normalize rows:\n[0.25  0.75]  (row 1 / 4)\n[0.17  0.83]  (row 2 / 12)\nColumn sums: [0.42, 1.58]\nIteration 1 - Normalize columns:\n[0.60  0.47]  (col 1 / 0.42, col 2 / 1.58)\n[0.40  0.53]\nRow sums: [1.07, 0.93]\nAfter ~20 iterations, both constraints are satisfied to high precision! The Birkhoff polytope is a convex set, and Sinkhorn-Knopp is performing an alternating projection between two linear constraints, guaranteed to converge.\nWhat Do These Matrices Actually Look Like?  Figure 8 from the paper - Visualizations of learned mappings. Top row shows unstable HC matrices with extreme values. Bottom row shows mHC’s doubly stochastic matrices with controlled, balanced weights.\nThis figure reveals the difference in practice:\nHC (top row): The unconstrained matrices show extreme values (ranging from -259 to +509 in composite mappings). When you see a row sum of 18.73 or -15.29, that’s the “rocket launch” happening - signals being amplified or attenuated wildly. Notice how the forward signal gain and backward gradient gain (labeled on axes) deviate massively from 1.0.\nmHC (bottom row): Every matrix is beautifully balanced. Individual entries vary (showing the network learned something!), but crucially: row sums ≈ 1.0, column sums ≈ 1.0. Even in the composite mapping ∏ P_Mres(H^res) after 60 layers, the gains stay near 1.0. The Sinkhorn constraint is working exactly as designed."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#this-is-the-before-and-after-of-manifold-constraints---transforming-chaos-into-controlled-stable-mixing.",
    "href": "posts/2026-01-03-mHC/index.html#this-is-the-before-and-after-of-manifold-constraints---transforming-chaos-into-controlled-stable-mixing.",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "This is the “before and after” of manifold constraints - transforming chaos into controlled, stable mixing.",
    "text": "This is the “before and after” of manifold constraints - transforming chaos into controlled, stable mixing."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#part-5-cheating-the-memory-wall",
    "href": "posts/2026-01-03-mHC/index.html#part-5-cheating-the-memory-wall",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "Part 5: Cheating the Memory Wall",
    "text": "Part 5: Cheating the Memory Wall\nThe mathematical elegance of the Birkhoff Polytope comes with a heavy price tag. By setting the expansion rate to n=4, the authors effectively widened the highway by four times, creating a massive data pile-up.\n\nQuantifying the Cost\nConsider training a standard 100-layer Large Language Model with hidden dimension d = 4096, batch size = 1 million tokens, and FP16 precision (2 bytes per number).\nStandard Model (n=1):\nMemory = 100 layers × 1M tokens × 4096 dim × 2 bytes\n       = 819.2 GB ≈ 800 GB\nHyper-Connected Model (n=4):\nMemory = 100 layers × 1M tokens × (4 × 4096) dim × 2 bytes\n       = 3,276.8 GB ≈ 3.2 TB\nThis 3.2 Terabytes is the Memory Wall. For reference, an NVIDIA H100 GPU has 80 GB of HBM memory. You’d need 41 H100 GPUs just to hold the activations!\n\n\nWhy is mHC Memory-Heavy but Compute-Light?\nLet’s break down the operations to understand this crucial trade-off.\nComputational Complexity (FLOPs): For a mixing operation \\(H^{res}_l x_l\\) where \\(H^{res}_l \\in \\mathbb{R}^{4 \\times 4}\\) and \\(x_l \\in \\mathbb{R}^{4 \\times 4096}\\):\nFLOPs = 2 × 4 × 4 × 4096 ≈ 131K operations per token\nCompare this to the FFN layer:\nFLOPs = 2 × 4096 × (4 × 4096) ≈ 134M operations per token\nThe mHC mixing is 1000× cheaper in terms of compute! It’s literally just multiplying a tiny 4×4 matrix by the streams. This is “lightweight math.”\nMemory Complexity (Bytes): But we need to store those 4 expanded streams:\nMemory = 4 streams × 4096 dim × 2 bytes = 32,768 bytes per token\nvs.\nStandard = 1 stream × 4096 dim × 2 bytes = 8,192 bytes per token\n4× more memory, but the computation is negligible. Modern GPUs are memory-bandwidth limited, not compute-limited. Reading 3.2 TB of data from memory takes over 1 second, even if the actual math only takes 0.1 seconds! This is why the “recomputation trick” works - we trade a cheap 0.1s of extra compute to avoid paying the expensive 1s+ of memory I/O.\n\n\nSolution 1: Kernel Fusion (The “Countertop” Strategy)\nThe first bottleneck is speed. The Sinkhorn algorithm requires reading and writing the matrix from memory 40 times (20 iterations × 2 operations per iteration).\nThe Delivery Truck Problem: Without fusion, each iteration loads from slow GPU HBM memory, performs fast computation, then writes back. The frequent memory transfers dominate the execution time.\nWith Kernel Fusion: 1. Load matrix from HBM to GPU registers (on-chip, high bandwidth) 2. Do ALL 20 iterations entirely in registers 3. Write final result back to HBM\nThe solution is Kernel Fusion. By writing a custom kernel (using TileLang), the engineers load the small \\(n \\times n\\) mixing matrix into the GPU’s ultra-fast registers. They perform all 20 iterations of the math right there, without ever sending intermediate results back to main memory. This turns a bandwidth-bound operation into a compute-bound one, significantly reducing the overhead of the Sinkhorn iterations.\n\n\nSolution 2: Selective Recomputing (The “Salt” Trick)\nFusion fixes the speed, but we still have a 3.2 TB storage problem. This is where Selective Recomputing saves the day.\nThe engineers realized that the mHC mixing operation is computationally cheap (lightweight math) but memory-heavy (massive tensors). Therefore, they made the strategic decision to delete the massive output immediately after using it. Instead of paying the “rent” of storing these expanded streams, they pay a tiny “tax” of extra compute to re-calculate them from scratch during the backward pass.\nThe optimal block size:\n\\[L_r^* = \\sqrt{\\frac{nL}{n+2}}\\]\nThis formula minimizes total memory by balancing two factors: If blocks are too small, you need to store many checkpoints; if blocks are too large, you need huge transient memory for the active block.\nFor a 100-layer model with n=4:\nMemory breakdown with \\(L_r = 10\\):\n\nResident memory (first layer of each block):\n  10 blocks × 1M tokens × 4 × 4096 × 2 bytes = 328 GB\n\nTransient memory (active block during backprop):\n  From Table 3: (n+2)C per layer = (4+2) × 4096 = 24,576 elements\n  10 layers × 1M tokens × 24,576 × 2 bytes = 492 GB\n\nTotal peak: 820 GB (down from 3.2 TB, a 4× reduction!)\n Figure 4 from the paper - shows the communication-computation overlapping strategy\nThis trade-off allows the impossible model to fit onto standard hardware."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#part-6-from-theory-to-code",
    "href": "posts/2026-01-03-mHC/index.html#part-6-from-theory-to-code",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "Part 6: From Theory to Code",
    "text": "Part 6: From Theory to Code\nThis is just the basic variant, I will be writing more about the code in details in coming posts.\nTo realize the savings we calculated - avoiding the 3.2 TB memory explosion - we must translate our “Traffic Rules” and “Salt Trick” into actual code. We can replicate the exact logic using JAX, where JIT (Just-In-Time) handles Kernel Fusion and Checkpointing handles Recomputing.\nNote: The code below shows only the Sinkhorn-Knopp projection and core mixing operation. A complete mHC implementation requires integrating the pre-aggregation (\\(H^{pre}_l\\)) and post-distribution (\\(H^{post}_l\\)) matrices along with the residual function \\(F(x, W)\\). See the paper for full architectural details.\n# 1. THE ENFORCER: Sinkhorn-Knopp Projection\n@jax.jit\ndef sinkhorn_knopp(log_matrix, iterations=20, eps=1e-8):\n    # Line below implements: M^(0) = exp(H̃^res_l)\n    # From Equation 8 (page 9): H^res_l = Sinkhorn-Knopp(H̃^res_l)\n    # Initial step before Equation 9\n    M = jnp.exp(log_matrix)  # ← Initial M^(0)\n    \n    def body_fun(i, mat):\n        # Both lines below implement Equation 9 (page 9):\n        # M^(t) = T_r(T_c(M^(t-1)))\n        \n        mat = mat / (jnp.sum(mat, axis=1, keepdims=True) + eps)  # ← T_r (row normalization)\n        mat = mat / (jnp.sum(mat, axis=0, keepdims=True) + eps)  # ← T_c (column normalization)\n        return mat\n\n    M = jax.lax.fori_loop(0, iterations, body_fun, M)  # ← Iterates Equation 9 for t_max iterations\n    return M\n\n\n# 2. THE LAYER: Putting it together\n@jax.checkpoint \ndef mhc_layer(x, w_res_log):\n    # Line below: Third line of Equation 8 (page 9)\n    # H^res_l = Sinkhorn-Knopp(H̃^res_l)\n    H_res = sinkhorn_knopp(w_res_log)  # ← Equation 8 (third line)\n    \n    # Line below: First term of Equation 3 (page 3)\n    # x_{l+1} = H^res_l x_l + (H^post_l)^T F(H^pre_l x_l, W_l)\n    #           ^^^^^^^^^^^^ (this part only)\n    x_new = jnp.matmul(H_res, x)  # ← First term of Equation 3\n    return x_new\nKey Implementation Details:\n@jax.jit - Kernel Fusion: Tells JAX to compile the function into optimized GPU kernels. The fori_loop iterations are unrolled and fused into a single kernel where all 20 Sinkhorn iterations stay in GPU registers.\n@jax.checkpoint - Selective Recomputing: Discards function outputs after forward pass and automatically recomputes them during backward pass. Saves \\(4 \\times\\) memory at ~0.4% compute cost."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#part-7-so-where-are-we",
    "href": "posts/2026-01-03-mHC/index.html#part-7-so-where-are-we",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "Part 7: So where are we?",
    "text": "Part 7: So where are we?\nWe’ve walked through the theoretical crash of Hyper-Connections and the engineering gymnastics required to fix it. But does the Manifold Constraint actually work in practice? The results from the DeepSeek-V3 technical report offer a definitive “yes.”\n\nThe “Rocket Launch” Confirmed\nRecall our fear that unconstrained Hyper-Connections would lead to exploding gradients. We can now look at the empirical evidence in the paper.\n Figure 7 from the paper - shows mHC maintains stable gradient magnitude around 1.0 while HC explodes to nearly 10^4\nThe blue line (standard Hyper-Connected model, unconstrained) shoots up exponentially, with the backward gradient gain reaching a magnitude of nearly 10^4. This is the “Rocket Launch” in real life - a signal explosion that destroys training stability.\nIn stark contrast, the grey line (mHC model) stays perfectly flat near 1.0. The “Traffic Rules” work. The signal is conserved, allowing the model to train as stably as a standard ResNet, even with the expanded highway.\n\n\nStability at Scale\nThis stability isn’t just a neat chart; it translates directly to training performance.\n Figure 5 from the paper - shows smooth training curves for mHC vs unstable HC\nmHC achieves a loss gap improvement of roughly 0.021 compared to the baseline. In the world of Large Language Models, where improvements are measured in fractions of a percent, this is a massive leap in efficiency.\n\n\nThe Cost of Safety\nThe most impressive part of this story, however, is the price tag. Because of the Kernel Fusion and Selective Recomputing strategies, the paper reports that mHC introduces only a 6.7% increase in training time compared to a standard model.\nThis is the “Free Lunch” of Deep Learning: we get the massive capacity increase of a 4-lane highway for nearly the price of a single-lane road.\n Figure 6 from the paper - shows mHC maintains advantages across different scales\n\n\nPerformance on Real Benchmarks\n\n\n\nBenchmark\nBaseline\nHC\nmHC\n\n\n\n\nBBH (Reasoning)\n43.8\n48.9\n51.0\n\n\nDROP (Reading)\n47.0\n51.6\n53.9\n\n\nGSM8K (Math)\n46.7\n53.2\n53.8\n\n\nMMLU (Knowledge)\n59.0\n63.0\n63.4\n\n\n\nmHC yields comprehensive improvements, consistently outperforming the baseline and surpassing HC on the majority of tasks. Notably, compared to HC, mHC further enhances the model’s reasoning capabilities, delivering performance gains of 2.1% on BBH and 2.3% on DROP."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#part-8-open-questions",
    "href": "posts/2026-01-03-mHC/index.html#part-8-open-questions",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "Part 8: Open Questions",
    "text": "Part 8: Open Questions\nWhile mHC has solved the immediate problem of stabilizing Hyper-Connections, it opens a fascinating door for future research. We used the Birkhoff Polytope because it intuitively maps to “conservation of energy.” But is this the only - or even the best - manifold for deep learning?\n\nAlternative Manifolds: A Brief History\nThe idea of constraining weights to specific manifolds isn’t new. It’s part of a rich research tradition in Geometric Deep Learning. Two notable successes stand out:\nSpectral Normalization (2018) - One of the most successful manifold constraints for GANs (Miyato et al., 2018). They constrain weight matrices to have a maximum singular value of 1, which is geometrically equivalent to projecting onto a specific manifold. Just like mHC, spectral normalization prevents gradient explosions by bounding the Lipschitz constant of the network. It became standard in GAN training because it stabilized discriminator training.\nStiefel Manifold for Orthogonal Weights - The set of all orthonormal matrices (where columns are perpendicular unit vectors). Several papers have explored this:\nOrthogonal RNNs (Henaff et al., 2016) showed that orthogonal recurrent weight matrices help RNNs learn long-term dependencies. The Riemannian Approach to Batch Normalization (Cho & Lee, 2017) used manifold optimization for normalization layers.\nThe connection to mHC: Orthogonality constraints preserve norm (like doubly stochastic matrices) but they force diversity between features rather than mixing. mHC chose doubly stochastic because it allows flexible mixing while preserving total energy. Could the Stiefel manifold work for mHC? It might encourage more specialized stream representations. The challenge is computational cost - orthogonal projections require SVD, which is more expensive than Sinkhorn iterations.\nMathematically, the Birkhoff Polytope is just one of many choices. We could imagine projecting weights onto other manifolds that capture different properties of the loss landscape.\n\n\nEfficiency Questions\nThere is also the question of efficiency. We currently use 20 iterations of Sinkhorn-Knopp to enforce the rules. Could we get away with 5? Or is there a learned approximation - a small neural network that predicts the projection in a single step - that could replace the iterative loop entirely? As models continue to grow, these questions of “Geometric Deep Learning” will likely become the new frontier of optimization."
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#conclusion-the-physics-of-the-signal",
    "href": "posts/2026-01-03-mHC/index.html#conclusion-the-physics-of-the-signal",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "Conclusion: The Physics of the Signal",
    "text": "Conclusion: The Physics of the Signal\nThe story of DeepSeek-V3’s Manifold-Constrained Hyper-Connections is a masterclass in modern AI research. It identifies a fundamental structural flaw (the lack of Identity Mapping in expanded streams), solves it with rigorous mathematics (the Birkhoff Polytope), and makes it feasible with hardcore systems engineering (TileLang Fusion and Recomputing).\nFor us developers and researchers, the lesson is clear: Scaling isn’t just about making things bigger. It’s about understanding the “Physics” of the signal. If you can control the flow of information - keeping it on the “Safe Manifold” - you can break the memory wall and build models that are both larger and smarter than we thought possible. (See Figure 1(c) at the beginning of this post)"
  },
  {
    "objectID": "posts/2026-01-03-mHC/index.html#references-further-reading",
    "href": "posts/2026-01-03-mHC/index.html#references-further-reading",
    "title": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections",
    "section": "References & Further Reading",
    "text": "References & Further Reading\nPaper: mHC: Manifold-Constrained Hyper-Connections (DeepSeek-AI, 2025)\nOriginal HC Paper: Hyper-Connections (Zhu et al., 2024)\nClassic Reference: Identity Mappings in Deep Residual Networks (He et al., 2016)\nManifold Constraints: - Spectral Normalization for GANs (Miyato et al., 2018) - Orthogonal RNNs (Henaff et al., 2016) - Riemannian Batch Normalization (Cho & Lee, 2017)"
  },
  {
    "objectID": "projects/hsid/index.html",
    "href": "projects/hsid/index.html",
    "title": "HSID-CNN",
    "section": "",
    "text": "Resources\n\nCode Repository: GitHub Link"
  },
  {
    "objectID": "projects/projec1/index.html",
    "href": "projects/projec1/index.html",
    "title": "Unsupervised Cross Spectral Stereo Matching",
    "section": "",
    "text": "Resources\n\nCode Repository: GitHub Link"
  },
  {
    "objectID": "projects/compiler/index.html",
    "href": "projects/compiler/index.html",
    "title": "Compiler Construction",
    "section": "",
    "text": "Resources\n\nCode Repository: GitHub Link"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rishav’s Website",
    "section": "",
    "text": "GitHub\n  \n  \n    \n     Email\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Twitter\n  \n  \n    \n     Google Scholar\n  \n\n  \n  \nI’m a graduate researcher at Mila, broadly interested in building reliable machine learning systems that work at scale. At Mila, my research has focused on topics in RL, mainly offline (explainability and adaptive regularization), real-time RL, and benchmarking SSL methods for Atari games.\nMy larger goal is to develop safe, interpretable, real-time, and sample-efficient agents that scale, motivating my interests in areas like mechanistic interpretability, world models, and real-time distributed systems.\nBefore Mila, I co-founded Offside (scaled to 100k users), spent ~2 years at DFKI in Germany developing real-time vision algorithms for precision farming, and earned a BEng (Thesis) in Computer Science from BITS Pilani in 2020."
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "Rishav’s Website",
    "section": "Latest Posts",
    "text": "Latest Posts\n\n\n\n\n\n\n\n\n\n\nThe Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections\n\n\nmHC\n\n\n\nJan 3, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nReal-Time Reinforcement Learning\n\n\nArticle on real-time reinforcement learning\n\n\n\nJun 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nGSPO vs GRPO - Theory, Practice, and the Limits of Approximation\n\n\nGSPO vs GRPO\n\n\n\nNov 23, 2025\n\n\n\n\n\n\nNo matching items\n\n\nAll Posts »"
  },
  {
    "objectID": "index.html#beyond-research",
    "href": "index.html#beyond-research",
    "title": "Rishav’s Website",
    "section": "Beyond Research",
    "text": "Beyond Research\nOutside of research, I enjoy reading about ancient civilizations, listening to classic rock, trekking, and strength training. I also write blogs reflecting on projects and life learnings."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "I’m a graduate student at Mila, broadly interested in building reliable machine learning systems that work at scale. At Mila, my research has focused on topics in RL, mainly offline (explainability and adaptive regularization), real-time RL, and benchmarking SSL methods for Atari games.\nMy larger goal is to develop safe, interpretable, real-time, and sample-efficient agents, motivating my interests in areas like mechanistic interpretability, world models, and real-time systems.\nBefore Mila, I co-founded Offside (scaled to 100k users), spent ~2 years at DFKI in Germany developing real-time vision algorithms for precision farming (blog post), and earned a BEng (Thesis) in Computer Science from BITS Pilani in 2020."
  },
  {
    "objectID": "about.html#beyond-research",
    "href": "about.html#beyond-research",
    "title": "about",
    "section": "Beyond Research",
    "text": "Beyond Research\nOutside of research, I enjoy reading about ancient civilizations, listening to classic rock, trekking, and strength training. I also write blogs reflecting on projects and life learnings. Check them out here."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "What’s New & Updated",
    "section": "",
    "text": "The Identity Crisis: How DeepSeek Fixed the Flaw in Hyper-Connections\n\n\n\nai\n\n\n\nmHC\n\n\n\n\n\nJan 3, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nReal-Time Reinforcement Learning\n\n\n\nai\n\n\n\nArticle on real-time reinforcement learning\n\n\n\n\n\nJun 20, 2025\n\n\nIvan Anokhin, Matthew Riemer, Rishav, Gopeshh Subbaraj, Glen Berseth\n\n\n\n\n\n\n\n\n\n\n\n\nGSPO vs GRPO - Theory, Practice, and the Limits of Approximation\n\n\n\nai\n\n\n\nGSPO vs GRPO\n\n\n\n\n\nNov 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDistributed Training with JAX Simplified\n\n\n\nai\n\n\n\nTraining a large model like GPT-3 with JAX.\n\n\n\n\n\nOct 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration\n\n\n\nai\n\n\n\nCuda Programming\n\n\n\n\n\nMar 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPrecision Weeding in Sugarbeets - End-to-End Real-Time Computer Vision System\n\n\n\nai\n\n\n\nSpot Spraying Project\n\n\n\n\n\nDec 24, 2024\n\n\n\n\n\nNo matching items"
  }
]