---
layout: post
title: CUDA from Scratch - Matrix Multiplication, Memory Models, and the Road to RL Acceleration
date: 2025-3-20 21:00:00
description: Cuda Programming 
tags: cuda
categories: ai
thumbnail: assets/img/cuda.png
---
So you have been thinking about CUDA programming from a while and now want to learn but are confused how to proceed and know about memory models, etc. in CUDA. Well, even I am confused, so let's learn together. This is the first blog of the CUDA series where the end goal is to code DQN in C and accelerate training and inference with CUDA.

With AI rapidly advancing, its integration into high-stakes environments is becoming inevitable. In this landscape, efficient deployment is arguably more critical than the pace of new research. As the number of AI tools continues to grow, **efficiency may well become the defining moat** for the tools that endure. 

For nearly every use case, there‚Äôs potential to unlock significant performance gains ‚Äî often by writing custom CUDA kernels tailored to the task.

**RL lives in my heart.** Thus, the end goal of this learning experience is to write a DQN completely in C and accelerate the training using CUDA kernels.

So let's go...!

Let's start off with memory organization in CUDA, then intuitively I will explain my first program: **matrix multiplication in CUDA**.

## First Principles: Memory Organization in CUDA

Before writing a single kernel, let‚Äôs zoom into the GPU's memory layout. This is where most CUDA bugs (and performance pitfalls) are born.

In CUDA, like in C/C++, **matrices are stored in row-major order**.

### What does row-major mean?

Imagine a 2D matrix:

```cpp
A = [[1, 2, 3],
     [4, 5, 6]]
```

Even though it's 2D logically, it's stored in a flat 1D array as:

```cpp
[1, 2, 3, 4, 5, 6]
```

To access the element at `A[row][col]`, the index becomes:

```cpp
index = row * numCols + col
```

For the above matrix, `A[1][2]` is `1*3 + 2 = 5`, and sure enough, `A[5] = 6`.

This little indexing trick ‚Äî `row * numCols + col` ‚Äî is foundational. We‚Äôll use it repeatedly in our CUDA code to compute where each value lives in memory.

## Matrix Multiplication Recap

We have two matrices:

- A of size **M √ó K**
- B of size **K √ó N**

We want to compute:
- C = A √ó B ‚Üí a matrix of size **M √ó N**

The formula for each element of C is:

```cpp
C[row][col] = sum over i of A[row][i] * B[i][col]
```

That‚Äôs a dot product between a row from A and a column from B.

## üöÄ Writing the CUDA Kernel

Here‚Äôs our minimal working kernel:

```cpp
__global__ void matMulKernel(const float* A, const float* B, float* C, int M, int K, int N) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) {
        float sum = 0.0f;
        for (int i = 0; i < K; ++i) {
            sum += A[row * K + i] * B[i * N + col];
        }
        C[row * N + col] = sum;
    }
}
```

## Thread Indexing: Why `.y` for Row and `.x` for Col?

```cpp
int row = blockIdx.y * blockDim.y + threadIdx.y;
int col = blockIdx.x * blockDim.x + threadIdx.x;
```

- Threads are arranged in a **2D grid**
- Each thread computes one element of the result matrix `C[row][col]`
- Rows ‚Üí vertical ‚Üí **Y-axis**
- Columns ‚Üí horizontal ‚Üí **X-axis**

Hence, `row` is computed using `.y` components, and `col` using `.x`.

Makes your mental model clean and your code correct.

---

## Memory Access: Why `row * N + col`?

This was one of the core doubts that tripped me up early on:

> ‚ÄúShouldn‚Äôt it be `row * M + col` since the matrix has M rows?‚Äù

Turns out ‚Äî **no**. It‚Äôs `row * N + col` because:

- We're accessing matrix C, which is **M √ó N**
- Each row of C has **N elements**
- So to access the start of `row`, we skip `row * N` elements
- Then move `col` steps in ‚Üí `row * N + col`

Always remember: it‚Äôs about how **many columns** per row, not how many rows in total.

---

## What about B‚Äôs indexing: `B[i * N + col]`?

Another ‚Äúaha!‚Äù moment:

> ‚ÄúAren‚Äôt we accessing B column-wise here? Shouldn‚Äôt it be column-major?‚Äù

Good instinct ‚Äî but even though we‚Äôre *accessing down a column*, we‚Äôre still treating B as **row-major**.

- We're accessing `B[i][col]`
- Since B has **N columns**, row-major indexing says:

```
B[i][col] = i * N + col
```

So `B[i * N + col]` is still perfectly valid row-major indexing, even if the access pattern feels ‚Äúcolumn-ish‚Äù.

---

## ‚ö†Ô∏è Edge Case Gotcha: Tiny Matrices

Say we multiply:

- A = `[1 2 3]` ‚Üí 1√ó3
- B = `[1, 2, 3]·µó` ‚Üí 3√ó1

The result should be:

```
1√ó1 = [1√ó1 + 2√ó2 + 3√ó3] = [14]
```

But your code might crash or give garbage. Why?

Because your kernel might launch **a full grid of threads**, and many of them will access memory out-of-bounds.

‚úÖ Solution:
```cpp
if (row < M && col < N)
```

Always **bound your threads**. Especially with small matrices.

---

## üß™ Test Case

Here‚Äôs a minimal test:

```cpp
int M = 1, K = 3, N = 1;
float A[] = {1, 2, 3};   // 1x3
float B[] = {1, 2, 3};   // 3x1
float C[1];              // 1x1 output

// Launch with grid/block of 1
dim3 blockSize(1, 1);
dim3 gridSize(1, 1);

// Run the kernel ‚Üí expect C[0] = 14
```

---

## üéØ The End Goal: DQN in Pure C + CUDA

This matrix multiplication was a warm-up.

My goal? To build **Deep Q-Networks** (DQN) completely in C, and **accelerate training with CUDA** ‚Äî no PyTorch, no TensorFlow. Just raw speed and full control.

Reinforcement learning lives in my heart ‚ù§Ô∏è  
And this journey is all about learning from first principles.

---

## üîú What‚Äôs Next?

- Tiled matrix multiplication (for speed!)
- Shared memory optimization
- Writing `ReLU`, `Softmax`, `Linear` layers in CUDA
- Custom CUDA-based experience replay
- CUDA kernels for DQN forward/backward pass

Stay tuned ‚Äî this is just the start!  

